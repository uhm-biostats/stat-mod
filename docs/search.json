[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Statistical Modeling",
    "section": "",
    "text": "Welina mai!\nThis is the course website for ZOOL 631: Intro to Statistical Modeling at the University of Hawaiʻi at Mānoa.",
    "crumbs": [
      "Welina mai!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introductory material",
    "section": "",
    "text": "Here we will introduce the data we will be working with throughout this course. In the context of data we will discusss ethical issues of Indigenous data sov and reproducible science. We will also review the necessary R and version control knowledge for succeeding in this course. Such knowledge will include data structures in R, data input and output, data manipulation, visualization, quarto dynamic documents, and using GitHub.\nJump straight to:\n\nLecture: Data we will work with\nLecture: CARE and FAIR principles\nR refresher\nManaging code with git and GitHub\nDynamic documents with quarto\nBrining it all together",
    "crumbs": [
      "Introductory material"
    ]
  },
  {
    "objectID": "datasets-intro.html",
    "href": "datasets-intro.html",
    "title": "1  Data we will work with",
    "section": "",
    "text": "1.1 Intorducing the data\nWe will be working with data from forest monitoring plots across the pae ʻāina Hawaiʻi as well as environmental and geophysical data measured or interpolated at these monitoring plots. The forest monitoring plot data are organized and distributed in a data resource called OpenNahele (Craven et al. 2018).\nIn this context open refers to the data being openly shared and accessible in alignment with the FAIR principles which we discuss in the next section. Nahele in ʻōlelo Hawaiʻi means, in this context, forest. The use of ʻōlelo Hawaiʻi points to the fact that the data pertain to the pae ʻāina Hawaiʻi. This naming choice could show respect and acknowledgement of the Hawaiian provenance of these data, but could also be seen as appropriation. The interpretation of the naming choice depends in part on whether the data not only comply with FAIR, but also the CARE principles, which, again, we discuss in the next section. The CARE principles mandate that Indigenous provenance and governance rights are acknowledged and respected in the stewardship of data. In the case of OpenNahele, the CARE principles are not met. While the good intentions of those distributing the data is not being questioned, we also recognize there is opportunity for making right the stewardship of these, and other, data from Hawaiʻi. We dive much deeper into this discussion in the next section on CARE and FAIR.\nThe environmental and geophysical data we will use include climate variables, information about human impact, elevation, and geologic age of substrates at the locations of the plot data. For climate variables we use the Hawaiʻi Climate Data Portal (McLean et al. 2023). Elevation, geologic age, and human impact data have been organized by the authors of the OpenNahele data set in a separate data publication (Craven 2019) and we will use those already compiled data rather than re-gather them from the primary sources. However, for the sake of completeness, the primary sources are as follows: elevation (Jarvis et al. 2008), substrate age (Sherrod et al. 2007), and human impact (Society & International Earth Science Information Network 2005).\nWhile these environmental and geophysical data meet the FAIR principles, they again are not CARE compliant. Part of our work in this course will be discussing and envisioning how data collected in Hawaiʻi can live up to the CARE principles.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#preliminary-description-of-the-data",
    "href": "datasets-intro.html#preliminary-description-of-the-data",
    "title": "1  Data we will work with",
    "section": "1.2 Preliminary description of the data",
    "text": "1.2 Preliminary description of the data\n\n1.2.1 Forest plot data\nThe forest plot data are found in data/OpenNahele_Tree_Data.csv. The data contain taxonomic identity and diameter at breast height (DBH) for 43,590 individual trees found across 530 plots. Figure 1.1 shows the locations of plots across the pae ʻāina.\n\n\n\n\n\n\n\n\nFigure 1.1: Distribution of plot locations across the pae ʻāina Hawaiʻi. Points are semi-transparent to better show plot locations when those locations are very proximate, thus darker colors represent a higher density of plots.\n\n\n\n\n\nRows in the forest plot data represent individual trees. Therefore, some data across the columns will be duplicated, for example trees from the same plot will have the same plot ID. Columns in the forest plot data are described in data/README_for_OpenNahele_Tree_Data.txt, repreduced below:\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nIsland\nIsland name\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nStudy\nBrief name of study\n\n\nPlot_area\nPlot area in m2\n\n\nLongitude\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nLatitude\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nYear\nYear in which plot data was collected\n\n\nCensus\nNumeric identifier for each census\n\n\nTree_ID\nUnique numeric identifier for each individual\n\n\nScientific_name\nGenus and species of each individual following TPL v. 1.1\n\n\nFamily\nFamily of each individual following TPL v. 1.1\n\n\nAngiosperm\nBinary variable (1 = yes, 0 = no) indicating whether an individual is classified as an angiosperm following APG III\n\n\nMonocot\nBinary variable (1 = yes, 0 = no) indicating whether an individual is classified as a monocot following APG III\n\n\nNative_Status\nCategorical variable (‘native’, ‘alien’, ‘uncertain’) indicating alien status of each individual following Wagner et al. (2005)\n\n\nCultivated_Status\nBinary variable (1 = yes, 0 = no, NA = not applicable) indicating if species is cultivated following PIER\n\n\nAbundance\nNumber of individuals (all = 1)\n\n\nAbundance_ha\nAbundance of each individual on a per hectare basis\n\n\nDBH_cm\nDiameter at 1.3 m (DBH) for each individual; NA indicates that size was not measured, but was classified by size class\n\n\n\n\n\n\n\n1.2.2 Climate data\nClimate data are found in data/plot_climate.csv. Rows here are unique plots and no information in any column is duplicated. Columns are described in data/README_for_plot_climate.txt, reproduced below:\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nlon\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nlat\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nevapotrans_annual_mm\nActual annual evapotranspiration in mm\n\n\navbl_energy_annual_wm2\nAnnual available energy in W/m^2\n\n\ncloud_freq_annual\nAnnual cloud frequency in days/year\n\n\nndvi_annual\nNormalized Difference Vegetation Index\n\n\nrain_annual_mm\nAnnual rain fall in mm\n\n\navg_temp_annual_c\nAnnual average temperature in celsius\n\n\n\n\n\n\n\n1.2.3 Human impact and geophysical data\nHuman impact data and geophysical data are found in the same file, only because they are from the same source. That file is data/hii_geo.csv. Its rows are also unique plots and no information in columns is duplicated across rows. Columns are described in `\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nlon\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nlat\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nhii\nHuman impact index\n\n\nage_yr\nGeologic substrate age in years before present\n\n\nelev_m\nElevation in meters\n\n\n\n\n\nWe won’t go deeper into describing the data now because we’ll save that for our R refresher when we’ll review coding tools by describing the data numerically and visually.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#ecological-questions-well-consider",
    "href": "datasets-intro.html#ecological-questions-well-consider",
    "title": "1  Data we will work with",
    "section": "1.3 Ecological questions we’ll consider",
    "text": "1.3 Ecological questions we’ll consider\nForests in Hawaiʻi are shaped by a multitude of processes (barton?). This is not a forest ecology class, but it is a class about using statistical models to help us answer questions in ecology and evolution. So we will engage with questions and hypotheses in ecology and evolution. The flora of Hawaiʻi has been a source of inspiration, sustenance, medicine, and scientific inquiry for millennia (abbot-paha?).\n\nKilo\nwestern tradition (eco-evo-anthro)\ninvasion interacting with evo-eco\n\nDownload assignment",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#references",
    "href": "datasets-intro.html#references",
    "title": "1  Data we will work with",
    "section": "1.4 References",
    "text": "1.4 References\n\n\n\n\nCraven D. 2019, June. Dylancraven/hawaii_diversity: beta. Zenodo. Available from https://doi.org/10.5281/zenodo.3250638.\n\n\nCraven D, Knight TM, Barton KE, Bialic-Murphy L, Cordell S, Giardina CP, Gillespie TW, Ostertag R, Sack L, Chase JM. 2018. OpenNahele: The open hawaiian forest plot database. Biodiversity Data Journal:e28406.\n\n\nJarvis A, Guevara E, Reuter H, Nelson A. 2008. Hole-filled SRTM for the globe: Version 4: Data grid.\n\n\nMcLean J, Cleveland SB, Dodge M, Lucas MP, Longman RJ, Giambelluca TW, Jacobs GA. 2023. Building a portal for climate data—mapping automation, visualization, and dissemination. Concurrency and Computation: Practice and Experience 35:e6727. Wiley Online Library.\n\n\nSherrod DR, Sinton JM, Watkins SE, Brunt KM. 2007. Geologic map of the state of hawaii, sheet 3: Island of oahu. United States Geological Survey Open-File Report 1089.\n\n\nSociety WCWC, International Earth Science Information Network C for. 2005. Last of the wild project, version 2 (LWP-2): Global human footprint dataset (geographic).",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "care-fair.html",
    "href": "care-fair.html",
    "title": "2  CARE and FAIR principles",
    "section": "",
    "text": "2.1 FAIR\nCARE and FAIR are principles that specify different but complimentary ethical considerations for the collection, management, use, and governance of data (Carroll et al. 2021). The concepts underlying CARE articulate ideas of Indigenous data sovereignty and governance that have deep historical roots (UN General Assembly 2007; Kukutai & Taylor 2016; Carroll et al. 2020; Carroll et al. 2021). The concepts in FAIR relate to the more recent idea of open and reproducible science. We will discuss FAIR first, though it is not the chronologically earlier concept nor more important, because the current articulation of CARE contends with the fact that FAIR is the emerging default for data (Carroll et al. 2021).\nFAIR (Wilkinson et al. 2016) stands for Findable, Accessible, Interpretable, and Reusable. We’ll go through what each of those mean in practice and then briefly discuss the underlying ethics.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#fair",
    "href": "care-fair.html#fair",
    "title": "2  CARE and FAIR principles",
    "section": "",
    "text": "2.1.1 Findable\nThis means data and metadata should be easy to find by both humans and computers. Data should have persistent identifiers such as a digital object identifier (DOI) and have sufficient structured information such that it is searchable. As an example, the Global Biodiversity Information Facility (GBIF) houses species occurrence data (think museum and herbarium specimens) and assigns DOIs to every submitted data set and every user download. That downloads have a DOI means that research using those downloaded data can reference the DOI, making the exact data underpinning the research finable. GBIF also enables searching for data by different criteria including taxonomy and geographic location.\n\n\n2.1.2 Accessible\nThis means that once found, data and metadata should be retrievable by both humans and computers. Importantly, accessible in this context means metadata are always accessible, even if raw data cannot be accessed (e.g. for ethical reasons). The method for downloading or requesting access to (meta)data should be clear and not require specialized or proprietary tools but rather standardized, open (as in open source) protocols. For example. GBIF allows humans to select data for download through a web interface and complete the download through standard HTTP protocols. GBIF also serves an application programming interface (API) which allows data to be downloaded through automated computer workflows. Other examples of repositories with accessible data are Dryad Digital Repository and Zenodo, two resources we retrieved data from for this class.\n\n\n2.1.3 Interoperable\nData and metadata should use broadly applicable, formal data standards that are themselves FAIR. It should use standardized formats that allow data to be integrated with other data sets and used with various applications. As an example, GBIF uses the Darwin Core standard which provides a shared vocabulary for occurrence-based biodiversity (meta)data, with terms like scientificName, decimalLatitude, and eventDate. These terms can be thought of as standard column names that all data should have in order to meet the Darwin Core standard. All other data that use Darwin Core, or another standard that can be transliterated to Darwin Core, can now interoperate.\n\n\n2.1.4 Reusable\nTo be refundable data must be well-described with accurate metadata, including provenance, and have clear licensing that allows for reuse. In this context provenance is taken to mean the researchers who generated the data and how they generated it. In our discussion of CARE, provenance will take on a much deeper and more accurate meaning. The Darwin Core standard enables metadata to meet the reusability standard as long as the specific licence allows reuse. GBIF, for example, requires data to be licensed under a Creative Commons license. Another metadata standard worth noting is the Ecological Metadata Language which provides additional ways of describing data that do not neatly fit within the occurrence-type data most easily described by Darwin Core.\n\n\n2.1.5 Ethics of FAIR\nGlobal and local disparities in science funding access (Ma et al. 2015; Petersen 2021; Chen et al. 2022; Nguyen et al. 2023; Larregue & Nielsen 2024) means that a small proportion of researchers have dominated the means of producing data. If data were not FAIR, researchers without access to the same funding would be further disadvantaged and their contributions to science would be further lost. FAIR also came to the scientific forefront in response to the replication crisis (Nosek et al. 2015; Wilkinson et al. 2016). It is hoped that making data open and reusable will make science more transparent and reproducible (Parker et al. 2016; Filazzola & Cahill Jr 2021). Greater transiency and reproducibility should make science less prone to the analytical errors and biases that led to the replication crisis, as well as make the scientific record quicker and easier to correct when errors do occur. Most scientific data is also publicly funded and therefore there are ethical considerations for making data broadly accessible by everyone, public included, and reusable to ensure the highest scientific return on the investment from the public (Stebbins 2013; Maglia 2015).",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#care",
    "href": "care-fair.html#care",
    "title": "2  CARE and FAIR principles",
    "section": "2.2 CARE",
    "text": "2.2 CARE\nWhile ethics are implied in FAIR, CARE intentionally addresses them head-on in the context of Indigenous peoples’ rights (UN General Assembly 2007; Carroll et al. 2020; Carroll et al. 2021). CARE is an articulation of Indigenous data sovereignty (IDSov) and Indigenous data governance (IDGov). IDSov and IDGov apply to Indigenous data. Indigenous data are any form of data (including traditional knowledge, language, physical materials, digital records, and more) in any format that originate from or impact Indigenous peoples, people, communities, nations, territories (including traditional territories now occupied by settlers), and all their constituent pieces from natural “resources” to human bodies to cultural expression (Carroll et al. 2021).\nWhere data “originate from” is a way of saying what is the provenance of the data. We saw provenance when discussing the necessity of clearly describing data under FAIR. From a colonial perspective the provenance of data sits with the researcher extracting data. But a de-colonial perspective reveals that the provenance sits with the original knowledge holders and/or stewards of the sources (e.g. lands and waters) of knowledge. The provenance of an academic recording of ʻōlelo Hawaiʻi spoken by a mānaleo does not sit with the academic but with the speaker and more broadly with Kānaka ʻŌiwi. The provenance of an entomological collection from the Waiʻanae Mountains on Oʻahu does not sit with the collector (whose name is likely attached to each specimen) but with the ʻāina, the lāhui Hawaiʻi, and Kānaka ʻŌiwi. The distinction about provenance is important because, under western intellectual property systems, provenance and ownership are assumed to be equivalent. Who “owns” the data governs the data. IDSov and IDGov work to recenter Indigenous worldviews, rights, self-determination, and wellbeing in data stewardship, rather than western notions of property.\nWe will start to build understanding of IDSov and IDGov by defining CARE. CARE (GIDA 2019; Carroll et al. 2020) stands for Collective benefit, Authority to control, Responsibility, and Ethics. We will dive deeper into each.\n\n\n\n\n\nimage credit: Carroll et al. (2020)\n\n\n\n\n\n2.2.1 Collective benefit\nData inherently hold potential for innovation, value generation, and decision making. People, communities, and nations can benefit from data. The collective benefit principle stipulates that benefits should be shared collectively and specifically that data stewardship practices must enable Indigenous peoples and people to derive benefit from Indigenous data. Benefit is sometimes—in western worldviews—reflexively equated with monetary value, but benefit takes many forms including the benefit of using data to inform governance and decision making, and generally to promote wellbeing.\n\n\n2.2.2 Authority to control\nIndigenous peoples have a right to govern how Indigenous data are collected, used, managed, and shared. This means that cultural protocols from Indigenous people(s) are prioritized in the stewardship of data. Authority to control can be enabled by FAIR principles if those principles are applied to enable Indigenous peoples to access their data and use it advance their right to self-determination.\n\n\n2.2.3 Responsibility\nThose working with Indigenous data have a responsibility to engage with respect, reciprocity, and earned trust. Respect must be given to cultural protocols and rights to self-determination. Reciprocity is about ensuring that not only the benefits derived from data can advance Indigenous peoples’ wellbeing, but the process of data collection and stewardship have embedded practices of collaboration that expand the capacity (e.g. through increased data literacy) of Indigenous peoples and people to engage with data. Reciprocity and respect are also about seeking mutual understanding of worldviews and ensuring that data and their products are communicated in a way consistent with those worldviews, including in the Indigenous language of the people of provenance. Earned trust means those working with Indigenous data embrace the responsibility of collaborating with integrity and consistently support positive relationships.\n\n\n2.2.4 Ethics\nEthics means centering the rights, wellbeing, and ethical frameworks of Indigenous Peoples in the stewardship of Indigenous data. To do so requires grappling with the ways colonialism and colonists create power imbalances, scarcities (real and constructed), and trauma. This work is not trivial, which also means ethics include honoring that work by ensuring the longevity of data and its potential to produce benefits into the future. Indicating Indigenous provenance is part of ensuring future ethical use.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#care-and-fair",
    "href": "care-fair.html#care-and-fair",
    "title": "2  CARE and FAIR principles",
    "section": "2.3 CARE and FAIR",
    "text": "2.3 CARE and FAIR\nStephanie Carroll and colleagues (2021), leaders in IDSov and IDGov, point out that FAIR principles can support operationalizing CARE and vice versa. Indigenous data are all too often rendered hidden, inaccessible, not reusable, and not interoperable by colonial power imbalances and institutions (Carroll et al. 2021). Making such data FAIR is necessary in these cases to begin the process of implementing CARE. From personal experience (Andy speaking here) CARE can be perceived by some to curtail the implementation of FAIR, but this is misguided.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#implementing-care-with-local-contexts-labels-and-notices",
    "href": "care-fair.html#implementing-care-with-local-contexts-labels-and-notices",
    "title": "2  CARE and FAIR principles",
    "section": "2.4 Implementing CARE with Local Contexts Labels and Notices",
    "text": "2.4 Implementing CARE with Local Contexts Labels and Notices\nMetadata result from operationalizing CARE. Having metadata standards is one important aspect of FAIR. Local Contexts offers one emerging implementation of standard CARE metadata via their Labels and Notices (Local Contexts 2025a, 2025b). Labels are used by Indigenous communities and nations to articulate their rights to and interests in Indigenous data. Notices are used by researchers and colonial institutions to affirm the existence of Indigenous rights and interests and acknowledge that work is underway to articulate those rights and interests. Notices also indicate the readiness of researchers and colonial institutions to fulfill their responsibilities (the R in CARE) of centering Indigenous worldviews, self-determination, wellbeing, and rights in data stewardship.\nFor metadata to be useful they need to be FAIR. To make Local Contexts Labels and Notices FAIR they are hosted on the Local Contexts Hub, an online repository. To work with these metadata, communities, researchers, and colonial institutions must first make an account on the Hub (free for communities and researchers, paid on a subscription basis for institutions). Local Contexts Hub is structured by projects. Labels and Notices are applied to data by connecting both to a project. Projects have permanent unique identifiers and persistent URLs based on those identifiers, replicating the permanence of DOIs.\nTo understand this better we will walk through the a Local Contexts workflow from the perspective of a researcher initiated the process, because “researcher” is likely the common aspect of identity shared by those participating in this course.\n\n2.4.1 A researcher applies Notices and notifies communities\nThe Local Contexts Hub workflow for a researcher should ideally begin with co-production of research goals and data management with local communities in real life outside of the Hub. For archival data already collected (the majority of data) this likely did not occur. Within the Hub a researcher creates a project to correspond to Indigenous data they are working with. The Hub does not house the data itself, only the metadata. Then Notices are applied to the project and communities are notified via the Hub. Again, ideally, researchers and communities will already be working together because even the act of defining what data are and the geographic or other bounds that delineate one data set (and thus one Hub project) from another can be surprisingly different across different ways of knowing. But projects can still be created and Notices applied in situations where co-production has not occurred (don’t let perfect be the enemy of good).\nNotices, again, affirm the existence of Indigenous rights to and interests in specific data. A notice is three things: 1) a visual badge drawing the focus of a person engaging with Indigenous data; 2) human-readable text explaining the significance of the Notice; and 3) a machine-readable string that can be integrated into data processing and management workflows. There are multiple Notices each with unique purpose. We will focus on four that individual researchers are most likely to use.\n\n2.4.1.1 Open to Collaborate\n\n\n\n\nThe Open to Collaborate Notice is an Engagement Notice that can be used to indicate a researcher’s or institution’s commitment to CARE principles and working collaboratively on issues of IDSov and IDGov. Engagement Notices are not applied to data, but rather are appropriate to display on, for example, research or institution websites. See full information here.\n\n\n\n\n2.4.1.2 Attribution Incomplete\n\n\n\n\nThe Attribution Incomplete Notice is applied to data to indicate that attribution (including provenance and contributors) is incomplete, inaccurate, or missing. This notice indicates a work in progress toward correcting the mistake of incomplete attribution. See full information here.\n\n\n\n\n2.4.1.3 Traditional Knowledge\n\n\n\n\nThe TK Notice indicates that data representing Traditional Knowledge and related terms/concepts (Traditional Ecological Knowledge, Indigenous Knowledge, Indigenous Science) carry Indigenous rights, protocols, and responsibilities. See full information here.\n\n\n\n\n2.4.1.4 Biocultural\n\n\n\n\nThe BC Notice affirms the rights of Indigenous peoples to govern the stewardship of data generated from biological sources within their traditional lands, waters, and territories. See full information here.\n\n\nNotices are not meant to be the permanent metadata associated with Indigenous data because the Notices do not themselves articulate the rights and protocols of Indigenous peoples. That is the job for Labels.\n\n\n\n2.4.2 A community applies Labels\nTo re-iterate, this workflow is from a researcher’s perspective. Indigenous communities can also create projects connected to data without need for researchers external to their communities creating the project and applying Notices. We are using the researcher-initiated example here because we are all researchers, but not all of us may be Indigenous or authorized to speak for Indigenous communities.\nIn the researcher-initiated workflow, once a Notice(s) is applied to data via a project, an Indigenous community can then replace Notices with Labels. Unlike Notices, Labels do articulate the rights and protocols of Indigenous peoples. There are many different kinds of Labels\n\nnot for researchers to apply\nhere are some categories\nthough not for researchers to apply, should all understand and be ready to act\n\n\n\n2.4.3 What’s next?\n\nprojects integrated with database holding data and journals\nusers develop culture of respect and understanding of CARE metadata",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#references",
    "href": "care-fair.html#references",
    "title": "2  CARE and FAIR principles",
    "section": "2.5 References",
    "text": "2.5 References\n\n\n\n\nCarroll S et al. 2020. The CARE principles for indigenous data governance. Data science journal 19. Ubiquity Press.\n\n\nCarroll SR, Herczog E, Hudson M, Russell K, Stall S. 2021. Operationalizing the CARE and FAIR principles for indigenous data futures. Scientific data 8:108. Nature Publishing Group UK London.\n\n\nChen CY, Kahanamoku SS, Tripati A, Alegado RA, Morris VR, Andrade K, Hosbey J. 2022. Systemic racial disparities in funding rates at the national science foundation. Elife 11:e83071. eLife Sciences Publications, Ltd.\n\n\nFilazzola A, Cahill Jr JF. 2021. Replication in field ecology: Identifying challenges and proposing solutions. Methods in Ecology and Evolution 12:1780–1792. Wiley Online Library.\n\n\nGIDA. 2019. Research Data Alliance International Indigenous Data Sovereignty Interest Group. CARE Principles for Indigenous Data Governance. https://www.gida-global.org/care.\n\n\nKukutai T, Taylor J. 2016. Indigenous data sovereignty: Toward an agenda. ANU press.\n\n\nLarregue J, Nielsen MW. 2024. Knowledge hierarchies and gender disparities in social science funding. Sociology 58:45–65. SAGE Publications Sage UK: London, England.\n\n\nLocal Contexts. 2025a. Local contexts labels. https://localcontexts.org/labels/about-the-labels/.\n\n\nLocal Contexts. 2025b. Local contexts notices. https://localcontexts.org/notices/about-the-notices/.\n\n\nMa A, Mondragón RJ, Latora V. 2015. Anatomy of funded research in science. Proceedings of the National Academy of Sciences 112:14760–14765. National Academy of Sciences.\n\n\nMaglia A. 2015. NSF data management and public access initiatives.\n\n\nNguyen M, Gonzalez L, Chaudhry SI, Ahuja N, Pomahac B, Newman A, Cannon A, Zarebski SA, Dardik A, Boatright D. 2023. Gender disparity in national institutes of health funding among surgeon-scientists from 1995 to 2020. JAMA network open 6:e233630–e233630. American Medical Association.\n\n\nNosek BA et al. 2015. Promoting an open research culture. Science 348:1422–1425. American Association for the Advancement of Science.\n\n\nParker TH, Forstmeier W, Koricheva J, Fidler F, Hadfield JD, Chee YE, Kelly CD, Gurevitch J, Nakagawa S. 2016. Transparency in ecology and evolution: Real problems, real solutions. Trends in Ecology & Evolution 31:711–719. Elsevier.\n\n\nPetersen OH. 2021. Inequality of research funding between different countries and regions is a serious problem for global science. Function 2:zqab060. Oxford University Press.\n\n\nStebbins M. 2013. Expanding public access to the results of federally funded research.\n\n\nUN General Assembly. 2007. United nations declaration on the rights of indigenous peoples 12:1–18.\n\n\nWilkinson MD et al. 2016. The FAIR guiding principles for scientific data management and stewardship. Scientific data 3:1–9. Nature Publishing Group.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "r-refresh.html",
    "href": "r-refresh.html",
    "title": "3  R refresher",
    "section": "",
    "text": "3.1 Resources\nhttps://datacarpentry.github.io/R-ecology-lesson/\nbut note: they use %&gt;% and load entire tidyverse",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "4  Managing code with git and GitHub",
    "section": "",
    "text": "4.1 What is git and GitHub and how do they help us?\nCoding and analyzing data are complex tasks. We often need to try different approaches, step away from our code for a while, or share code with others to get their input and collaboration. We might often find ourselves in a situation like this: we make a script, say analysis.R, and then revise it based on feedback, but we don’t want to loose our old work, so we call the new version analysis_v2.R. Then we can’t figure out how to make a plot we want, so we email it to a friend, they edit it and send back a new file called analysis_v2_ajr-edits.R. We resolve those edits and add some other code and save the file with the date so we remember where we were: analysis_2025-01-28.R. But then we don’t get a chance to work on the analysis again for two months. By that time, we can’t remember if analysis_v2.R is the right version or analysis_v2_ajr-edits.R or analysis_2025-01-28.R. You can’t even remember exactly what the edits were and why you needed them.\nAvoiding this situation is the job for git and GitHub. Git is a kind of version control software that runs locally on your computer and tracks changes to files. Those files live in a project folder (usually called a “repo” short for “repository”). GitHub is a web-based platform that hosts git repositories online, allowing you to back up your work, share it with others, and contribute to others’ projects too.\nTogether, git and GitHub help us in several ways. They provide a complete history of our work. Every change we make is recorded with a timestamp and a description we write about what is important in that change. If we make a mistake or want to try something experimental, we can easily revert back to an earlier version. GitHub and git facilitate collaboration. Multiple people can work on the same project simultaneously, and git helps merge their contributions together. Given the web-based hosting and backup, this is kind of like google docs for code (but a little less automated because we manually document and describe changes). GitHub makes it easy to share code, review changes, and give and receive feedback. The feedback bit is shockingly helpful via a tool called “issues” which we will discuss in the Workflow section. Hosting code on GitHub is also a real way to work toward open and reproducible science. Anyone can see exactly how we carried out our analysis.\nAs another important motivator: we will use git and GitHub for all class assignments moving forward.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#using-git-and-github-with-help-from-rstudio",
    "href": "github.html#using-git-and-github-with-help-from-rstudio",
    "title": "4  Managing code with git and GitHub",
    "section": "4.2 Using git and GitHub with help from RStudio",
    "text": "4.2 Using git and GitHub with help from RStudio\nTo start working with git and GitHub we will need to make a free GitHub account and to install the actual git software on our computers. Here are external instructions for those tasks:\n\nmake a free GitHub account; no need to follow the “Next steps” (although eventually you should set up 2FA)\ndownloaded and installed git (click on your operating system here for instruction)\n\nInstead of step (2), if you think you have done this before, check if git is already on your machine by opening the terminal (from RStudio is fine) and run this command:\ngit --version\nYou will see a version number if git is available, you will see an error if it is not.\nNow we can integrate git, GitHub, and RStudio. This is not necessary for using git and GitHub but it makes the experience easier, especially because we are already using RStudio.\nWe will use two R packages to help us connect RStudio and GitHub: usethis and gitcreds. Let’s install those packages by typing the following in the R console\n\ninstall.packages(\"usethis\")\ninstall.packages(\"gitcreds\")\n\nA common problem has been that usethis has a dependency on the package crayon which is currently not playing nice. If you have errors involving crayon try this:\n\ninstall.packages(\"crayon\", type = \"source\")\ninstall.packages(\"usethis\")\n\nNow we can use functions from those packages to help us continue. First we use usethis to set up a personal access tokin that will serve as our login credential with GitHub. We do that by typing this into the R console:\n\nlibrary(usethis)\ncreate_github_token()\n\nThat should open up a web browser page where you might be promoted to log into your GitHub account. Do that and then on the next page scroll to the bottom and click the “Generate token” button. You will then see on a new page a string of numbers and letters—that’s the token. Copy that token and paste it somewhere temporarily, like a blank text editor document or a blank R script, where you paste it doesn’t matter—you’ll soon delete it.\nNow come back to RStudio and run the following in the R console:\n\nlibrary(gitcreds)\ngitcreds_set()\n\nThis will prompt you to enter a password or token; now you can paste the token you just generated here and hit enter. You should now be good to go!\nIt’s possible you’ve already gone through this process. If that’s the case you’ll see this:\n-&gt; Your current credentials for 'https://github.com':\n\n  protocol: https\n  host    : github.com\n  username: PersonalAccessToken\n  password: &lt;-- hidden --&gt;\n\n-&gt; What would you like to do? \n\n1: Keep these credentials\n2: Replace these credentials\n3: See the password / token\n\nSelection: \nEnter 1 for your selection. There might be be an error message but you can ignore it.\nNow we should hopefully be all connected!\n\n4.2.1 Testing our connection\nNow we’ll double check that you’ve connected RStudio with GitHub by going to our GitHub profile and making a new repository. We’ll then copy (i.e. “clone”) this repo to our computers, modify it locally, and send those changes back to GitHub via git (all this sounds overly technical right now, we will gain a deeper understanding in the next section on learning the git workflow).\nFirst let’s make a new repo. Go to your GitHub profile and click the plus sign in the top right, select “New repository”.\n\nGive the repository a name, a description, make it public and select the option to add a README. Then hit “Create repository”\n\nNext hit the green “Code” button and copy the HTTPS URL to your clipboard.\n\nNow head back to RStudio and selection File &gt; New Project. Then select “Version control” then “Git”. Finally, paste the HTTPS URL that you copied from GitHub. The “Project directory name” should auto-population. Choose a meaningful place on your computer to house this project, select the option to open in a new session and hit “Create Project”.\n\nYou should now have a new project open! Navigate to the “Git” tab in RStudio and notice that two file names are listed with yellow question marks by them.\n\nThose files were auto-generated by RStudio. The question marks indicate that they are new files which git is currently not tracking. Click the radio button next to each one under the “Staged” column. Checking those buttons stages the files. We can now hit “Commit” to add a commit message and commit the changes. If you have success it should look something like this (possibly with a warning about username or email, that’s ok!):\n\nOnce we’ve committed the changes we can send those changes to GitHub by hitting the “push” button Hopefully the result of hitting “push” looks something like this (possibly with a warning about username or email, again, that’s ok!):\n\nGo back to your web browser, refresh the GitHub repo page, and you should see the new files you just pushed!\nWhile committing and pushing you might get warning messages about your username and email. If you see these warnings you need to let git and GitHub know who you are. You can do this through the Terminal. To open a terminal tab through RStudio go to Tools &gt; Terminal &gt; New Terminal. This will open a new terminal tab next to the R Console tab. The terminal is different than the R Console. You can think of the terminal as accessing the guts of your computer. Raw R commands will not work here. However, we can interact with git here and that’s what we need to do to identify ourselves. So in the terminal type\ngit config --global user.name \"github_user_name\"\nreplace github_user_name with your actual username; hit enter. Then type\ngit config --global user.email \"my_email\"\nagain replace my_email with the actual email address you used to register your GitHub account; hit enter.\nNow your identity is known and you should be all set!",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#learning-the-git-and-github-workflow",
    "href": "github.html#learning-the-git-and-github-workflow",
    "title": "4  Managing code with git and GitHub",
    "section": "4.3 Learning the git and GitHub workflow",
    "text": "4.3 Learning the git and GitHub workflow\nWe have in fact already seen a big part of this workflow when we confirmed that RStudio is properly communicating with GitHub. Now we will work toward really understanding it.\n\n4.3.1 Clone\nCloning means copying a remote repo, like one on GitHub, to your local computer. This is how you typically first start a local git repo. Cloning is a little more complex than just downloading all the files: cloning also maintains the relationship between your local copy and the remote repo as well as all the version history.\n\n\n4.3.2 Stage and commit\nOnce you have your git repo locally, you will start changing things. “Staging” changes (hitting the radio button next to a file name in RStudio’s “Git” panel) let’s git know to pay attention to the changes you made. We can stage changes from one or multiple files at a time. Once staged, we can then write a commit message. This message briefly records what changes were made and (perhaps) why or how. These messages should be brief, think of them as notes to your future self (or collaborators). Recall from the intro to this chapter that git enables you to roll back to previous versions of your code (for example if you messed something up). The commit message is your way of knowing which version to roll back to. We will cover how to roll back or recover past versions soon.\nThese changes, and their commit messages, still only exist locally on your computer. The next steps are about interacting with the repo hosted on GitHub.\n\n\n4.3.3 Push and pull\nIf we want our local changes to be reflected in the GitHub repo, we need to “push” them. You can push from RStudio by hitting the green up arrow in the “Git” panel. Pushing does just that, it pushes all your committed changes to the GitHub repo.\nConceptually, you should think of your local version of the repo as your temporary, personal copy of the official GitHub repo. Your computer could die, you could loose it, a cat could walk over the keys and mess it all up. Your local version is not permanent, but the GitHub version is. The fact that your local copy is ephemeral also means you should commit often, push often, and pull often.\nWait, what is pull? Pull is how you keep your local copy up-to-date with the official copy on GitHub. You can pull from RStudio by hitting the turquoise down arrow in the “Git” panel. But you might ask: how could the official copy change? Primarily through the commits and pushes of your collaborators. More on that in “Working collaboratively.”\n\n\n4.3.4 Recovering past versions\nTo actually recover previous versions of your code, you have a few options. You can view the history of your repo (as it is preserved from within your local copy) right inside RStudio. On the “Git” panel, hit the watch icon and a window will pop-up showing the history of all commits along with their messages. You can click on any of those commits to see the past versions of your work. The easiest way to recover it is to copy the code of the past version and paste it into the script you’re currently working on. This gives you maximum control to selectively recover specific parts of your past work. Another option is to use code in the terminal to roll back the entire repo to a past version. When you clicked a specific commit, you saw, amoung other information, a long SHA key. You can paste that SHA key into this code to roll back to that commit:\ngit revert &lt;SHA-key&gt;\nYet another option is to use the GitHub interface to view past versions. On the page for the GitHub repo you will see another watch-looking icon. Click that and again you can see a list of past commits which you can click through to see the associated versions of the files/scripts at the times of those commits. You can again copy-paste the code you might want from these past versions.\n\n\n4.3.5 Issues: toward working collaboratively\n\n\n4.3.6 Working collaboratively: branches, forks, and pull requests",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#resources",
    "href": "github.html#resources",
    "title": "4  Managing code with git and GitHub",
    "section": "4.4 Resources",
    "text": "4.4 Resources\nOne of the absolute best references on git, GitHub, and working with both through RStudio is Jenny Bryan’s online book Happy git with R.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "dynamic-doc.html",
    "href": "dynamic-doc.html",
    "title": "5  Dynamic documents with quarto",
    "section": "",
    "text": "5.1 Brief how-to",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic documents with quarto</span>"
    ]
  },
  {
    "objectID": "dynamic-doc.html#brief-how-to",
    "href": "dynamic-doc.html#brief-how-to",
    "title": "5  Dynamic documents with quarto",
    "section": "",
    "text": "yaml\nheaders\nlists\ncode",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic documents with quarto</span>"
    ]
  },
  {
    "objectID": "dynamic-doc.html#now-its-your-turn",
    "href": "dynamic-doc.html#now-its-your-turn",
    "title": "5  Dynamic documents with quarto",
    "section": "5.2 Now it’s your turn",
    "text": "5.2 Now it’s your turn\nconvert R refresh lab into qmd file and render. push to github",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic documents with quarto</span>"
    ]
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Probability",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Probability"
    ]
  },
  {
    "objectID": "ml-method.html",
    "href": "ml-method.html",
    "title": "14  How likelihood maximization works",
    "section": "",
    "text": "should bring up ward intervals here\nmaybe not about maximization at all\nmaybe about uncertainty",
    "crumbs": [
      "Introducing the method of maximum likelihood",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>How likelihood maximization works</span>"
    ]
  },
  {
    "objectID": "care-fair.html#care-and-fair-together",
    "href": "care-fair.html#care-and-fair-together",
    "title": "2  CARE and FAIR principles",
    "section": "2.3 CARE and FAIR together",
    "text": "2.3 CARE and FAIR together\nStephanie Carroll and colleagues (2021), leaders in IDSov and IDGov, point out that FAIR principles can support operationalizing CARE and vice versa. Indigenous data are all too often rendered hidden, inaccessible, not reusable, and not interoperable by colonial power imbalances and institutions (Carroll et al. 2021). Making such data FAIR is necessary in these cases to begin the process of implementing CARE. From personal experience (Andy speaking here) CARE can be perceived by some researchers in colonial institutions to curtail the implementation of FAIR, but this is misguided. As pointed out in the articulation of CARE (GIDA 2019), Indigenous data cannot be used in good conscience without “relationships built on respect, reciprocity, trust, and mutual understanding, as defined by the Indigenous Peoples to whom those data relate” (quoting from GIDA (2019)). Thus without CARE, the R (reusable) in FAIR is invalid. Operationalizing CARE results in richer, more complete metadata, and complete metadata underpin all aspects of FAIR.\nPerceived conflicts between FAIR and CARE might be resolved by asking “FAIR for whom?” and “who gets to decide?” Who gets to decide which protocols are implemented to access and reuse data? Who can find the data (i.e. findable for whom)? Who decided what metadata to expose to make data findable by which search terms? With whose worldview do those search terms align? Contemplating these questions might result in the conclusion that individuals or institutions that are asked to share power—power that previously was not shared—may perceive CARE to limit FAIR. But CARE does not limit FAIR. Rather, colonial power imbalances made data only FAIR for some, which is not actually fulfilling FAIR at all.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  }
]