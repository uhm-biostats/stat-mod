[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Statistical Modeling",
    "section": "",
    "text": "Welina mai!\nThis is the course website for ZOOL 631: Intro to Statistical Modeling at the University of Hawaiʻi at Mānoa.",
    "crumbs": [
      "Welina mai!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introductory material",
    "section": "",
    "text": "Intro stuff\nJump straight to:\n\nLecture: Data sets we will work with\nLecture: CARE and FAIR principles\nR refresher",
    "crumbs": [
      "Introductory material"
    ]
  },
  {
    "objectID": "datasets-intro.html",
    "href": "datasets-intro.html",
    "title": "1  Data we will work with",
    "section": "",
    "text": "1.1 Intorducing the data\nWe will be working with data from forest monitoring plots across the pae ʻāina Hawaiʻi as well as environmental and geophysical data measured or interpolated at these monitoring plots. The forest monitoring plot data are organized and distributed in a data resource called OpenNahele (Craven et al. 2018).\nIn this context open refers to the data being openly shared and accessible in alignment with the FAIR principles which we discuss in the next section. Nahele in ʻōlelo Hawaiʻi means, in this context, forest. The use of ʻōlelo Hawaiʻi points to the fact that the data pertain to the pae ʻāina Hawaiʻi. This naming choice could show respect and acknowledgement of the Hawaiian provenance of these data, but could also be seen as appropriation. The interpretation of the naming choice depends in part on whether the data not only comply with FAIR, but also the CARE principles, which, again, we discuss in the next section. The CARE principles mandate that Indigenous provenance and governance rights are acknowledged and respected in the stewardship of data. In the case of OpenNahele, the CARE principles are not met. While the good intentions of those distributing the data is not being questioned, we also recognize there is opportunity for making right the stewardship of these, and other, data from Hawaiʻi. We dive much deeper into this discussion in the next section on CARE and FAIR.\nThe environmental and geophysical data we will use include climate variables, information about human impact, elevation, and geologic age of substrates at the locations of the plot data. For climate variables we use the Hawaiʻi Climate Data Portal (McLean et al. 2023). Elevation, geologic age, and human impact data have been organized by the authors of the OpenNahele data set in a separate data publication (Craven 2019) and we will use those already compiled data rather than re-gather them from the primary sources. However, for the sake of completeness, the primary sources are as follows: elevation (Jarvis et al. 2008), substrate age (Sherrod et al. 2007), and human impact (Society & International Earth Science Information Network 2005).\nWhile these environmental and geophysical data meet the FAIR principles, they again are not CARE compliant. Part of our work in this course will be discussing and envisioning how data collected in Hawaiʻi can live up to the CARE principles.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#preliminary-description-of-the-data",
    "href": "datasets-intro.html#preliminary-description-of-the-data",
    "title": "1  Data we will work with",
    "section": "1.2 Preliminary description of the data",
    "text": "1.2 Preliminary description of the data\n\n1.2.1 Forest plot data\nThe forest plot data are found in data/OpenNahele_Tree_Data.csv. The data contain taxonomic identity and diameter at breast height (DBH) for 43,590 individual trees found across 530 plots. Figure 1.1 shows the locations of plots across the pae ʻāina.\n\n\n\n\n\n\n\n\nFigure 1.1: Distribution of plot locations across the pae ʻāina Hawaiʻi. Points are semi-transparent to better show plot locations when those locations are very proximate, thus darker colors represent a higher density of plots.\n\n\n\n\n\nRows in the forest plot data represent individual trees. Therefore, some data across the columns will be duplicated, for example trees from the same plot will have the same plot ID. Columns in the forest plot data are described in data/README_for_OpenNahele_Tree_Data.txt, repreduced below:\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nIsland\nIsland name\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nStudy\nBrief name of study\n\n\nPlot_area\nPlot area in m2\n\n\nLongitude\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nLatitude\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nYear\nYear in which plot data was collected\n\n\nCensus\nNumeric identifier for each census\n\n\nTree_ID\nUnique numeric identifier for each individual\n\n\nScientific_name\nGenus and species of each individual following TPL v. 1.1\n\n\nFamily\nFamily of each individual following TPL v. 1.1\n\n\nAngiosperm\nBinary variable (1 = yes, 0 = no) indicating whether an individual is classified as an angiosperm following APG III\n\n\nMonocot\nBinary variable (1 = yes, 0 = no) indicating whether an individual is classified as a monocot following APG III\n\n\nNative_Status\nCategorical variable (‘native’, ‘alien’, ‘uncertain’) indicating alien status of each individual following Wagner et al. (2005)\n\n\nCultivated_Status\nBinary variable (1 = yes, 0 = no, NA = not applicable) indicating if species is cultivated following PIER\n\n\nAbundance\nNumber of individuals (all = 1)\n\n\nAbundance_ha\nAbundance of each individual on a per hectare basis\n\n\nDBH_cm\nDiameter at 1.3 m (DBH) for each individual; NA indicates that size was not measured, but was classified by size class\n\n\n\n\n\n\n\n1.2.2 Climate data\nClimate data are found in data/plot_climate.csv. Rows here are unique plots and no information in any column is duplicated. Columns are described in data/README_for_plot_climate.txt, reproduced below:\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nlon\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nlat\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nevapotrans_annual_mm\nActual annual evapotranspiration in mm\n\n\navbl_energy_annual_wm2\nAnnual available energy in W/m^2\n\n\ncloud_freq_annual\nAnnual cloud frequency in days/year\n\n\nndvi_annual\nNormalized Difference Vegetation Index\n\n\nrain_annual_mm\nAnnual rain fall in mm\n\n\navg_temp_annual_c\nAnnual average temperature in celsius\n\n\n\n\n\n\n\n1.2.3 Human impact and geophysical data\nHuman impact data and geophysical data are found in the same file, only because they are from the same source. That file is data/hii_geo.csv. Its rows are also unique plots and no information in columns is duplicated across rows. Columns are described in `\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nlon\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nlat\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nhii\nHuman impact index\n\n\nage_yr\nGeologic substrate age in years before present\n\n\nelev_m\nElevation in meters\n\n\n\n\n\nWe won’t go deeper into describing the data now because we’ll save that for our R refresher when we’ll review coding tools by describing the data numerically and visually.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#ecological-questions-well-consider",
    "href": "datasets-intro.html#ecological-questions-well-consider",
    "title": "1  Data we will work with",
    "section": "1.3 Ecological questions we’ll consider",
    "text": "1.3 Ecological questions we’ll consider\nForests in Hawaiʻi are shaped by a multitude of processes (barton?). This is not a forest ecology class, but it is a class about using statistical models to help us answer questions in ecology and evolution. So we will engage with questions and hypotheses in ecology and evolution. The flora of Hawaiʻi has been a source of inspiration, sustenance, medicine, and scientific inquiry for millennia (abbot-paha?).\n\nKilo\nwestern tradition (eco-evo-anthro)\ninvasion interacting with evo-eco",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#references",
    "href": "datasets-intro.html#references",
    "title": "1  Data we will work with",
    "section": "1.4 References",
    "text": "1.4 References\n\n\n\n\nCraven, D. (2019). Dylancraven/hawaii_diversity: beta.\n\n\nCraven, D., Knight, T.M., Barton, K.E., Bialic-Murphy, L., Cordell, S., Giardina, C.P., et al. (2018). OpenNahele: The open hawaiian forest plot database. Biodiversity Data Journal, e28406.\n\n\nJarvis, A., Guevara, E., Reuter, H. & Nelson, A. (2008). Hole-filled SRTM for the globe: Version 4: Data grid.\n\n\nMcLean, J., Cleveland, S.B., Dodge, M., Lucas, M.P., Longman, R.J., Giambelluca, T.W., et al. (2023). Building a portal for climate data—mapping automation, visualization, and dissemination. Concurrency and Computation: Practice and Experience, 35, e6727.\n\n\nSherrod, D.R., Sinton, J.M., Watkins, S.E. & Brunt, K.M. (2007). Geologic map of the state of hawaii, sheet 3: Island of oahu. United States Geological Survey Open-File Report, 1089.\n\n\nSociety, W.C.W.C. & International Earth Science Information Network, C. for. (2005). Last of the wild project, version 2 (LWP-2): Global human footprint dataset (geographic).",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "care-fair.html",
    "href": "care-fair.html",
    "title": "2  CARE and FAIR principles",
    "section": "",
    "text": "2.1 FAIR\nCARE and FAIR are principles that specify different but, at times, complimentary ethical considerations for the collection, management, use, and governance of data (Carroll et al. 2021). The concepts underlying CARE articulate ideas of Indigenous data sovereignty and governance that have deep historical roots (Carroll et al. 2020; Carroll et al. 2021; Kukutai & Taylor 2016; UN General Assembly 2007). The concepts in FAIR relate to the more recent idea of open and reproducible science. We will discuss FAIR first, though it is not the chronologically earlier concept nor more important, because the current articulation of CARE contends with the fact that FAIR is the emerging default for data (Carroll et al. 2021).\nFAIR (Wilkinson et al. 2016) stands for Findable, Accessible, Interpretable, and Reusable. We’ll go through what each of those mean in practice and then briefly discuss the underlying ethics.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#fair",
    "href": "care-fair.html#fair",
    "title": "2  CARE and FAIR principles",
    "section": "",
    "text": "2.1.1 Findable\nThis means data and metadata should be easy to find by both humans and computers. Data should have persistent identifiers such as a digital object identifier (DOI) and have sufficient structured information such that it is searchable. As an example, the Global Biodiversity Information Facility (GBIF) houses species occurrence data (think museum and herbarium specimens) and assigns DOIs to every submitted dataset and every user download. That downloads have a DOI means that research using those downloaded data can reference the DOI, making the exact data underpinning the research finable. GBIF also enables searching for data by different criteria including taxonomy and geographic location.\n\n\n2.1.2 Accessible\nThis means that once found, data and metadata should be retrievable by both humans and computers. The method for downloading or requesting access to the (meta)data should be clear and not require specialized or proprietary tools but rather standardized, open (as in open source) protocols. For example. GBIF allows humans to select data for download through a web interface and complete the download through standard HTTP protocols. GBIF also serves an application programming interface (API) which allows data to be downloaded through automated computer workflows. Other examples of repositories with accessible data are Dryad Digital Repository and Zenodo, two resources we retrieved data from for this class.\n\n\n2.1.3 Interoperable\nData and metadata should use broadly applicable, formal data standards that are themselves FAIR. It should use standardized formats that allow data to be integrated with other datasets and used with various applications. As an example, GBIF uses the Darwin Core standard which provides a shared vocabulary for occurrence-based biodiversity (meta)data, with terms like scientificName, decimalLatitude, and eventDate. These terms can be thought of as standard column names that all data should have in order to meet the Darwin Core standard. All other data that use Darwin Core, or another standard that can be transliterated to Darwin Core, can now interoperate.\n\n\n2.1.4 Reusable\nTo be refundable data must be well-described with accurate metadata, including provenance, and have clear licensing that allows for reuse. I’m this context provenance is taken to mean the researchers who generated the data and how they generated it. In our discussion of CARE, provenance will take on a much deeper and more accurate meaning. The Darwin Core standard enables metadata to meet the reusability standard as long as the specific licence allows reuse. GBIF, for example, requires data to be licensed under a Creative Commons license. Another metadata standard worth noting is the Ecological Metadata Language which provides additional ways of describing data that do not neatly fit within the occurrence-type data most easily described by Darwin Core.\n\n\n2.1.5 Ethics of FAIR\nGlobal and local disparities in science funding access (Chen et al. 2022; Larregue & Nielsen 2024; Ma et al. 2015; Nguyen et al. 2023; Petersen 2021) means that a small proportion of researchers have dominated the means of producing data. If data were not FAIR, researchers without access to the same funding would be further disadvantaged and their contributions to science would be further lost. FAIR also came to the scientific forefront in response to the replication crisis (Nosek et al. 2015; Wilkinson et al. 2016). It is hoped that making data open and reusable will make science more transparent and reproducible (Filazzola & Cahill Jr 2021; Parker et al. 2016). Greater transiency and reproducibility should make science less prone to the analytical errors and biases that led to the replication crisis, as well as make the scientific record quicker and easier to correct when errors do occur. Much of scientific data production is also publicly funded and therefore there are ethical considerations for making data broadly accessible by everyone, public included, and reusable to ensure the highest scientific return on the investment from the public (Maglia 2015).",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#care",
    "href": "care-fair.html#care",
    "title": "2  CARE and FAIR principles",
    "section": "2.2 CARE",
    "text": "2.2 CARE\n\n\n\n\nCarroll, S., Garba, I., Figueroa-Rodrı́guez, O., Holbrook, J., Lovett, R., Materechera, S., et al. (2020). The CARE principles for indigenous data governance. Data science journal, 19.\n\n\nCarroll, S.R., Herczog, E., Hudson, M., Russell, K. & Stall, S. (2021). Operationalizing the CARE and FAIR principles for indigenous data futures. Scientific data, 8, 108.\n\n\nChen, C.Y., Kahanamoku, S.S., Tripati, A., Alegado, R.A., Morris, V.R., Andrade, K., et al. (2022). Systemic racial disparities in funding rates at the national science foundation. Elife, 11, e83071.\n\n\nFilazzola, A. & Cahill Jr, J.F. (2021). Replication in field ecology: Identifying challenges and proposing solutions. Methods in Ecology and Evolution, 12, 1780–1792.\n\n\nKukutai, T. & Taylor, J. (2016). Indigenous data sovereignty: Toward an agenda. ANU press.\n\n\nLarregue, J. & Nielsen, M.W. (2024). Knowledge hierarchies and gender disparities in social science funding. Sociology, 58, 45–65.\n\n\nMa, A., Mondragón, R.J. & Latora, V. (2015). Anatomy of funded research in science. Proceedings of the National Academy of Sciences, 112, 14760–14765.\n\n\nMaglia, A. (2015). NSF data management and public access initiatives.\n\n\nNguyen, M., Gonzalez, L., Chaudhry, S.I., Ahuja, N., Pomahac, B., Newman, A., et al. (2023). Gender disparity in national institutes of health funding among surgeon-scientists from 1995 to 2020. JAMA network open, 6, e233630–e233630.\n\n\nNosek, B.A., Alter, G., Banks, G.C., Borsboom, D., Bowman, S.D., Breckler, S.J., et al. (2015). Promoting an open research culture. Science, 348, 1422–1425.\n\n\nParker, T.H., Forstmeier, W., Koricheva, J., Fidler, F., Hadfield, J.D., Chee, Y.E., et al. (2016). Transparency in ecology and evolution: Real problems, real solutions. Trends in Ecology & Evolution, 31, 711–719.\n\n\nPetersen, O.H. (2021). Inequality of research funding between different countries and regions is a serious problem for global science. Function, 2, zqab060.\n\n\nUN General Assembly. (2007). United nations declaration on the rights of indigenous peoples, 12, 1–18.\n\n\nWilkinson, M.D., Dumontier, M., Aalbersberg, Ij.J., Appleton, G., Axton, M., Baak, A., et al. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific data, 3, 1–9.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "r-refresh.html",
    "href": "r-refresh.html",
    "title": "3  R refresher",
    "section": "",
    "text": "3.1 Resources\nhttps://datacarpentry.github.io/R-ecology-lesson/\nbut note: they use %&gt;% and load entire tidyverse",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "4  Managing code with git and GitHub",
    "section": "",
    "text": "4.1 What is git and GitHub and how do they help us?",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#using-git-and-github-with-help-from-rstudio",
    "href": "github.html#using-git-and-github-with-help-from-rstudio",
    "title": "4  Managing code with git and GitHub",
    "section": "4.2 Using git and GitHub with help from RStudio",
    "text": "4.2 Using git and GitHub with help from RStudio\nTo start working with git and GitHub we will need to make a free GitHub account and to install the actual git software on our computers. Here are external instructions for those tasks:\n\nmake a free GitHub account; no need to follow the “Next steps” (although eventually you should set up 2FA)\ndownloaded and installed git (click on your operating system here for instruction)\n\nInstead of step (2), if you think you have done this before, check if git is already on your machine by opening the terminal (from RStudio is fine) and run this command:\ngit --version\nYou will see a version number if git is available, you will see an error if it is not.\nNow we can integrate git, GitHub, and RStudio. This is not necessary for using git and GitHub but it makes the experience easier, especially because we are already using RStudio.\nWe will use two R packages to help us connect RStudio and GitHub: usethis and gitcreds. Let’s install those packages by typing the following in the R console\n\ninstall.packages(\"usethis\")\ninstall.packages(\"gitcreds\")\n\nA common problem has been that usethis has a dependency on the package crayon which is currently not playing nice. If you have errors involving crayon try this:\n\ninstall.packages(\"crayon\", type = \"source\")\ninstall.packages(\"usethis\")\n\nNow we can use functions from those packages to help us continue. First we use usethis to set up a personal access tokin that will serve as our login credential with GitHub. We do that by typing this into the R console:\n\nlibrary(usethis)\ncreate_github_token()\n\nThat should open up a web browser page where you might be promoted to log into your GitHub account. Do that and then on the next page scroll to the bottom and click the “Generate token” button. You will then see on a new page a string of numbers and letters—that’s the token. Copy that token and paste it somewhere temporarily, like a blank text editor document or a blank R script, where you paste it doesn’t matter—you’ll soon delete it.\nNow come back to RStudio and run the following in the R console:\n\nlibrary(gitcreds)\ngitcreds_set()\n\nThis will prompt you to enter a password or token; now you can paste the token you just generated here and hit enter. You should now be good to go!\nIt’s possible you’ve already gone through this process. If that’s the case you’ll see this:\n-&gt; Your current credentials for 'https://github.com':\n\n  protocol: https\n  host    : github.com\n  username: PersonalAccessToken\n  password: &lt;-- hidden --&gt;\n\n-&gt; What would you like to do? \n\n1: Keep these credentials\n2: Replace these credentials\n3: See the password / token\n\nSelection: \nEnter 1 for your selection. There might be be an error message but you can ignore it.\nNow we should hopefully be all connected!\n\n4.2.1 Testing our connection\nNow we’ll double check that you’ve connected RStudio with GitHub by going to our GitHub profile and making a new repository. We’ll then copy (i.e. “clone”) this repo to our computers, modify it locally, and send those changes back to GitHub via git (all this sounds overly technical right now, we will gain a deeper understanding in the next section on learning the git workflow).\nFirst let’s make a new repo. Go to your GitHub profile and click the plus sign in the top right, select “New repository”.\n\nGive the repository a name, a description, make it public and select the option to add a README. Then hit “Create repository”\n\nNext hit the green “Code” button and copy the HTTPS URL to your clipboard.\n\nNow head back to RStudio and selection File &gt; New Project. Then select “Version control” then “Git”. Finally, paste the HTTPS URL that you copied from GitHub. The “Project directory name” should auto-population. Choose a meaningful place on your computer to house this project, select the option to open in a new session and hit “Create Project”.\n\nYou should now have a new project open! Navigate to the “Git” tab in RStudio and notice that two file names are listed with yellow question marks by them.\n\nThose files were auto-generated by RStudio. The question marks indicate that they are new files which git is currently not tracking. Click the radio button next to each one under the “Staged” column. Checking those buttons stages the files. We can now hit “Commit” to add a commit message and commit the changes. If you have success it should look something like this (possibly with a warning about username or email, that’s ok!):\n\nOnce we’ve committed the changes we can send those changes to GitHub by hitting the “push” button Hopefully the result of hitting “push” looks something like this (possibly with a warning about username or email, again, that’s ok!):\n\nGo back to your web browser, refresh the GitHub repo page, and you should see the new files you just pushed!\nWhile committing and pushing you might get warning messages about your username and email. If you see these warnings you need to let git and GitHub know who you are. You can do this through the Terminal. To open a terminal tab through RStudio go to Tools &gt; Terminal &gt; New Terminal. This will open a new terminal tab next to the R Console tab. The terminal is different than the R Console. You can think of the terminal as accessing the guts of your computer. Raw R commands will not work here. However, we can interact with git here and that’s what we need to do to identify ourselves. So in the terminal type\ngit config --global user.name \"github_user_name\"\nreplace github_user_name with your actual username; hit enter. Then type\ngit config --global user.email \"my_email\"\nagain replace my_email with the actual email address you used to register your GitHub account; hit enter.\nNow your identity is known and you should be all set!",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#learning-the-git-workflow",
    "href": "github.html#learning-the-git-workflow",
    "title": "4  Managing code with git and GitHub",
    "section": "4.3 Learning the git workflow",
    "text": "4.3 Learning the git workflow\n\n4.3.1 Stage and commit\n\n\n4.3.2 Push and pull",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#working-collaboratively-branches-forks-and-pull-requests",
    "href": "github.html#working-collaboratively-branches-forks-and-pull-requests",
    "title": "4  Managing code with git and GitHub",
    "section": "4.4 Working collaboratively: branches, forks, and pull requests",
    "text": "4.4 Working collaboratively: branches, forks, and pull requests",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Probability",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Probability"
    ]
  },
  {
    "objectID": "ml-method.html",
    "href": "ml-method.html",
    "title": "14  How likelihood maximization works",
    "section": "",
    "text": "should bring up ward intervals here\nmaybe not about maximization at all\nmaybe about uncertainty",
    "crumbs": [
      "Introducing the method of maximum likelihood",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>How likelihood maximization works</span>"
    ]
  }
]