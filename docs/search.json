[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Statistical Modeling",
    "section": "",
    "text": "Welina mai!\nThis is the course website for ZOOL 631: Intro to Statistical Modeling at the University of Hawaiʻi at Mānoa.\nBeing in Hawaiʻi we will center this place in learning about the conceptual, practical, and broader societal aspects of analyzing data with statistical models. Centering Hawaiʻi means acknowledging what our presence here means. For each of us that meaning is probably unique. Speaking personally as me, Andy, your instructor, I am a haole settler in Hawaiʻi. From that position I adapt and shorten the Land Acknowledgement example presented by the Hawaiian Place of Learning Advancement Office: This ʻāina on which our University resides is part of the larger territory recognized by Kānaka ʻŌiwi as their ancestral grandmother, Papahānaumoku. Her majesty Queen Liliʻuokalani yielded the Hawaiian Kingdom and these territories under duress and protest to the United States to avoid the bloodshed of her people. Hawaiʻi remains illegally occupied by the United States. This acknowledgement is offered fully recognizing that acknowledgement is only a first step and insufficient by itself. This course deals deeply with the intersection of statistics and colonialism in Hawaiʻi and globally. To that end we also display the Local Contexts Open to Collaborate Notice (which we will learn more about soon).",
    "crumbs": [
      "Welina mai!"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Intro to Statistical Modeling",
    "section": "Learning Goals",
    "text": "Learning Goals\nEach module contains three types of learning objectives: i) conceptual foundations, ii) applications, iii) societal contexts and ethics. Conceptual foundations will help you understand how and why we apply statistics to the real world and also help you understand and evaluate the validity of the scientific literature. Applications is where you learn how to actually make these tools work for you. Societal contexts and ethics are all too often overlooked in the teaching and practice of statistics, but ultimately statistics is a central tool in how western scientists reach conclusions about the world; we have a mandate to make sure our conclusions, and the very process we take to make those conclusions, is relevant and pono.\nWe will use the R programming language as our primary tool for doing statistics. We will focus our statistical learning on the concept of likelihood and the method of Maximum Likelihood Estimation. This is a powerful approach that unifies many seemingly disparate statistical methods from logistic regression to ANOVA to contingency analysis. Likelihood methods focus on building statistical models of how we hypothesize the world works and using data to test whether those models are supported. Unlike the toy examples and cookbook style methods we often encounter in introductory statistics courses, likelihood methods will be your friend for a wide range of challenges from the simple to the complex. We will not abandon methods you may have encountered in past classes like least squares regression or t-tests, but rather show how those emerge as special cases of the more general underlying likelihood framework.",
    "crumbs": [
      "Welina mai!"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Intro to Statistical Modeling",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nThis code of conduct is for students and instructors of this course at UH Mānoa. A code of conduct for curriculum collaborators will be available after June 2026.\nWe will create a learning environment that is safe, welcoming, and supportive of individual needs. This conduct is in part mandated by UH Mānoa policy. Beyond mandates, we seek to embrace this ethic as an imperative to make the work we do as academics, researchers, teachers, and community members more just and inclusive. We commit ourselves to not discriminating against or harassing our fellow learners based on any aspect of their identities and to celebrate differences as having individual value and collective energy for creativity and good. At the same time, we commit to acknowledging that power imbalances and differences in privilege are attached to diversity by our society and by individuals. We will seek to counter power imbalances and privilege by actively listening to one another’s experiences and meeting needs that arise from those experiences. In this course there will be active sharing of ideas and work for the purposes of peer-to-peer learning, peer review, and collaboration. Our commitment to creating a safe, welcoming, and supportive learning environment is key to this type of learning. We cannot allow bullying or condescending each other about anything, including the ideas and work we share. All learners bear responsibility to uphold this conduct, but as the instructor, Andy Rominger is ultimately responsible for fostering this learning environment through modeling this code of conduct and maintaining adherence to it. Please communicate deviations from this code of conduct to Andy. Recognizing that your instructor may himself deviate from this code of conduct, please make use of formal recourse and support options available from UH Mānoa if need arises.",
    "crumbs": [
      "Welina mai!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introductory material",
    "section": "",
    "text": "Here we will introduce the data we will work with throughout this course. In the context of data we will discuss ethical issues of Indigenous data sovereignty and reproducible science. We will also review the necessary R and version control knowledge for succeeding in this course. Such knowledge will include data structures in R, data input and output, data manipulation, visualization, quarto dynamic documents, and using GitHub.\nJump straight to:\n\nLecture: Data we will work with\nLecture: CARE and FAIR principles\nLab and lecture: R refresher\nLab and lecture: Managing code with git and GitHub\nLab and lecture: Dynamic documents with quarto\nLab: Brining it all together",
    "crumbs": [
      "Introductory material"
    ]
  },
  {
    "objectID": "datasets-intro.html",
    "href": "datasets-intro.html",
    "title": "1  Data we will work with",
    "section": "",
    "text": "1.1 Intorducing the data\nWe will be working with data from forest monitoring plots across the pae ʻāina Hawaiʻi as well as environmental and geophysical data measured or interpolated at these monitoring plots. The forest monitoring plot data are organized and distributed in a data resource called OpenNahele (Craven et al. 2018).\nIn this context “open” refers to the data being openly shared and accessible in alignment with the FAIR principles which we discuss in the next section. Nahele in ʻōlelo Hawaiʻi means, in this context, forest. The use of ʻōlelo Hawaiʻi points to the fact that the data pertain to the pae ʻāina Hawaiʻi. This naming choice could show respect and acknowledgement of the Hawaiian provenance of these data, but could also be seen as appropriation. The interpretation of the naming choice depends in part on whether the data not only comply with FAIR, but also the CARE principles, which, again, we discuss in the next section. The CARE principles mandate that Indigenous provenance and governance rights are acknowledged and respected in the stewardship of Indigenous data. The data in OpenNahele originate from the pae ʻāina Hawaiʻi and thus within the traditional lands of the Kanaka ʻŌiwi, making them Indigenous data. Without attribution, or any indication about ʻŌiwi protocols and rights for data collection and use, OpenNahele does not meet the CARE principles. While the good intentions of those distributing the data is not being questioned, we also recognize there is opportunity for making right the stewardship of these, and other, data from Hawaiʻi. We dive much deeper into this discussion in the next section on CARE and FAIR.\nThe environmental and geophysical data we will use include climate variables, information about human impact, elevation, and geologic age of substrates at the locations of the plot data. For climate variables we use the Hawaiʻi Climate Data Portal (McLean et al. 2023). Elevation, geologic age, and human impact data have been organized by the authors of the OpenNahele data set in a separate data publication (Craven 2019) and we will use those already compiled data rather than re-gather them from the primary sources. However, for the sake of completeness, the primary sources are as follows: elevation (Jarvis et al. 2008), substrate age (Sherrod et al. 2007), and human impact (Society & International Earth Science Information Network 2005).\nWhile these environmental and geophysical data meet the FAIR principles, they again are not CARE compliant. Part of our work in this course will be discussing and envisioning how data collected in Hawaiʻi can live up to the CARE principles.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#preliminary-description-of-the-data",
    "href": "datasets-intro.html#preliminary-description-of-the-data",
    "title": "1  Data we will work with",
    "section": "1.2 Preliminary description of the data",
    "text": "1.2 Preliminary description of the data\nWe won’t go deep into visualizing and numerically describing the data here because we’ll save that for our R refresher when we’ll review coding tools by working with these data.\n\n1.2.1 Forest plot data\nThe forest plot data are found in data/OpenNahele_Tree_Data.csv. The data contain taxonomic identity and diameter at breast height (DBH) for 43,590 individual trees found across 530 plots. Figure 1.1 shows the locations of plots across the pae ʻāina.\n\n\n\n\n\n\n\n\nFigure 1.1: Distribution of plot locations across the pae ʻāina Hawaiʻi. Points are semi-transparent to better show plot locations when those locations are very proximate, thus darker colors represent a higher density of plots.\n\n\n\n\n\nRows in the forest plot data represent individual trees. Therefore, some data across the columns will be duplicated, for example trees from the same plot will have the same plot ID. Columns in the forest plot data are described in data/README_for_OpenNahele_Tree_Data.txt, reproduced below:\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nIsland\nIsland name\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nStudy\nBrief name of study\n\n\nPlot_area\nPlot area in m2\n\n\nLongitude\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nLatitude\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nYear\nYear in which plot data was collected\n\n\nCensus\nNumeric identifier for each census\n\n\nTree_ID\nUnique numeric identifier for each individual\n\n\nScientific_name\nGenus and species of each individual following TPL v. 1.1\n\n\nFamily\nFamily of each individual following TPL v. 1.1\n\n\nAngiosperm\nBinary variable (1 = yes, 0 = no) indicating whether an individual is classified as an angiosperm following APG III\n\n\nMonocot\nBinary variable (1 = yes, 0 = no) indicating whether an individual is classified as a monocot following APG III\n\n\nNative_Status\nCategorical variable (‘native’, ‘alien’, ‘uncertain’) indicating alien status of each individual following Wagner et al. (2005)\n\n\nCultivated_Status\nBinary variable (1 = yes, 0 = no, NA = not applicable) indicating if species is cultivated following PIER\n\n\nAbundance\nNumber of individuals (all = 1)\n\n\nAbundance_ha\nAbundance of each individual on a per hectare basis\n\n\nDBH_cm\nDiameter at 1.3 m (DBH) for each individual; NA indicates that size was not measured, but was classified by size class\n\n\n\n\n\n\n\n1.2.2 Climate data\nClimate data are found in data/plot_climate.csv. Rows here are unique plots and no information in any column is duplicated. Columns are described in data/README_for_plot_climate.txt, reproduced below:\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nlon\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nlat\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nevapotrans_annual_mm\nActual annual evapotranspiration in mm\n\n\navbl_energy_annual_wm2\nAnnual available energy in W/m^2\n\n\ncloud_freq_annual\nAnnual cloud frequency in days/year\n\n\nndvi_annual\nNormalized Difference Vegetation Index\n\n\nrain_annual_mm\nAnnual rain fall in mm\n\n\navg_temp_annual_c\nAnnual average temperature in celsius\n\n\n\n\n\n\n\n1.2.3 Human impact and geophysical data\nHuman impact data and geophysical data are found in the same file, only because they are from the same source. That file is data/hii_geo.csv. Its rows are also unique plots and no information in columns is duplicated across rows. Columns are described in data/README_for_hii_geo.txt, reproduced below:\n\n\n\n\n\n\n\n\n\nColumn label\nColumn description\n\n\n\n\nPlotID\nUnique numeric identifier for each plot\n\n\nlon\nLongitude of plot in decimal degrees; WGS84 coordinate system\n\n\nlat\nLatitude of plot in decimal degrees; WGS84 coordinate system\n\n\nhii\nHuman impact index\n\n\nage_yr\nGeologic substrate age in years before present\n\n\nelev_m\nElevation in meters\n\n\n\n\n\n\n\n1.2.4 Biocultural dimensions\nLāʻau are found in the Kumulipo, an ʻŌiwi creation chant, and many significant genealogical moʻolelo such as that of Hāloanakalaukapalili (the first kalo) and Hāloa (his younger brother, the first Hawaiian), bringing the plants we’ll work with into the moʻokūʻauhau of Kānaka ʻŌiwi. The author (me Andy your instructor) is not trained to understand these relationships but follows the lead of ʻŌiwi scholars and their work such as Hutchins et al. (2023) and Goldberg-Hiller & Silva (2015). Genealogical relations require deep regard and respect.\nWestern science might use the term “biocultural” for the connections between human and non-human aspects of the world. In addition to “biocultural”1 moʻokūʻauhau, there are at least two other “biocultural” dimensions to consider.\n\nLāʻau lapaʻau and ethnobotanical connections to plants\nWahi pana and different categories of wao (e.g. wao akua) along with other ʻōiwi geographies that carry specific cultural protocols and sensitivities\n\nIn regard to lāʻau lapaʻau and ethnobotany, the Bernice Pauahi Bishop Museum provides a Hawaiian ethnobotany database built largely on the work of esteemed ʻŌiwi scholar Isabella Aiona Abbott (1992). This database is neither CARE nor FAIR compliant (not even providing full bibliographic information to fully credit Abbott (1992)) so we will not linger on it except to compare the plant names listed as ethnobotanically significant with plant names in the data we will work with. Doing so reveals that in the data set we will work with, 88% of individual trees and 56% of unique taxa have ethnobotanical significance.\nA similar analysis of the geographic overlap of wahi pana and wao with the coordinates of plot data is not as simply completed. Different wao have specific geographic boundaries and carry different cultural protocols, knowledge held by cultural practitioners and not always interoperated with western cartography [which would allow analysis with plot data; but see Winter & Lucas (2017)]. However, there is a working assumption among some cultural and natural resource practitioners in Hawaiʻi that the wao akua corresponds roughly to high elevation, native forest areas. This is not universally agreed upon but in the current context could have utility as a heuristic. What constitutes native forest is open for debate, and any choice comes with inaccuracies. However, we could ask what percent of plots are composed of at least three quarters native trees. The answer is 70%. So perhaps 70% of plots are located in wao akua across the pae ʻāina. The wao akua is a realm with strict cultural protocols relating to its kapu status (Winter et al. 2018), protocols that were likely rarely if ever met when collecting these data.\nMany—perhaps most—wahi pana are recorded in written Hawaiian language sources (e.g. nūpepa) that are cataloged in the Papakilo Database. However, Andy is not proficient enough in the Papakilo Database nor ʻōlelo Hawaiʻi nor place names across the entire pae ʻāina to analyze plot coordinates in relation to wahi pana, at least for now. But we should assume that many plots are located in or near wahi pana.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#ecological-questions-well-consider",
    "href": "datasets-intro.html#ecological-questions-well-consider",
    "title": "1  Data we will work with",
    "section": "1.3 Ecological questions we’ll consider",
    "text": "1.3 Ecological questions we’ll consider\nThe flora of Hawaiʻi has been a source of inspiration, sustenance, medicine, and scientific inquiry for millennia (Abbott 1992). Kānaka ʻŌiwi produced, shared, and continue to produce the ancestral knowledge needed to understand the flora of Hawaiʻi and its connect to humans. ʻŌiwi scientists do so using ʻŌiwi methodologies (for one compilation see Thapar 2013) grounded in epistemologies that at times share parallels with western epistemologies Keawe (2008) and at other times do not (Handy & Pukui 2058). ʻŌiwi methodologies are self-evidently rigorous and deserve equal respect with western methodologies. Using ancestral knowledge and Indigenous methodologies, Kānaka ʻŌiwi sustained large populations over long periods of time with effectively zero external resource input, in contrast to the current situation in Hawaiʻi of an unsustainable population, not much larger than pre-contact ʻŌiwi populations, dependent on external inputs for 90% of our needs (Gon III et al. 2018). ʻŌiwi methodologies should be learned but this class could not possibly do that justice. Therefore this class stays within western epistemologies, not out of a sense of superiority, but of positionality.\nFrom a western perspective, forests in Hawaiʻi are shaped by a multitude of processes (Barton et al. 2021). This is not a forest ecology class, but it is a class about using statistical models to help us answer questions in ecology and evolution. So we will engage with questions and hypotheses in ecology and evolution. We will foreshadow some of our future analyses by highlighting a few interesting drivers of and processes underlying plant biodiversity in Hawaiʻi.\nTwo striking features about the flora of Hawaiʻi are 1) it is abundant with diverse and unique forms of plant life; and 2) it is highly modified by human activities, notably pervasive and numerous invasive or non-native species brought by settler colonialism. Many students may not be plant people, e kala mai, so it is worth saying that neither, really, is Andy, and the two striking features about the flora of Hawaiʻi are true for the larger biota of Hawaiʻi. While the unique physiological and demographic mechanisms underlying the flora are surely different from the fauna, the courser ecological and evolutionary processes likely share some generalities.\nOf those possible generalities, we will look into a couple—with statistical models—throughout this class:\n\nHow have ecology and evolution played out across the multi-million year chronosequence of the pae ʻāina to shape differences in biodiversity across and within islands (investigated, for example, by Gillespie 2016; Rominger et al. 2016)?\nHow have human impacts and climatic variables modulated biodiversity (investigated, for example, by Lim et al. 2021; Hutchins et al. 2023)\nCan data on human impacts and climate help us understand how invasive/non-native species occupy ecosystems (investigated, for example, by Blackburn et al. 2011; Fortini et al. 2013)",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#references",
    "href": "datasets-intro.html#references",
    "title": "1  Data we will work with",
    "section": "1.4 References",
    "text": "1.4 References\n\nlibrary(quarto)\n\nsystem(\"rm -fr docs/data-slides\")\nsystem(\"mkdir docs/data-slides\")\n\nquarto_render(input = \"slides/data-slides/data-slides.qmd\", \n              output_file = \"index.html\",\n              quiet = FALSE)\n\nsystem(\"cp -r slides/data-slides docs\")\n\n\n\n\n\n\n\n\nAbbott IA. 1992. Lāʻau Hawaiʻi: Traditional hawaiian uses of plants. Bishop Museum Press.\n\n\nBarton KE et al. 2021. Hawai‘i forest review: Synthesizing the ecology, evolution, and conservation of a model system. Perspectives in Plant Ecology, Evolution and Systematics 52:125631. Elsevier.\n\n\nBlackburn TM, Pyšek P, Bacher S, Carlton JT, Duncan RP, Jarošı́k V, Wilson JR, Richardson DM. 2011. A proposed unified framework for biological invasions. Trends in ecology & evolution 26:333–339. Elsevier.\n\n\nCraven D. 2019, June. Dylancraven/hawaii_diversity: beta. Zenodo. Available from https://doi.org/10.5281/zenodo.3250638.\n\n\nCraven D, Knight TM, Barton KE, Bialic-Murphy L, Cordell S, Giardina CP, Gillespie TW, Ostertag R, Sack L, Chase JM. 2018. OpenNahele: The open hawaiian forest plot database. Biodiversity Data Journal:e28406.\n\n\nFortini LB et al. 2013. A landscape-based assessment of climate change vulnerability for all native hawaiian plants. University of Hawaii.\n\n\nGillespie RG. 2016. Island time and the interplay between ecology and evolution in species diversification. Evolutionary applications 9:53–73. Wiley Online Library.\n\n\nGoldberg-Hiller J, Silva NK. 2015. The botany of emergence: Kanaka ontology and biocolonialism in hawai’i. Native American and Indigenous Studies 2:1–26. JSTOR.\n\n\nGon III SM, Tom SL, Woodside U. 2018. ʻĀina momona, honua au loli—productive lands, changing world: Using the hawaiian footprint to inform biocultural restoration and future sustainability in hawai ‘i. Sustainability 10:3420. MDPI.\n\n\nHandy EC, Pukui MK. 2058. The polynesian family system in ka-ʻU, Hawaiʻi. Polynesian Society.\n\n\nHutchins L, Mc Cartney A, Graham N, Gillespie R, Guzman A. 2023. Arthropods are kin: Operationalizing indigenous data sovereignty to respectfully utilize genomic data from indigenous lands. Molecular Ecology Resources 25:e13822. Wiley Online Library.\n\n\nJarvis A, Guevara E, Reuter H, Nelson A. 2008. Hole-filled SRTM for the globe: Version 4: Data grid.\n\n\nJohnson RK. 1981. Kumulipo, the hawaiian hymn of creation. Topgallant Publishing Company.\n\n\nKeawe LO. 2008. Ki’i papalua: Imagery and colonialism in hawai’i. PhD thesis. University of Hawaii at Manoa.\n\n\nLim JY, Patiño J, Noriyuki S, Cayetano L, Gillespie RG, Krehenwinkel H. 2021. Semi-quantitative metabarcoding reveals how climate shapes arthropod community assembly along elevation gradients on hawaii island. Molecular Ecology 31:1416–1429. Wiley Online Library.\n\n\nMcLean J, Cleveland SB, Dodge M, Lucas MP, Longman RJ, Giambelluca TW, Jacobs GA. 2023. Building a portal for climate data—mapping automation, visualization, and dissemination. Concurrency and Computation: Practice and Experience 35:e6727. Wiley Online Library.\n\n\nRominger A et al. 2016. Community assembly on isolated islands: Macroecology meets evolution. Global ecology and biogeography 25:769–780. Wiley Online Library.\n\n\nSherrod DR, Sinton JM, Watkins SE, Brunt KM. 2007. Geologic map of the state of hawaii, sheet 3: Island of oahu. United States Geological Survey Open-File Report 1089.\n\n\nSociety WCWC, International Earth Science Information Network C for. 2005. Last of the wild project, version 2 (LWP-2): Global human footprint dataset (geographic).\n\n\nThapar R. 2013. The past before us. Harvard University Press.\n\n\nWinter KB, Beamer K, Vaughan MB, Friedlander AM, Kido MH, Whitehead AN, Akutagawa MK, Kurashima N, Lucas MP, Nyberg B. 2018. The moku system: Managing biocultural resources for abundance within social-ecological regions in Hawaiʻi. Sustainability 10:3554. MDPI.\n\n\nWinter KB, Lucas M. 2017. Spatial modeling of social-ecological management zones of the ali’i era on the island of kaua’i with implications for large-scale biocultural conservation and forest restoration efforts in hawai’i. Pacific Science 71:457–477. University of Hawai’i Press.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "datasets-intro.html#footnotes",
    "href": "datasets-intro.html#footnotes",
    "title": "1  Data we will work with",
    "section": "",
    "text": "“Biocultural” is in quotes here to indicate the deficiency of the term in this context and especially when grammatically attached to moʻokūʻauhau↩︎",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data we will work with</span>"
    ]
  },
  {
    "objectID": "care-fair.html",
    "href": "care-fair.html",
    "title": "2  CARE and FAIR principles",
    "section": "",
    "text": "2.1 FAIR\nCARE and FAIR are principles that specify different but complimentary ethical considerations for the collection, management, use, and governance of data (Carroll et al. 2021). The concepts underlying CARE articulate ideas of Indigenous data sovereignty and governance that have deep historical roots (UN General Assembly 2007; Kukutai & Taylor 2016; Carroll et al. 2020; Carroll et al. 2021). The concepts in FAIR relate to the more recent idea of open and reproducible science. We will discuss FAIR first, though it is not the chronologically earlier concept nor more important, because the current articulation of CARE contends with the fact that FAIR is the emerging default for data (Carroll et al. 2021).\nFAIR (Wilkinson et al. 2016) stands for Findable, Accessible, Interpretable, and Reusable. We’ll go through what each of those mean in practice and then briefly discuss the underlying ethics.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#fair",
    "href": "care-fair.html#fair",
    "title": "2  CARE and FAIR principles",
    "section": "",
    "text": "2.1.1 Findable\nThis means data and metadata should be easy to find by both humans and computers. Data should have persistent identifiers such as a digital object identifier (DOI) and have sufficient structured information such that it is searchable. As an example, the Global Biodiversity Information Facility (GBIF) houses species occurrence data (think museum and herbarium specimens) and assigns DOIs to every submitted data set and every user download. That downloads have a DOI means that research using those downloaded data can reference the DOI, making the exact data underpinning the research finable. GBIF also enables searching for data by different criteria including taxonomy and geographic location.\n\n\n2.1.2 Accessible\nThis means that once found, data and metadata should be retrievable by both humans and computers. Importantly, accessible in this context means metadata are always accessible, even if raw data cannot be accessed (e.g. for ethical reasons). The method for downloading or requesting access to (meta)data should be clear and not require specialized or proprietary tools but rather standardized, open (as in open source) protocols. For example. GBIF allows humans to select data for download through a web interface and complete the download through standard HTTP protocols. GBIF also serves an application programming interface (API) which allows data to be downloaded through automated computer workflows. Other examples of repositories with accessible data are Dryad Digital Repository and Zenodo, two resources we retrieved data from for this class.\n\n\n2.1.3 Interoperable\nData and metadata should use broadly applicable, formal data standards that are themselves FAIR. It should use standardized formats that allow data to be integrated with other data sets and used with various applications. As an example, GBIF uses the Darwin Core standard which provides a shared vocabulary for occurrence-based biodiversity (meta)data, with terms like scientificName, decimalLatitude, and eventDate. These terms can be thought of as standard column names that all data should have in order to meet the Darwin Core standard. All other data that use Darwin Core, or another standard that can be transliterated to Darwin Core, can now interoperate.\n\n\n2.1.4 Reusable\nTo be refundable data must be well-described with accurate metadata, including provenance, and have clear licensing that allows for reuse. In this context provenance is taken to mean the researchers who generated the data and how they generated it. In our discussion of CARE, provenance will take on a much deeper and more accurate meaning. The Darwin Core standard enables metadata to meet the reusability standard as long as the specific licence allows reuse. GBIF, for example, requires data to be licensed under a Creative Commons license. Another metadata standard worth noting is the Ecological Metadata Language which provides additional ways of describing data that do not neatly fit within the occurrence-type data most easily described by Darwin Core.\n\n\n2.1.5 Ethics of FAIR\nGlobal and local disparities in science funding access (Ma et al. 2015; Petersen 2021; Chen et al. 2022; Nguyen et al. 2023; Larregue & Nielsen 2024) means that a small proportion of researchers have dominated the means of producing data. If data were not FAIR, researchers without access to the same funding would be further disadvantaged and their contributions to science would be further lost. FAIR also came to the scientific forefront in response to the replication crisis (Nosek et al. 2015; Wilkinson et al. 2016). It is hoped that making data open and reusable will make science more transparent and reproducible (Parker et al. 2016; Filazzola & Cahill Jr 2021). Greater transiency and reproducibility should make science less prone to the analytical errors and biases that led to the replication crisis, as well as make the scientific record quicker and easier to correct when errors do occur. Most scientific data is also publicly funded and therefore there are ethical considerations for making data broadly accessible by everyone, public included, and reusable to ensure the highest scientific return on the investment from the public (Stebbins 2013; Maglia 2015).",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#care",
    "href": "care-fair.html#care",
    "title": "2  CARE and FAIR principles",
    "section": "2.2 CARE",
    "text": "2.2 CARE\nWhile ethics are implied in FAIR, CARE intentionally addresses them head-on in the context of Indigenous peoples’ rights (UN General Assembly 2007; Carroll et al. 2020; Carroll et al. 2021). CARE is an articulation of Indigenous data sovereignty (IDSov) and Indigenous data governance (IDGov). IDSov and IDGov apply to Indigenous data. Indigenous data are any form of data (including traditional knowledge, language, physical materials, digital records, and more) in any format that originate from or impact Indigenous peoples, people, communities, nations, territories (including traditional territories now occupied by colonizers), and all their constituent pieces from natural “resources” to human bodies to cultural expression (Carroll et al. 2021).\nWhere data “originate from” is a way of saying what is the provenance of the data. We saw provenance when discussing the necessity of clearly describing data under FAIR. From a colonial perspective the provenance of data sits with the researcher extracting data. But a de-colonial perspective reveals that the provenance sits with the original knowledge holders and/or stewards of the sources (e.g. lands and waters) of knowledge. The provenance of an academic recording of ʻōlelo Hawaiʻi spoken by a mānaleo does not sit with the academic but with the speaker and more broadly with Kānaka ʻŌiwi. The provenance of an entomological collection from the Waiʻanae Mountains on Oʻahu does not sit with the collector (whose name is likely attached to each specimen) but with the ʻāina, the lāhui Hawaiʻi, and Kānaka ʻŌiwi. The distinction about provenance is important because, under western intellectual property systems, provenance and ownership are assumed to be equivalent. Who “owns” the data governs the data. IDSov and IDGov work to recenter Indigenous worldviews, rights, self-determination, and wellbeing in data stewardship, rather than western notions of property.\nWe will start to build understanding of IDSov and IDGov by defining CARE. CARE (GIDA 2019; Carroll et al. 2020) stands for Collective benefit, Authority to control, Responsibility, and Ethics. We will dive deeper into each.\n\n\n\n\n\nimage credit: Carroll et al. (2020)\n\n\n\n\n\n2.2.1 Collective benefit\nData inherently hold potential for innovation, value generation, and decision making. People, communities, and nations can benefit from data. The collective benefit principle stipulates that benefits should be shared collectively and specifically that data stewardship practices must enable Indigenous peoples and people to derive benefit from Indigenous data. Benefit is sometimes—in western worldviews—reflexively equated with monetary value, but benefit takes many forms including the benefit of using data to inform governance and decision making, and generally to promote wellbeing.\n\n\n2.2.2 Authority to control\nIndigenous peoples have a right to govern how Indigenous data are collected, used, managed, and shared. This means that cultural protocols from Indigenous people(s) are prioritized in the stewardship of data. Authority to control can be enabled by FAIR principles if those principles are applied to enable Indigenous peoples to access their data and use it advance their right to self-determination.\n\n\n2.2.3 Responsibility\nThose working with Indigenous data have a responsibility to engage with respect, reciprocity, and earned trust. Respect must be given to cultural protocols and rights to self-determination. Reciprocity is about ensuring that not only the benefits derived from data can advance Indigenous peoples’ wellbeing, but the process of data collection and stewardship have embedded practices of collaboration that expand the capacity (e.g. through increased data literacy) of Indigenous peoples and people to engage with data. Reciprocity and respect are also about seeking mutual understanding of worldviews and ensuring that data and their products are communicated in a way consistent with those worldviews, including in the Indigenous language of the people of provenance. Earned trust means those working with Indigenous data embrace the responsibility of collaborating with integrity and consistently support positive relationships.\n\n\n2.2.4 Ethics\nEthics means centering the rights, wellbeing, and ethical frameworks of Indigenous Peoples in the stewardship of Indigenous data. To do so requires grappling with the ways colonialism and colonists create power imbalances, scarcities (real and constructed), and trauma. This work is not trivial, which also means ethics include honoring that work by ensuring the longevity of data and its potential to produce benefits into the future. Indicating Indigenous provenance is part of ensuring future ethical use.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#care-and-fair-together",
    "href": "care-fair.html#care-and-fair-together",
    "title": "2  CARE and FAIR principles",
    "section": "2.3 CARE and FAIR together",
    "text": "2.3 CARE and FAIR together\nStephanie Carroll and colleagues (2021), leaders in IDSov and IDGov, point out that FAIR principles can support operationalizing CARE and vice versa. Indigenous data are all too often rendered hidden, inaccessible, not reusable, and not interoperable by colonial power imbalances and institutions (Carroll et al. 2021). Making such data FAIR is necessary in these cases to begin the process of implementing CARE. From personal experience (Andy speaking here) CARE can be perceived by some researchers in colonizer institutions to curtail the implementation of FAIR, but this is misguided. As pointed out in the articulation of CARE (GIDA 2019), Indigenous data cannot be used in good conscience without “relationships built on respect, reciprocity, trust, and mutual understanding, as defined by the Indigenous Peoples to whom those data relate” (quoting from GIDA (2019)). Thus without CARE, the R (reusable) in FAIR is invalid. Operationalizing CARE results in richer, more complete metadata, and complete metadata underpin all aspects of FAIR.\nPerceived conflicts between FAIR and CARE might be resolved by asking “FAIR for whom?” and “who gets to decide?” Who gets to decide which protocols are implemented to access and reuse data? Who can find and access the data (i.e. findable and accessible for whom)? Who decided what metadata to expose to make data findable by which search terms? With whose worldview do those search terms align? Contemplating these questions might result in the conclusion that individuals or institutions that are asked to share power—power that previously was not shared—may perceive this CAREful sharing as limiting FAIR. But CARE does not limit FAIR. Rather, colonial power imbalances make data “FAIR” for some but not all, which is not actually FAIR.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#implementing-care-with-local-contexts-labels-and-notices",
    "href": "care-fair.html#implementing-care-with-local-contexts-labels-and-notices",
    "title": "2  CARE and FAIR principles",
    "section": "2.4 Implementing CARE with Local Contexts Labels and Notices",
    "text": "2.4 Implementing CARE with Local Contexts Labels and Notices\nOperationalizing CARE produces metadata. Having metadata standards is one important aspect of FAIR. The IDSov and IDGov focused organization Local Contexts offers one emerging implementation of standard CARE metadata via their Labels and Notices (Local Contexts 2025a, 2025b). Labels are used by Indigenous communities and nations to articulate their rights to and interests in Indigenous data. Notices are used by researchers and colonizer institutions to affirm the existence of Indigenous rights and interests and acknowledge that work is underway to articulate those rights and interests. Notices also indicate the readiness of researchers and colonizer institutions to fulfill their responsibilities (the R in CARE) of centering Indigenous worldviews, self-determination, wellbeing, and rights in data stewardship.\nFor metadata to be useful they need to be FAIR. To make Local Contexts Labels and Notices FAIR they are hosted on the Local Contexts Hub, an online repository. To work with these metadata, communities, researchers, and colonizer institutions must first make an account on the Hub (free for communities and researchers, paid on a subscription basis for institutions). Local Contexts Hub is structured by projects. Labels and Notices are applied to data by connecting both to a project. Projects have permanent unique identifiers and persistent URLs based on those identifiers, replicating the permanence of DOIs.\nTo understand this better we will walk through the a Local Contexts workflow from the perspective of a researcher initiated the process, because “researcher” is likely the common aspect of identity shared by those participating in this course.\n\n2.4.1 A researcher applies Notices and notifies communities\nThe Local Contexts Hub workflow for a researcher should ideally begin with co-production of research goals and data management with local communities in real life outside of the Hub. For archival data already collected (the majority of data) this likely did not occur. Within the Hub a researcher creates a project to correspond to Indigenous data they are working with. The Hub does not house the data itself, only the metadata. Then Notices are applied to the project and communities are notified via the Hub. Again, ideally, researchers and communities will already be working together because even the act of defining what data are and the geographic or other bounds that delineate one data set (and thus one Hub project) from another can be surprisingly different across different ways of knowing. But projects can still be created and Notices applied in situations where co-production has not occurred (don’t let perfect be the enemy of good).\nNotices, again, affirm the existence of Indigenous rights to and interests in specific data. A notice is three things: 1) a visual badge drawing the focus of a person engaging with Indigenous data; 2) human-readable text explaining the significance of the Notice; and 3) a machine-readable string that can be integrated into data processing and management workflows. There are multiple Notices each with unique purpose. We will focus on four that individual researchers are most likely to use.\n\n2.4.1.1 Open to Collaborate\n\n\n\n\n\nNote: this Notice is being displayed here for educational purposes\n\n\n\nThe Open to Collaborate Notice is an Engagement Notice that can be used to indicate a researcher’s or institution’s commitment to CARE principles and working collaboratively on issues of IDSov and IDGov. Engagement Notices are not applied to data, but rather are appropriate to display on, for example, research or institution websites. You might recall we use this notice on the landing page of this course website. See full information here.\n\n\n\n\n2.4.1.2 Attribution Incomplete\n\n\n\n\n\nNote: this Notice is being displayed here for educational purposes and is not attached to data\n\n\n\nThe Attribution Incomplete Notice is applied to data to indicate that attribution (including provenance and contributors) is incomplete, inaccurate, or missing. This notice indicates a work in progress toward correcting the mistake of incomplete attribution. See full information here.\n\n\n\n\n2.4.1.3 Traditional Knowledge\n\n\n\n\n\nNote: this Notice is being displayed here for educational purposes and is not attached to data\n\n\n\nThe TK Notice indicates that data representing Traditional Knowledge and related terms/concepts (Traditional Ecological Knowledge, Indigenous Knowledge, Indigenous Science) carry Indigenous rights, protocols, and responsibilities. See full information here.\n\n\n\n\n2.4.1.4 Biocultural\n\n\n\n\n\nNote: this Notice is being displayed here for educational purposes and is not attached to data\n\n\n\nThe BC Notice affirms the rights of Indigenous peoples to govern the stewardship of data generated from biological sources within their traditional lands, waters, and territories. See full information here.\n\n\nNotices, because they represent a commitment to Responsibility and CARE, are expressly not to be modified by researchers/colonizer institutions according to the Notices usage guide. Notices are also not meant to be the permanent metadata associated with Indigenous data because the Notices do not themselves articulate the rights and protocols of Indigenous peoples. That is the job for Labels.\n\n\n\n2.4.2 A community applies Labels\nTo re-iterate, this workflow is from a researcher’s perspective. Indigenous communities can also create projects connected to data without need for researchers external to their communities creating the project and applying Notices. We are using the researcher-initiated example here because we are all researchers, but not all of us may be Indigenous or authorized to speak for Indigenous communities.\nIn the researcher-initiated workflow, once Notices are applied to data via a project, an Indigenous community can then replace Notices with Labels. Unlike Notices, Labels do articulate the rights, interests, and protocols of Indigenous peoples. There are many different Labels, indeed there needs to be many to express rights, interests, and protocols. Only communities can apply Labels, they are out of the purview of researchers and colonizer institutions. It should of course be noted that researchers can belong to the community of origin for the data they work with. Such scholars may have roles in both applying Notices and applying Labels if they are given that authority by their community.\nThe Labels are organized into Traditional Knowledge (TK) and Bicultural (BC) categories, and within each category there are Provenance, Protocol, and Permission Labels. Below are example Labels from each grouping (template text is quoted directly from the hyperlinked sources).\n\n2.4.2.1 Provenance Labels\n\n\n\n\n\nNote: this Label is being displayed here for educational purposes and is not attached to data\n\n\n\nTK Attribution (TK A)\nTemplate Text\nThis Label is being used to correct historical mistakes or exclusions pertaining to this material. This is especially in relation to the names of the people involved in performing or making this work and/or correctly naming the community from which it originally derives. As a user you are being asked to also apply the correct attribution in any future use of this work.\n\n\n\n\n\n\n\n\nNote: this Label is being displayed here for educational purposes and is not attached to data\n\n\n\nBC Provenance (BC P)\nTemplate Text\nThis Label is being used to affirm an inherent interest Indigenous people have in the scientific collections and data about communities, peoples, and the biodiversity found within traditional lands, waters and territories. [Community name or authorizing party] retains the right to be named and associated with it into the future. This association reflects a significant relationship and responsibility to [the species or biological entity] and associated scientific collections and data.\n\n\n\n\n2.4.2.2 Permission Labels\n\n\n\n\n\nNote: this Label is being displayed here for educational purposes and is not attached to data\n\n\n\nTK Culturally Sensitive (TK CS)\nTemplate Text\nThis Label is being used to indicate that this material has cultural and/or historical sensitivities. The Label asks for care to be taken when this material is accessed, used, and circulated, especially when materials are first returned or reunited with communities of origin. In some instances, this Label will indicate that there are specific permissions for use of this material required directly from the community itself.\n\n\n\n\n\n\n\n\nNote: this Label is being displayed here for educational purposes and is not attached to data\n\n\n\nBC Consent Verified (BC CV)\nTemplate Text\nThis Label is being used to verify that [community name or authorizing party] have consent conditions in place for the use of this information, collections, data, and digital sequence information. [These can be found at ….].\n\n\n\n\n2.4.2.3 Protocol Labels\n\n\n\n\n\nNote: this Label is being displayed here for educational purposes and is not attached to data\n\n\n\nTK Non-Commercial (TK NC)\nTemplate Text\nThis material has been designated as being available for non-commercial use. You are allowed to use this material for non-commercial purposes including for research, study, or public presentation and/or online in blogs or non-commercial websites. This Label asks you to think and act with fairness and responsibility towards this material and the original custodians.\n\n\n\n\n\n\n\n\nNote: this Label is being displayed here for educational purposes and is not attached to data\n\n\n\nBC Research Use (BC R)\nTemplate Text\nThis Label is being used by [community name or authorizing body] to allow this information, collection, data, and digital sequence information (DSI) to be used for unspecified research purposes. This Label does not provide permission for commercialization activities.\n[Optional return of research results statement]\n\n\n\nAs suggested in the template texts and made explicit in the Labels usage guide, Labels are intended to be customized by communities. The above examples are not a complete accounting of Labels. As researchers, we should be familiar with all Labels (link to TK here, and BC here) so we understand their implications and can be prepared to integrate that understanding into our work.\n\n\n\n2.4.3 What’s next?\nCreating a culture of understanding and responsibility to implement CARE—via Local Contexts Labels and Notices and/or other means—is a critical next step. Without understanding, metadata cannot be properly interpreted; without a cultural norm of responsibility, CARE principles will not be adopted. As a research community we have not yet realized a culture of understanding and responsibility. As a consequence, not all visions for operationalizing CARE have been fulfilled. In particular, many large, FAIR-compliant repositories such as GBIF and NCBI do not yet implement CARE, at least not across their entire databases. But very exciting work is underway by Local Contexts, the Global Indigenous Data Alliance, and others to collaborate with these repositories. See for example Local Contexts’ work with GBIF. IDSov and IDGov organizations such as the Native BioData Consortium have also produced new repositories with CARE built-in from the foundation as alternatives to existing repositories that are not CARE-compliant. There is also active innovation on how to harmonize CARE metadata with existing scientific community metadata standards, including this Local Contexts and Darwin Core example from Hutchins et al. (2023).\n\n\n\nsource: Hutchins et al. (2023)\n\n\nAnother goal is for journals to clearly indicate CARE metadata. Journals are adopting visible badges relating to FAIR (Kidwell et al. 2016), but not to CARE. For example this paper (Lim et al. 2021) based on data from Hawaiʻi prominently displays an “Open Data Badge” above the title, defining the badge later in the article.\n\n\n\nsource: Lim et al. (2021)\n\n\nAnother article centering CARE practices in a closely related journal using the same publishing platform, including overlapping authors, using data from within the same mokupuni, is not able to display Local Contexts Notices on equal footing with FAIR-related badges. Instead, Notices are displayed in data figures (the best solution available). But if Labels were to replace these Notices, the static figure would not reflect that update, whereas dynamic badges rendered by the journal itself would be able to.\n\n\n\nsource: Hutchins et al. (2023)\n\n\nThe article by Leke Hutchins and colleagues (2023) also does not earn an Open Data Badge despite metadata conforming to open standards and residing in BOLD, a FAIR repository, and the data itself residing in the Native BioData Consortium and reusable within conditions mandated by CARE.\nHawaiʻi contains a multitude of active work involving IDSov and IDGov. Among many examples, see the Paoakalani Declaration (2021), the Huamakahikina Declaration (2021), the Kūlana Noiʻi, work by the Ahupauʻa Accelerator Initiative (see “Information and Data Management, Storage, and Sharing” in their Ahupuaʻa Action Agenda), and work and advocacy by Rosie ʻAnolani Alegado (e.g. here) among many more. Among these efforts and initiatives is the critical work of organizing communities to identify authorized representatives and create decision making structures so that Labels (or another articulation of CARE metadata) can be applied to data. Currently (as of December 2025) such authorized bodies may not exist for most data stewardship scenarios, for example the data in Hutchins et al. (2023).\nThe work of advancing IDSov and IDGov in Hawaiʻi is rooted in correcting and healing the violence against and violations of human rights and Indigenous rights perpetrated against the Kanaka1 ʻŌiwi by European and American colonizers from first European contact, through the US-backed illegal overthrow, through illegal US annexation, to the present. Settler colonialism is why there is an absence of authorized bodies to articulate Indigenous rights and interests in data, let alone other aspects of wellbeing and self-determination. The research apparatus, including the University of Hawaiʻi, and scientific racism were and remain integral parts of Euro-American settler colonialism (Trask 1993; Kauai & Balutski 2024). Therefore, academic research in Hawaiʻi holds a distinct kuleana to confront and correct continued colonial practices and kākoʻo the Kanaka ʻŌiwi right to ea and self-determination.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#care-and-fair-for-this-class",
    "href": "care-fair.html#care-and-fair-for-this-class",
    "title": "2  CARE and FAIR principles",
    "section": "2.5 CARE and FAIR for this class",
    "text": "2.5 CARE and FAIR for this class\nOur class uses data already collected/compiled and published by other researchers (Craven et al. 2018; Craven 2019; McLean et al. 2023). These data have been published in compliance with FAIR. FAIR compliance takes the form of:\n\nFindable: within their respective databases, metadata can be searched to located the intended data and data have DOIs or other permanent identifiers. For convenience, we use copies of these data for this class, so we will not directly engage with the databases themselves.\nAccessible: the data can be downloaded through HTTP or via API using open protocols and the data are stored in open formats\nInteroperable: metadata (see tables in the previous chapter) clearly define all data fields allowing data to be merged or integrated into other workflows\nReusable: the data are licenced under open licences allowing reuse\n\nHowever, none of the data are CARE compliant. That leaves us with the question of what to do? There are a series of compromises to consider:\n\nCentering place-based ecosystems and data in a class situated in Hawaiʻi honors this place and our connection to it\nBut: working with non-CARE-complying data that are already collected and published means we have no control over how the data were collected and limited impact on how their re-use is governed\nOur limited impact can include reflecting on our own use of the data and identifying ways our individual use can align with CARE\nBut: our individual respect for CARE does not correct the larger problem of non-CARE-complying data being published\nOur limited impact can extend a little further if we collaborate in this class to articulate a vision for how these data could meet the CARE principles\nBut: that vision may not be achievable\n\nWe will proceed in working with these data but our first assignment for this course will be drafting our considerations for respecting CARE individually and drafting ideas for a proposal to make the official stewardship of the data CARE-compliant.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#references",
    "href": "care-fair.html#references",
    "title": "2  CARE and FAIR principles",
    "section": "2.6 References",
    "text": "2.6 References\n\n\n\n\nCarroll S et al. 2020. The CARE principles for indigenous data governance. Data science journal 19. Ubiquity Press.\n\n\nCarroll SR, Herczog E, Hudson M, Russell K, Stall S. 2021. Operationalizing the CARE and FAIR principles for indigenous data futures. Scientific data 8:108. Nature Publishing Group UK London.\n\n\nChen CY, Kahanamoku SS, Tripati A, Alegado RA, Morris VR, Andrade K, Hosbey J. 2022. Systemic racial disparities in funding rates at the national science foundation. Elife 11:e83071. eLife Sciences Publications, Ltd.\n\n\nCraven D. 2019, June. Dylancraven/hawaii_diversity: beta. Zenodo. Available from https://doi.org/10.5281/zenodo.3250638.\n\n\nCraven D, Knight TM, Barton KE, Bialic-Murphy L, Cordell S, Giardina CP, Gillespie TW, Ostertag R, Sack L, Chase JM. 2018. OpenNahele: The open hawaiian forest plot database. Biodiversity Data Journal:e28406.\n\n\nFilazzola A, Cahill Jr JF. 2021. Replication in field ecology: Identifying challenges and proposing solutions. Methods in Ecology and Evolution 12:1780–1792. Wiley Online Library.\n\n\nGIDA. 2019. Research Data Alliance International Indigenous Data Sovereignty Interest Group. CARE Principles for Indigenous Data Governance. https://www.gida-global.org/care.\n\n\nHuamakahikina. 2021. Huamakahikina declaration. https://nativehawaiianlegalcorp.org/wp-content/uploads/2024/06/Huamakahikina_Declaration_2021.pdf.\n\n\nHutchins L, Mc Cartney A, Graham N, Gillespie R, Guzman A. 2023. Arthropods are kin: Operationalizing indigenous data sovereignty to respectfully utilize genomic data from indigenous lands. Molecular Ecology Resources 25:e13822. Wiley Online Library.\n\n\nKauai W, Balutski BJN. 2024. A hawaiian place of learning under US occupation. Journal Committed to Social Change on Race and Ethnicity (JCSCORE) 10:113–129. JSTOR.\n\n\nKauanui JK. 2008. Hawaiian blood: Colonialism and the politics of sovereignty and indigeneity. Duke University Press.\n\n\nKidwell MC et al. 2016. Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS biology 14:e1002456. Public Library of Science San Francisco, CA USA.\n\n\nKukutai T, Taylor J. 2016. Indigenous data sovereignty: Toward an agenda. ANU press.\n\n\nLarregue J, Nielsen MW. 2024. Knowledge hierarchies and gender disparities in social science funding. Sociology 58:45–65. SAGE Publications Sage UK: London, England.\n\n\nLim JY, Patiño J, Noriyuki S, Cayetano L, Gillespie RG, Krehenwinkel H. 2021. Semi-quantitative metabarcoding reveals how climate shapes arthropod community assembly along elevation gradients on hawaii island. Molecular Ecology 31:1416–1429. Wiley Online Library.\n\n\nLocal Contexts. 2025a. Local contexts labels. https://localcontexts.org/labels/about-the-labels/.\n\n\nLocal Contexts. 2025b. Local contexts notices. https://localcontexts.org/notices/about-the-notices/.\n\n\nMa A, Mondragón RJ, Latora V. 2015. Anatomy of funded research in science. Proceedings of the National Academy of Sciences 112:14760–14765. National Academy of Sciences.\n\n\nMaglia A. 2015. NSF data management and public access initiatives.\n\n\nMcLean J, Cleveland SB, Dodge M, Lucas MP, Longman RJ, Giambelluca TW, Jacobs GA. 2023. Building a portal for climate data—mapping automation, visualization, and dissemination. Concurrency and Computation: Practice and Experience 35:e6727. Wiley Online Library.\n\n\nNguyen M, Gonzalez L, Chaudhry SI, Ahuja N, Pomahac B, Newman A, Cannon A, Zarebski SA, Dardik A, Boatright D. 2023. Gender disparity in national institutes of health funding among surgeon-scientists from 1995 to 2020. JAMA network open 6:e233630–e233630. American Medical Association.\n\n\nNosek BA et al. 2015. Promoting an open research culture. Science 348:1422–1425. American Association for the Advancement of Science.\n\n\nPaoakalani Declaration. 2021. https://nativehawaiianlegalcorp.org/wp-content/uploads/2024/06/Paoakalani-Declaration.pdf.\n\n\nParker TH, Forstmeier W, Koricheva J, Fidler F, Hadfield JD, Chee YE, Kelly CD, Gurevitch J, Nakagawa S. 2016. Transparency in ecology and evolution: Real problems, real solutions. Trends in Ecology & Evolution 31:711–719. Elsevier.\n\n\nPetersen OH. 2021. Inequality of research funding between different countries and regions is a serious problem for global science. Function 2:zqab060. Oxford University Press.\n\n\nStebbins M. 2013. Expanding public access to the results of federally funded research.\n\n\nTrask H-K. 1993. From a native daughter: Colonialism and sovereignty in Hawaiʻi. University of Hawaii Press.\n\n\nUN General Assembly. 2007. United nations declaration on the rights of indigenous peoples 12:1–18.\n\n\nWilkinson MD et al. 2016. The FAIR guiding principles for scientific data management and stewardship. Scientific data 3:1–9. Nature Publishing Group.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "care-fair.html#footnotes",
    "href": "care-fair.html#footnotes",
    "title": "2  CARE and FAIR principles",
    "section": "",
    "text": "We follow Kauanui (2008) in using the plural Kānaka ʻŌiwi to refer to all people of ʻŌiwi ancestry and the singular Kanaka ʻŌiwi to refer to the singular collective People of ʻŌiwi decent (e.g. the Native Hawaiian People).↩︎",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CARE and FAIR principles</span>"
    ]
  },
  {
    "objectID": "r-refresh.html",
    "href": "r-refresh.html",
    "title": "3  R refresher",
    "section": "",
    "text": "3.1 R and RStudio\nThese tools have been described extensively elsewhere. For completeness we better link to one: Carpentries’ R for Ecologists.\nOne distinction to emphasize: R is the language and environment for doing statistics, RStudio is a tool to make working with R a little easier. Here is a quick orientation to RStudio:\nOne way that RStudio is very useful is that it bundles our work into projects, each project living in a folder on our computer, and each getting a .Rproj file that saves administrative details about the project. RStudio automatically helps R find files we might need (like data files) inside the project. That’s a big help for writing clear code, and code we can share with others. To follow along with this tutorial, make a new project with File &gt; New Project… and choose “New Directory”. Give your project the name “r-refresher” and save it somewhere on your computer. We will get more precise with naming and folder organization when we go over GitHub.\nYou can customize the way RStudio looks by going to Tools &gt; Global Options and then apply the options you’d like, including setting the Appearance to a dark theme like the one pictured above. There are a few options you should definitely modify. Under the General - Basic section, make sure your selections match this:\nWe don’t want to save or restore Workspace and History because we will end up completely bogging R down with months or years worth of random things. The only things you should treat as permanent and needing to be saved are scripts and data files.\nMost R code we write should live in an R script. Open a new one with File &gt; New File &gt; R Script to follow along. Before we forget, let’s save that file giving it the name tutorial.R.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#a-few-notes-on-r-style",
    "href": "r-refresh.html#a-few-notes-on-r-style",
    "title": "3  R refresher",
    "section": "3.2 A few notes on R style",
    "text": "3.2 A few notes on R style\nAndy your instructor has a problem (he’s aware of it) about being obsessive with coding style. To make life easier on us all, here are some guidelines we’ll stick to in the class:\n\n3.2.1 Comments\nUse # for comments and make sure there is a space after the #. Comments should look\n\n# like this\n\nand not\n\n#like this\n\n\n\n3.2.2 Assignment character\nR is a funny language, it let’s you make an object in multiple ways:\n\n# this is the same\nx &lt;- 10\n\n# as this\nx = 10\n\n# as this \n10 -&gt; x\n\nWe will stick to using the most common style: x &lt;- 10. This is called “assignment” because we are assigning the value 10 to the object x.\n\n\n3.2.3 Tab spacing\nSometimes a long line of code can get broken up into multiple lines for ease of reading. Or some types of commands are nearly always broken up across lines. In these cases, new lines of code get tab indented and how wide the indent looks is up to you:\n\n# 1 tab = 4 spaces\ndata |&gt; \n    filter()\n\n# 1 tab = 2 spaces\ndata |&gt; \n  filter()\n\nRStudio defaults to making an indent two spaces wide. Let’s not do that, let’s use four spaces (Andy just finds it easier to read…aging eyes perhaps). Go to Tools &gt; Global Options then select Code and set Tab width to 4. I poke fun at myself, but truth is, if your code gets complex, the extra space is a huge help in avoiding syntax errors and finding bugs when they do occur.\n\n\n3.2.4 Naming\nAs we’ve already seen we need to name things (like x &lt;- 10, here x is the name). There are lots of ways to do this\n\n# I'm camel case\nlongNameOfSomething &lt;- 100\n\n# I'm snake case\nlong_name_of_something &lt;- 100\n\n# I'm a bad idea, but technically not wrong\nlong.name.of.something &lt;- 100\n\nNo shade on long.name.of.something it’s just that the dot . is usually reserved for a special purpose—not naming—across many programming languages (even, confusingly, kind of in R). When we need to make a long name, let’s use snake case for this class because the “data science community” seems to be adopting it across multiple languages (e.g. R and python). But also sometimes a short name will do the trick and doesn’t need any kind of “case.”",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#real-basic-r",
    "href": "r-refresh.html#real-basic-r",
    "title": "3  R refresher",
    "section": "3.3 Real basic R",
    "text": "3.3 Real basic R\nYou’ll often here people talk about “objects.” In R, everything is an object.\n\n# what you might call a scalar is an \"object\"\nx &lt;- 10\n\n# what you might call a vector is an \"object\"\ny &lt;- 1:10\n\n# even functions are objects\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x126208678&gt;\n&lt;environment: namespace:base&gt;\n\n\nAn object is just something that we make with code or is made for us with code. Objects come in different types\n\n3.3.1 A few simple object types: numeric, character, factor\n\n# *numerics* hold numbers (whether a single number of many)\nx &lt;- 10\ny &lt;- 1:10\n\n# *characters* hold letter-type things (anything in quotes)\na &lt;- \"word\"\nb &lt;- c(\"three\", \"little\", \"words\")\n\n# *factors* are like numerics and characters combined:\n# they look like characters, but can behave like numbers if we need\nf &lt;- factor(c(\"a\", \"b\", \"c\"))\n\nf\n\n[1] a b c\nLevels: a b c\n\n# cast the factor to numeric\nas.numeric(f)\n\n[1] 1 2 3\n\n# can't do that with true characters\nas.numeric(b)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA\n\n\nFactors have levels that tell R what order to think about the characters in. That will end up being useful for plotting!\n\ng &lt;- factor(c(\"a\", \"b\", \"c\"), levels = c(\"b\", \"c\", \"a\"))\n\ng\n\n[1] a b c\nLevels: b c a\n\n# notice how the numbers changed\nas.numeric(g)\n\n[1] 3 1 2\n\n\nThe last “one-dimensional” type of object to learn about are booleans. These are just TRUEs and FALSEs that end up being very useful for wrangling data.\n\n# we can make a boolean like this\nbool &lt;- c(TRUE, FALSE, TRUE)\n\n# booleans also come by making \"logical\" comparisons\n10 &gt; 3\n\n[1] TRUE\n\n# we can compare objects \ny == x\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nThe operation == is a comparison that asks is “this” equal to “that”: this == that.\nWe can already get a peek at how booleans can help wrangle data:\n\nb[bool]\n\n[1] \"three\" \"words\"\n\n\nWhat just happened? Putting bool inside square brackets [] after b extracted the value from b where bool was true.\n\n\n\n\n\n\nNoteYour turn\n\n\n\nMake a boolean object called cool that extracts the 2nd element of the object b that we made earlier\n\n# recall `b` is\nb &lt;- c(\"three\", \"little\", \"words\")\n\n\n\n\n\n3.3.2 Combining objects into something more data-like\nWe can combine these one-dimensional objects into 2D objects—these 2D objects are what we typically think of when we think of data.\nCombine 2 or more objects of the same type and same size (like three elements long) to make a matrix\n\nm &lt;- cbind(f, g)\nm\n\n     f g\n[1,] 1 3\n[2,] 2 1\n[3,] 3 2\n\n\nCombine 2 or more objects of same or different types, but still of the same size, to make a data.frame.\n\ndat &lt;- data.frame(b, f, bool)\ndat\n\n       b f  bool\n1  three a  TRUE\n2 little b FALSE\n3  words c  TRUE\n\n\nWe can make custom column names if we like\n\nnew_dat &lt;- data.frame(words = b, groups = f, included = bool)\nnew_dat\n\n   words groups included\n1  three      a     TRUE\n2 little      b    FALSE\n3  words      c     TRUE\n\n\nWe can access columns in a data.frame in multiple ways:\n\n# by name with the dollar sign\nnew_dat$words\n\n[1] \"three\"  \"little\" \"words\" \n\n# by name using square brackets and a comma to indicate we want columns\nnew_dat[, \"words\"] \n\n[1] \"three\"  \"little\" \"words\" \n\n# by position using brackets\nnew_dat[, 1]\n\n[1] \"three\"  \"little\" \"words\" \n\n# with booleans and square brackets\nnew_dat[, bool]\n\n   words included\n1  three     TRUE\n2 little    FALSE\n3  words     TRUE\n\n\nThe data.frame is the data workhorse in R. There are also some things that seem confusing about them like this new_dat$words and this new_dat[, \"words\"] being equivalent. Those language design choices were made literal decades ago so we’re stuck with them. We are about to learn some newer approaches to working with data that attempt to bring greater consistency and clarity, but sometimes these “old school” methods are needed for specific tasks, so they are worth knowing.\n\n\n\n\n\n\nNoteYour turn\n\n\n\nMake a data.frame to hold data that look like these:\n\n\n\n\n\n\n\n\nx\ny\ngroup\n\n\n\n\n1\n2\ng1\n\n\n2\n8\ng1\n\n\n3\n6\ng2\n\n\n\n\n\n\n\n\n\nSometimes we may need to combine objects of different sizes, and maybe same or different types. That task requires one last type of object: the list\n\nl &lt;- list(y, g)\nl\n\n[[1]]\n [1]  1  2  3  4  5  6  7  8  9 10\n\n[[2]]\n[1] a b c\nLevels: b c a\n\n\nLists can get special names too\n\nnew_l &lt;- list(nums = y, group = g)\nnew_l\n\n$nums\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$group\n[1] a b c\nLevels: b c a",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#working-with-real-data",
    "href": "r-refresh.html#working-with-real-data",
    "title": "3  R refresher",
    "section": "3.4 Working with real data",
    "text": "3.4 Working with real data\nIn real practice, we are rarely making a data.frame by hand. We are usually reading it in from a file. Let’s get the data we’ll be working with for this class, downloading it into the folder where our RStudio project lives. First make a folder called data inside the folder holding your RStudio project. You can do that in your computers folder system, or from within RStudio by hitting the icon of a holder with a plus over in the Files panel.\nYour project folder should now have this structure\nr-refresh/\n├─── data/\n├─── r-refresh.Rproj\n└─── tutorial.R\nNow click the following to download each data set, making sure to save them in the data folder you just made\n\n Download OpenNahele Data \n Download Climate Data \n Download Human Impact and Geo Data \n\n\n3.4.1 Keeping data tidy\nLet’s read the data into R and have a look\n\ntree &lt;- read.csv(\"data/OpenNahele_Tree_Data.csv\")\n\nOne really REALLY nice thing about RStudio projects is that they take care of a lot of the headache around file paths for us. If I had to type out the full path to the data file it might look like /Users/Andy/teaching/zool631/r-refresh/data/OpenNahele_Tree_Data.csv or something else crazy depending on where I saved the folder r-refresh on my computer. Worse than the typing itself, if I shared my script, nobody else could just run it, they would have to modify the path. If they shared it back with me, I would have to change it again…and again…and again! RStudio let’s us use relative paths. No matter where the RStudio “r-refresh” project lives on our computer, it tells R to start looking for files inside the r-refresh folder.\n\n\n\n\n\n\nNoteYour turn\n\n\n\nRead in the climate data and combined human impact and geology data. It’s important we all use the same names so we can continue following along with this tutorial, so let’s store the climate data in an object called clim and the human impact + geology data in an object called geoh. Your code will include something like this:\n\nclim &lt;- .....\ngeoh &lt;- .....\n\n\n\nTo view the data we have a few options. We can always use a program or app like excel or google sheets to directly view the file. We can also look at the data from within R. Knowing how to look at data from within R is an important tool:\n\nwe need to be able to confirm the data were read in correctly\nas we wrangle the data we’ll need to quickly and repeatedly look at the data objects to make sure we’ve done the wrangling right\n\n\n# the `View` function opens up a tab where we can see the data.frame \n# in a tabular display like a spread sheet\n\nView(tree)\n\n\n# the `head` function prints out the first few rows into the R console\n# this can look messy for `data.frame`s with many columns\n\nhead(tree)\n\n          Island PlotID     Study Plot_Area Longitude Latitude Year Census\n1 Hawai'i Island      2 Zimmerman   1017.88   -154.91   19.444 2003      1\n2 Hawai'i Island      2 Zimmerman   1017.88   -154.91   19.444 2003      1\n3 Hawai'i Island      2 Zimmerman   1017.88   -154.91   19.444 2003      1\n4 Hawai'i Island      2 Zimmerman   1017.88   -154.91   19.444 2003      1\n5 Hawai'i Island      2 Zimmerman   1017.88   -154.91   19.444 2003      1\n6 Hawai'i Island      2 Zimmerman   1017.88   -154.91   19.444 2003      1\n  Tree_ID  Scientific_name Family Angiosperm Monocot Native_Status\n1    6219 Cibotium glaucum     14          0       0        native\n2    6202 Cibotium glaucum     14          0       0        native\n3    6193 Cibotium glaucum     14          0       0        native\n4    6248 Cibotium glaucum     14          0       0        native\n5    6239 Cibotium glaucum     14          0       0        native\n6    6241 Cibotium glaucum     14          0       0        native\n  Cultivated_Status Abundance Abundance_ha DBH_cm\n1                NA         1        39.30   14.3\n2                NA         1        39.30   13.5\n3                NA         1        39.30   17.8\n4                NA         1        39.30    7.2\n5                NA         1         9.82   30.9\n6                NA         1        39.30   14.2\n\n# the `str` function prints out the *structure* of the data, showing you\n# the data type of each column and a preview of its contents\n\nstr(tree)\n\n'data.frame':   43590 obs. of  18 variables:\n $ Island           : chr  \"Hawai'i Island\" \"Hawai'i Island\" \"Hawai'i Island\" \"Hawai'i Island\" ...\n $ PlotID           : int  2 2 2 2 2 2 2 2 2 2 ...\n $ Study            : chr  \"Zimmerman\" \"Zimmerman\" \"Zimmerman\" \"Zimmerman\" ...\n $ Plot_Area        : num  1018 1018 1018 1018 1018 ...\n $ Longitude        : num  -155 -155 -155 -155 -155 ...\n $ Latitude         : num  19.4 19.4 19.4 19.4 19.4 ...\n $ Year             : int  2003 2003 2003 2003 2003 2003 2003 2003 2003 2003 ...\n $ Census           : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Tree_ID          : int  6219 6202 6193 6248 6239 6241 6266 6274 6264 6289 ...\n $ Scientific_name  : chr  \"Cibotium glaucum\" \"Cibotium glaucum\" \"Cibotium glaucum\" \"Cibotium glaucum\" ...\n $ Family           : int  14 14 14 14 14 14 14 14 14 14 ...\n $ Angiosperm       : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Monocot          : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Native_Status    : chr  \"native\" \"native\" \"native\" \"native\" ...\n $ Cultivated_Status: int  NA NA NA NA NA NA NA NA NA NA ...\n $ Abundance        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Abundance_ha     : num  39.3 39.3 39.3 39.3 9.82 39.3 39.3 39.3 39.3 39.3 ...\n $ DBH_cm           : num  14.3 13.5 17.8 7.2 30.9 14.2 24.8 20 20 14.5 ...\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nUse View, head, and str to take a look at clim and geoh.\n\n\nLet’s talk about the organization of the data. As stated in the chapter on CARE and FAIR, well organized data is integral to making data Interoperable and Reusable which are integral to CARE as well.\nThe data in tree are organized so that each row is an individual tree and the columns represent data about that tree. Column Island tells us what island that particular tree was growing on. Column PlotID tells us the unique ID of the inventory plot where that tree was growing. Island and PlotID have many repeating values because many trees were sampled from the same plots on the same islands. The repeated values might seem like an inefficient way to store data, but actually there are many advantages to storing data this way.\nConsider an alternative: each unique PlotID gets its own row. Then we wouldn’t need to repeat information about the plot, but we would be stumped with how to store data on the trees within the plot. Does each tree get its own row? Different plots have different numbers of trees and we can’t have variable numbers of columns per row, so then we’d have a bunch of missing values? In terms of computer memory, columns are also very expensive to store, so a data.frame with many columns (but with little or no repeated information in rows) is actually much more inefficient to store compared to a data.frame with fewer columns but some repeated information across rows.\nThe other advantage to organizing the data with rows as trees is that it focuses our attention as data analysts on what the sampling unit is in these data. The sampling unit is indeed the tree. We collect multiple data points—or variables—about each tree: its species ID, its diameter at breast height (DBH), which plot it is found in, etc. With individual sampling units in rows, each variable fits cleanly into a single column.\nWe can contrast tree being the sampling unit with the level of sampling unit in the data sets clim and geoh. Here each row is indeed a unique plot. In these data, plot is the sampling unit. We do not have data about climate variables, geology variables, and human impact variables at the level of each individual tree, we only have that information at the level of plot.\nIf we want to combine all these data, and soon we will, then plot-level data like rain_annual_mm will be repeated across all the trees from the same plot. This might be an approximation of the estimated rainfall (in this example) each tree receives, or we might actually consider that the relevant biological scale for rainfall really is an entire plot, not the precise geographic location of where a tree’s trunk meets the ground.\nData organized with individual sampling units as rows and variables as columns have become known as tidy data (Wickham 2014). Hadly Wickham’s (2014) article on tidy data was not the first to champion this type of data organization (e.g. relational databases, the assumed data structure for regression and ANOVA in R) but it laid the groundwork for best practices in data organization that have subsequently enabled the development of coding tools in R and other languages that make working with data more clear and consistent.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#introducing-the-dplyr-package",
    "href": "r-refresh.html#introducing-the-dplyr-package",
    "title": "3  R refresher",
    "section": "3.5 Introducing the dplyr package",
    "text": "3.5 Introducing the dplyr package\nThe package dplyr by Wickham and colleagues (2023) is a central package for working with tidy data. To work with the dplyr package we need to load it into our R session. Try this:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nIf your console prints out the same message, great! You have loaded dplyr into your current R session. If you get an error message that looks like\nError in library(dplyr) : there is no package called ‘dplyr’\nThat means the package dplyr is not installed on your computer. No problem, install it with this code:\n\ninstall.packages(\"dplyr\", dependencies = TRUE)\n\nYou only need to run install.packages(\"dplyr\", dependencies = TRUE) if the package is not available or if you suspect your version of the package is out-of-date. You do not need to, and should not, run install.packages every time you start an R session. For this reason, don’t put any calls to install.packages in your R script. Type (or paste) those directly into the console. But you do need to run library every time you want to load a package into an R session. So calls to library should go in your R script. Often it’s considered best practice to put all calls to library at the top of a script.\nNow let’s have a look at what dplyr can do. Before applying dplyr functions to the real data, let’s use a toy data set that we can easily see when printed out in the R console:\n\ntoy_data &lt;- data.frame(tree_ID = 1:4, \n                       species = c(\"sp1\", \"sp1\", \"sp2\", \"sp2\"), \n                       x1 = c(3.2, 2.9, 5.1, 5.8), \n                       x2 = c(4.4, 3.6, 3.4, 5.1))\n\ntoy_data\n\n  tree_ID species  x1  x2\n1       1     sp1 3.2 4.4\n2       2     sp1 2.9 3.6\n3       3     sp2 5.1 3.4\n4       4     sp2 5.8 5.1\n\n\nWe can imagine tree_ID records the unique ID of each tree in some made-up inventory plot, species holds the made-up species ID, and x1 and x2 are some kind of measurements made on each tree.\n\n3.5.1 select\nWhat if we need to extract all the values in the x1 column. The dplyr approach is:\n\nselect(toy_data, x1)\n\n   x1\n1 3.2\n2 2.9\n3 5.1\n4 5.8\n\n\nQuick check-in: instead of the above output, did you get an error like this:\nError in select(toy_data, x1) : could not find function \"select\"\nThat’s because you didn’t actually run the command library(dplyr), just go back and run that.\nNotice that we can reference the x1 column without needing to use $ and without needing to put it in quotes like \"x1\". This is the most common approach, though there can be times when quoting the column name is helpful, and select allows us to do that:\n\nselect(toy_data, \"x1\")\n\n   x1\n1 3.2\n2 2.9\n3 5.1\n4 5.8\n\n\nWhat if we want to select multiple columns at the same time? No problem:\n\nselect(toy_data, species, x1)\n\n  species  x1\n1     sp1 3.2\n2     sp1 2.9\n3     sp2 5.1\n4     sp2 5.8\n\n\nWhat if we want to remove a certain column? Can!\n\nselect(toy_data, -tree_ID)\n\n  species  x1  x2\n1     sp1 3.2 4.4\n2     sp1 2.9 3.6\n3     sp2 5.1 3.4\n4     sp2 5.8 5.1\n\n\nIn fact select uses a flexible approach to indicating columns called &lt;tidy-select&gt;. Here’s one example: suppose we want to extract the very last column from the data:\n\nselect(toy_data, last_col())\n\n   x2\n1 4.4\n2 3.6\n3 3.4\n4 5.1\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nHave a look at the help documentation for select:\n\n?select\n\nAnd figure out two ways of selecting both columns x1 and x2 that do not look like select(toy_data, x1, x2).\n\n\n\n\n3.5.2 filter\nWe can extract or remove columns now, but what about rows. That is the job for filter. Suppose we want only the rows corresponding to “sp1”. Here is the dplyr approach:\n\nfilter(toy_data, species == \"sp1\")\n\n  tree_ID species  x1  x2\n1       1     sp1 3.2 4.4\n2       2     sp1 2.9 3.6\n\n\nNotice a few things there: we do need to put “sp1” in quotes because it is of type character; and we are using one of those logical comparisons ==. We saw logical comparisons earlier, but let’s refresh. The == operator asks R a question is this equal to that? or in code this == that. So we are asking is species equal to \"sp1\" and where it is, give us TRUE, where it is not, give us FALSE. The function filter then uses those TRUEs and FALSEs to extract the rows we want.\n\n\n3.5.3 Logical operators\nThere are a lot of logical operators and they are implemented in base R (we do not need dplyr to access them). Here is an accounting of the ones you are most likely to use\n\n\n\n\n\n\n\n\noperator\npurpose\nexample\n\n\n\n\n==\nequal?\nspecies == \"sp1\"\n\n\n!=\nnot equal?\nspecies != \"sp1\"\n\n\n&lt;\nless than?\nx1 &lt; 3\n\n\n&gt;\ngreater than?\nx1 &gt; 3\n\n\n&lt;=\nless than or equal to?\nx1 &lt;= 3\n\n\n&gt;=\ngreater than or equal to?\nx1 &gt;= 3\n\n\n&\ncombine comparisons with AND (both must be TRUE)\nx1 &lt; 3 & x1 &gt; 1 (can think of it mathematically as \\(1 &lt; x_1 &lt; 3\\))\n\n\n|\ncombine comparisons with OR (either can be TRUE)\nx1 &lt; 3 | x1 &gt; 5\n\n\n%in%\nequal? but multiple comparisons\nspecies %in% c(\"sp1\", \"sp2\") (same as species == \"sp1\" | species == \"sp2\")\n\n\nis.na\nequal to NA?\nis.na(species)\n\n\n!\nflip TRUE to FALSE and vice versa\n!is.na(species) (pronounced “bang!”)\n\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nLet’s look under the hood a bit at what these logical operators are doing. Use select to figure out what the comparison species == \"sp1\" is actually computing.\nNow try a few other operators out. Filter the toy_data so that we extract rows where x2 is less than or equal to 5 but greater than 3.4.\nNow try to predict (without running any code) what this will produce:\n\nfilter(toy_data, species %in% c(\"sp1\", \"sp2\"))\n\nGo ahead and run to code to see if you were right. If your prediction was off, why?\n\n\n\n\n3.5.4 rename\nSometimes our data do not have columns with the best names. One might want to change the raw data in the spread sheet, but wait! That might not be FAIR! What if you’re working with data that has well defined metadata and you want to share your code with a friend? They download the same data set but your code doesn’t work for them because you changed the column names in your copy of the raw data. Better to document the name change and make it reproducible by using code.\nHere’s an example. The column names x1 and x2 maybe are not great. Maybe we want to call them what they really are. Imagine x1 is specific leaf area (SLA) and x2 is DBH. We can rename them like this:\n\nrename(toy_data, sla = x1, dbh = x2)\n\n  tree_ID species sla dbh\n1       1     sp1 3.2 4.4\n2       2     sp1 2.9 3.6\n3       3     sp2 5.1 3.4\n4       4     sp2 5.8 5.1\n\n\nSweet! Now have a look at toy_data again\n\ntoy_data\n\n  tree_ID species  x1  x2\n1       1     sp1 3.2 4.4\n2       2     sp1 2.9 3.6\n3       3     sp2 5.1 3.4\n4       4     sp2 5.8 5.1\n\n\nNothing changed. That is not an error or bug. The dplyr functions do not change the data object itself, they make a new copy of it with the modifications. If we want to use that output, we need to assign it to an object:\n\nclean_toy_data &lt;- rename(toy_data, sla = x1, dbh = x2)\n\n\n\n3.5.5 mutate\nSometimes we need to add a column to a data.frame based on a computation we make in R. Say, for example, we want to analyze the log of DBH rather than DBH itself. The best way to set ourselves up for success is to add a column to the data.\n\nmutate(clean_toy_data, log_dbh = dbh)\n\n  tree_ID species sla dbh log_dbh\n1       1     sp1 3.2 4.4     4.4\n2       2     sp1 2.9 3.6     3.6\n3       3     sp2 5.1 3.4     3.4\n4       4     sp2 5.8 5.1     5.1\n\n\nAgain, if we actually want to keep that change, we need to assign it to an object. Maybe we just re-assign it to the same object name\n\nclean_toy_data &lt;- mutate(clean_toy_data, log_dbh = log(dbh)) \n\nclean_toy_data\n\n  tree_ID species sla dbh  log_dbh\n1       1     sp1 3.2 4.4 1.481605\n2       2     sp1 2.9 3.6 1.280934\n3       3     sp2 5.1 3.4 1.223775\n4       4     sp2 5.8 5.1 1.629241",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#piping",
    "href": "r-refresh.html#piping",
    "title": "3  R refresher",
    "section": "3.6 Piping",
    "text": "3.6 Piping\nWhat if we want to do multiple things to some data in order to arrive at the final object we want? Suppose, for example, that we have toy_data and we want to make a new data.frame with only \"sp1\" in it, without the tree_ID column, and with x1 and x2 renamed appropriately?\nWe could do that like this:\n\ndat_no_ID &lt;- select(toy_data, -tree_ID)\ndat_just_sp1 &lt;- filter(dat_no_ID, species == \"sp1\")\nfinal_dat &lt;- rename(dat_just_sp1, sla = x1, dbh = x2)\n\nfinal_dat\n\n  species sla dbh\n1     sp1 3.2 4.4\n2     sp1 2.9 3.6\n\n\nThe object final_dat is indeed what we want our data.frame to look like, but perhaps it was a little cumbersome writing out all those intermediate steps. Worse than cumbersome, all those object names in the intermediate steps introduce more and more opportunities for bugs and errors arising from typos or incorrect copy-pasting. We could just write-over the same object name over-and-over, that does reduce the risk of typos and incorrect copy-pasting.\n\nfinal_dat &lt;- select(toy_data, -tree_ID)\nfinal_dat &lt;- filter(dat_no_ID, species == \"sp1\")\nfinal_dat &lt;- rename(dat_just_sp1, sla = x1, dbh = x2)\n\nfinal_dat\n\n  species sla dbh\n1     sp1 3.2 4.4\n2     sp1 2.9 3.6\n\n\nAgain the data.frame we want, but many years of experience working with and teaching R has shown me that writing over the same name over-and-over introduces the risk that you might forget to run one line, or one line might error out without you noticing (possible in a long script) which leads to the final product being incorrect without you knowing. I’ve seen that cause plenty of headaches.\nWe could instead nest the functions. Nesting functions is like this:\n\nsum(c(1, 4, 8))\n\n[1] 13\n\n\nR reads this from inside out: first it makes a vector c(1, 4, 8), then it feeds that vector into sum(). Nesting just two simple functions like this example is pretty common and not necessarily something we need to avoid. But with our data wrangling case it is more complex:\n\nfinal_dat &lt;- rename(filter(select(toy_data, -tree_ID), \n                           species == \"sp1\"), \n                    sla = x1, dbh = x2)\n\nfinal_dat\n\n  species sla dbh\n1     sp1 3.2 4.4\n2     sp1 2.9 3.6\n\n\nAgain, the right data.frame. Breaking the code across multiple lines is critical for readability, and helps a little bit, but it is pretty hard to follow the flow of computations (they are kind of inside out and upside down) to understand what the code is doing.\nAll these less-than-ideal solutions motivate another approach: the pipe (|&gt;). The pipe operator lets you combine multiple computations in one workflow that is easier for human eyes to follow than nesting and avoids the issues with making or writing-over objects in intermediate steps. Here’s how it works\n\n# we could nest functions\n# this first computes output of c(1, 4, 8) and \n# then sum() uses that output as its own input\nsum(c(1, 4, 8))\n\n[1] 13\n\n# or we could achieve the same computation by\n# piping the output of c(1, 4, 8) into sum()\nc(1, 4, 8) |&gt; sum()\n\n[1] 13\n\n\nThe pipe operator lets R compute the first command (c(1, 4, 8)) and then pass the output as the first argument into sum(). That lets us write the code from left to right in the actual order that the computations will be done by R. We can also add line breaks so we can visually order the computations top to bottom, again in the same actual order that R will process them:\n\nc(1, 4, 8) |&gt; \n    sum()\n\n[1] 13\n\n\nNow let’s look at the data wrangling case using pipes\n\nfinal_dat &lt;- select(toy_data, -tree_ID) |&gt; \n    filter(species == \"sp1\") |&gt; \n    rename(sla = x1, dbh = x2)\n\nfinal_dat\n\n  species sla dbh\n1     sp1 3.2 4.4\n2     sp1 2.9 3.6\n\n\nThat’s the data.frame we want and hopefully a more clear, less error-prone way of getting there. One thing to note, that sometimes trips people up, is that after passing toy_data into select, we don’t see any names of data objects again. But remember, the output of select is being piped into filter as the first argument, and the output of filter is getting pipped into rename as its first argument. If you got that, great! If not, here is one more way to think about it\n\n# don't try to run this, it is not valid R code\nfinal_dat &lt;- select(toy_data, -tree_ID) |&gt; \n    filter(unseen_output_of_select, species == \"sp1\") |&gt; \n    rename(unseen_output_of_filter, dsla = x1, dbh = x2)\n\nWe don’t see the output of select or filter but it is there under-the-hood.\nIf typing out | and then &gt; to make the pipe |&gt; feels like gymnastics for your fingers, you can use the keyboard shortcut provided by RStudio. Look it up for your OS by going to Tools &gt; Keyboard Shortcuts Help\n\n3.6.1 The magrittr pipe %&gt;%\nIf this is not your first time around the block with pipes, you probably know the magrittr pipe %&gt;%. Following the advice of Hadley Wickham and other pros we will use the native pipe operator |&gt;. In most cases your knowledge of the magrittr %&gt;% will translate cleanly over to the native |&gt;. Two key differences that commonly arise:\n\nThis c(1, 4, 8) %&gt;% sum is valid magrittr code, but this c(1, 4, 8) |&gt; sum is not valid for the native |&gt;, you must include parentheses with the function name: c(1, 4, 8) |&gt; sum()\nThe placeholders are also different: . in magrittr and _ for the native |&gt;. There are other nuances with placeholders that we can address as they come up\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nNow let’s use these approaches with our real data. We’ll work with the tree data specifically, and here’s the goal:\n\nmake a data.frame with just the native species\nremove rows where Island is NA\nremove the columns Cultivated_Status, Angiosperm, and Monocot\nmake the column DBH_cm all lower case\nadd a column for log DBH and call it log_dbh_cm",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#merging-data",
    "href": "r-refresh.html#merging-data",
    "title": "3  R refresher",
    "section": "3.7 Merging data",
    "text": "3.7 Merging data\nLike we mentioned, we want to merge the tree, climate, and geology + human impact data. In database speak, this kind of task is called a “join.” dplyr offers us three kinds of joins: left_join, right_join, full_join. To understand these, we’ll use our toy data, but we need to make two other made-up data.frames to actually join things.\nLet’s say we have some higher taxonomy in another data.frame\n\ntax &lt;- data.frame(family = c(\"fam1\", \"fam2\", \"fam2\"), \n                  genus = c(\"gen1\", \"gen2\", \"gen3\"), \n                  species = c(\"sp1\", \"sp2\", \"sp3\"))\n\ntax\n\n  family genus species\n1   fam1  gen1     sp1\n2   fam2  gen2     sp2\n3   fam2  gen3     sp3\n\n\nI deliberately made fake taxonomic info for three species even though our clean_toy_data only has two.\nLet’s say we have some species average trait values in another data.frame\n\ntrt &lt;- data.frame(species = c(\"sp1\", \"sp2\", \"sp3\", \"sp4\"), \n                  sla = c(3.1, 5.6, 5.4, 2.1), \n                  leaf_nitrogen = c(8.7, 12.2, 12.7, 3.8), \n                  growth_form = c(\"shrub/tree\", \"tree\", \"tree\", \"tree\"))\n\ntrt\n\n  species sla leaf_nitrogen growth_form\n1     sp1 3.1           8.7  shrub/tree\n2     sp2 5.6          12.2        tree\n3     sp3 5.4          12.7        tree\n4     sp4 2.1           3.8        tree\n\n\nAgain, deliberately making a different number of species here.\nOK! So first let’s join clean_toy_data with tax. Here’s what left_join gives us\n\ntoy_tax &lt;- left_join(clean_toy_data, tax)\n\nJoining with `by = join_by(species)`\n\ntoy_tax\n\n  tree_ID species sla dbh  log_dbh family genus\n1       1     sp1 3.2 4.4 1.481605   fam1  gen1\n2       2     sp1 2.9 3.6 1.280934   fam1  gen1\n3       3     sp2 5.1 3.4 1.223775   fam2  gen2\n4       4     sp2 5.8 5.1 1.629241   fam2  gen2\n\n\nCool! The taxonomy info was added, and only for the species we actually have in clean_toy_data. It told us how it made the join too. The column species is shared across both data.frames and left_join used that column for joining.\n\n\n\n\n\n\nNoteYour turn\n\n\n\n\nTry full_join with clean_toy_data and tax; describe any differences\nTry right_join(tax, trt); what’s different?\n\n\n\nLet’s put everything together. We can use |&gt; to combine multiple joins. Also, imagine that clean_toy_data is the focus of our analysis, so we don’t need the “extra” species from tax and trt. That means we’ll use left_join.\n\nfinal_toy_data &lt;- left_join(clean_toy_data, tax) |&gt; \n    left_join(trt)\n\nJoining with `by = join_by(species)`\nJoining with `by = join_by(species, sla)`\n\n\nInteresting, the printed message says join_by(species) (that was from the first left_join) but then it says join_by(species, sla) which is from the second join. We better check what the data look like\n\nfinal_toy_data\n\n  tree_ID species sla dbh  log_dbh family genus leaf_nitrogen growth_form\n1       1     sp1 3.2 4.4 1.481605   fam1  gen1            NA        &lt;NA&gt;\n2       2     sp1 2.9 3.6 1.280934   fam1  gen1            NA        &lt;NA&gt;\n3       3     sp2 5.1 3.4 1.223775   fam2  gen2            NA        &lt;NA&gt;\n4       4     sp2 5.8 5.1 1.629241   fam2  gen2            NA        &lt;NA&gt;\n\n\nWe got no data from trt! The problem is that when we tried to join the output of left_join(clean_toy_data, tax) with trt, R tried to join not just with the shared column species but also the column sla, which has the same name in clean_toy_data and trt. Because the unique combined values of species and sla have no exact matches across the two data.frames, R can’t merge any data. That’s unfortunate. But luckily we can tell R to not join by all columns with the same name, we can tell it specific columns\n\nfinal_toy_data &lt;- left_join(clean_toy_data, tax) |&gt; \n    left_join(trt, by = join_by(species))\n\nJoining with `by = join_by(species)`\n\nfinal_toy_data\n\n  tree_ID species sla.x dbh  log_dbh family genus sla.y leaf_nitrogen\n1       1     sp1   3.2 4.4 1.481605   fam1  gen1   3.1           8.7\n2       2     sp1   2.9 3.6 1.280934   fam1  gen1   3.1           8.7\n3       3     sp2   5.1 3.4 1.223775   fam2  gen2   5.6          12.2\n4       4     sp2   5.8 5.1 1.629241   fam2  gen2   5.6          12.2\n  growth_form\n1  shrub/tree\n2  shrub/tree\n3        tree\n4        tree\n\n\nBetter! But now we have a column sla.y. We don’t really need that. Perhaps the better approach would be to remove the sla column from trt before joining. We can do that\n\nfinal_toy_data &lt;- select(trt, -sla) |&gt; \n    right_join(clean_toy_data) |&gt; \n    left_join(tax)\n\nJoining with `by = join_by(species)`\nJoining with `by = join_by(species)`\n\nfinal_toy_data\n\n  species leaf_nitrogen growth_form tree_ID sla dbh  log_dbh family genus\n1     sp1           8.7  shrub/tree       1 3.2 4.4 1.481605   fam1  gen1\n2     sp1           8.7  shrub/tree       2 2.9 3.6 1.280934   fam1  gen1\n3     sp2          12.2        tree       3 5.1 3.4 1.223775   fam2  gen2\n4     sp2          12.2        tree       4 5.8 5.1 1.629241   fam2  gen2\n\n\nAll the data are joined, great! The columns are kind of in a funny order. Really that shouldn’t deeply matter for the computer as long as they’re all there. But us humans tend to like things arranged in some kind of conceptual order. We can change the order of columns with relocate. That function also makes use of &lt;tidy-select&gt; to flexibly indicate which columns we want to work with. Here’s one arrangement of columns that might make sense:\n\nfinal_toy_data &lt;- select(trt, -sla) |&gt; \n    right_join(clean_toy_data) |&gt; \n    left_join(tax) |&gt; \n    relocate(c(tree_ID, family:genus), .before = species)\n\nJoining with `by = join_by(species)`\nJoining with `by = join_by(species)`\n\nfinal_toy_data\n\n  tree_ID family genus species leaf_nitrogen growth_form sla dbh  log_dbh\n1       1   fam1  gen1     sp1           8.7  shrub/tree 3.2 4.4 1.481605\n2       2   fam1  gen1     sp1           8.7  shrub/tree 2.9 3.6 1.280934\n3       3   fam2  gen2     sp2          12.2        tree 5.1 3.4 1.223775\n4       4   fam2  gen2     sp2          12.2        tree 5.8 5.1 1.629241\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nLet’s join the three real data sets we’ll be working with! Let’s call the product of joining all the data all_dat. Don’t forget to also remove rows where Island is NA. While we’re at it, let’s change the name of the column PlotID to be Plot_ID (more consistent with all the other names). And finally, let’s re-arrange the columns so they are ordered like this:\n\nPlot_ID\nIsland\nStudy\nPlot_Area\nYear\nCensus\nlon\nlat\nevapotrans_annual_mm\navbl_energy_annual_wm2\ncloud_freq_annual\nndvi_annual\nrain_annual_mm\navg_temp_annual_c\nhii\nage_yr\nelev_m\nTree_ID\nScientific_name\nFamily\nAngiosperm\nMonocot\nNative_Status\nCultivated_Status\nAbundance\nAbundance_ha\nDBH_cm",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#the-tibble",
    "href": "r-refresh.html#the-tibble",
    "title": "3  R refresher",
    "section": "3.8 The tibble",
    "text": "3.8 The tibble\nPrinting data.frames, even using head or str, can result in some hard-to-read output. The deeper you go in wrangling data you will also find some inconsistencies in the design choices (made decades ago) about how data.frames sometimes behave. For all those reasons, there is an alternative option that tries to retain all the good things about data.frames, tries to be more-or-less interoperable with data.frames, but also gives us some improvements. This is the tibble. To work with tibbles we’ll need the tibble package. To see the printing benefit of tibbles let’s turn our monstrous all_dat object into a tibble and have a look:\n\nlibrary(tibble) # if you don't have it use install.packages\n\nall_dat_tib &lt;- as_tibble(all_dat)\n\nall_dat_tib\n\n# A tibble: 43,590 × 27\n   Plot_ID Island  Study Plot_Area  Year Census   lon   lat evapotrans_annual_mm\n     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;                &lt;dbl&gt;\n 1       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n 2       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n 3       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n 4       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n 5       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n 6       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n 7       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n 8       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n 9       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n10       1 O'ahu … Knig…      1000  2012      1 -158.  21.4                 818.\n# ℹ 43,580 more rows\n# ℹ 18 more variables: avbl_energy_annual_wm2 &lt;dbl&gt;, cloud_freq_annual &lt;dbl&gt;,\n#   ndvi_annual &lt;dbl&gt;, rain_annual_mm &lt;dbl&gt;, avg_temp_annual_c &lt;dbl&gt;,\n#   hii &lt;dbl&gt;, age_yr &lt;dbl&gt;, elev_m &lt;dbl&gt;, Tree_ID &lt;int&gt;,\n#   Scientific_name &lt;chr&gt;, Family &lt;int&gt;, Angiosperm &lt;int&gt;, Monocot &lt;int&gt;,\n#   Native_Status &lt;chr&gt;, Cultivated_Status &lt;int&gt;, Abundance &lt;int&gt;,\n#   Abundance_ha &lt;dbl&gt;, DBH_cm &lt;dbl&gt;\n\n\nWe don’t even need to worry about using head, we get more information about each column compared to the output of head (kind of like head and str combined), and columns don’t run off the screen or wrap around the console, some are just not printed and we get a message about which ones were not printed.\nIf we want to avoid data.frames all together, we can read data directly into R as a tibble with the readr package:\n\nlibrary(readr) # if you don't have it use install.packages\n\ntree_tib &lt;- read_csv(\"data/OpenNahele_Tree_Data.csv\")\n\nRows: 43590 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Island, Study, Scientific_name, Native_Status\ndbl (14): PlotID, Plot_Area, Longitude, Latitude, Year, Census, Tree_ID, Fam...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntree_tib\n\n# A tibble: 43,590 × 18\n   Island         PlotID Study Plot_Area Longitude Latitude  Year Census Tree_ID\n   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6219\n 2 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6202\n 3 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6193\n 4 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6248\n 5 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6239\n 6 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6241\n 7 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6266\n 8 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6274\n 9 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6264\n10 Hawai'i Island      2 Zimm…     1018.     -155.     19.4  2003      1    6289\n# ℹ 43,580 more rows\n# ℹ 9 more variables: Scientific_name &lt;chr&gt;, Family &lt;dbl&gt;, Angiosperm &lt;dbl&gt;,\n#   Monocot &lt;dbl&gt;, Native_Status &lt;chr&gt;, Cultivated_Status &lt;dbl&gt;,\n#   Abundance &lt;dbl&gt;, Abundance_ha &lt;dbl&gt;, DBH_cm &lt;dbl&gt;",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#the-split-apply-combine-workflow",
    "href": "r-refresh.html#the-split-apply-combine-workflow",
    "title": "3  R refresher",
    "section": "3.9 The split-apply-combine workflow",
    "text": "3.9 The split-apply-combine workflow\nAggregating sampling units into more coarse-grained levels and calculating summary statistics is one of the most common tasks in data wrangling. Think of things like calculating the proportion of native species in each plot; or calculating the total basal area (sum of DBH) for each plot; or calculating the number of species found in each plot. Those are all examples of aggregating data on individual trees into more coarse-grained plot-level descriptions. dplyr offers us some useful tools for this kind of task in what is commonly called the “split-apply-combine” approach. We first split apart the data by some variable like Plot_ID, then apply functions to calculate descriptions of the data within each split apart level (e.g. within each plot), and then combine all those results in a convenient data object like a tibble or data.frame. To split we use the function group_by and to both apply and combine we use the function summarize. Often we clean up our final product with ungroup as we will see.\n\n3.9.1 Learning the basics of the workflow\nAs a motivating example, let’s figure out the number of species in each plot\n\n# notice we're using the tibble version of the data\nnspp_by_plot &lt;- group_by(all_dat_tib, Plot_ID) |&gt; \n    summarize(nspp = n_distinct(Scientific_name)) |&gt; \n    ungroup()\n\nnspp_by_plot\n\n# A tibble: 530 × 2\n   Plot_ID  nspp\n     &lt;int&gt; &lt;int&gt;\n 1       1     9\n 2       2     9\n 3       3     2\n 4       4     9\n 5       5     4\n 6       6     9\n 7       7     9\n 8       8     9\n 9       9     2\n10      10     8\n# ℹ 520 more rows\n\n\nGreat! It tells us the number of species for each Plot_ID. Let’s unpack the code a bit:\n\ngroup_by(all_dat_tib, Plot_ID) splits the data up by Plot_ID\nsummarize(nspp = n_distinct(Scientific_name)) computes the number of species\n\nnspp = .... means we will store those computed numbers in a new column called nspp\nn_distinct(Scientific_name) is the code that actually calculates the number of distinct species\n\nungroup() is not strictly necessary but its good practice so that our new tibble called nspp_by_plot is not, itself, split up by Plot_ID\n\nBut if we actually want to analyze how number of species is impacted by plot-level variables (like plot area, climate factors, etc.) we need to have more columns than just Plot_ID and nspp. To keep keep, for example, Plot_Area we could do this\n\n# notice we're using the tibble version of the data\nnspp_by_plot &lt;- group_by(all_dat_tib, Plot_ID) |&gt; \n    summarize(area = first(Plot_Area), \n              nspp = n_distinct(Scientific_name)) |&gt; \n    ungroup()\n\nnspp_by_plot\n\n# A tibble: 530 × 3\n   Plot_ID  area  nspp\n     &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1       1 1000      9\n 2       2 1018.     9\n 3       3 1018.     2\n 4       4 1018.     9\n 5       5 1018.     4\n 6       6 1018.     9\n 7       7 1018.     9\n 8       8 1018.     9\n 9       9 1018.     2\n10      10 1018.     8\n# ℹ 520 more rows\n\n\nNotice that summarize can create multiple new columns—here we made area and nspp. We also used the function first which just extracts the first value from the column Plot_Area within each plot. Why the first value? Because for a given plot (and we are splitting the data by plot) all the values of Plot_Area will be the same and we are making a tibble that has one row for each plot, so we need a single number for plot area in each row. Might as well be the first value since they’re all the same.\nBut what if we want more than just area and nspp? We probably are interested in all the climate, geology, and human impact data too. It would be cumbersome to type out all those columns inside summarize. Luckily there is an efficient alternative\n\nnspp_by_plot &lt;- group_by(all_dat_tib, Plot_ID) |&gt; \n    summarize(across(Island:elev_m, first), \n              nspp = n_distinct(Scientific_name)) |&gt; \n    ungroup()\n\nnspp_by_plot\n\n# A tibble: 530 × 18\n   Plot_ID Island  Study Plot_Area  Year Census   lon   lat evapotrans_annual_mm\n     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;                &lt;dbl&gt;\n 1       1 O'ahu … Knig…     1000   2012      1 -158.  21.4                 818.\n 2       2 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n 3       3 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n 4       4 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n 5       5 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n 6       6 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n 7       7 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n 8       8 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n 9       9 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n10      10 Hawai'… Zimm…     1018.  2003      1 -155.  19.4                 934.\n# ℹ 520 more rows\n# ℹ 9 more variables: avbl_energy_annual_wm2 &lt;dbl&gt;, cloud_freq_annual &lt;dbl&gt;,\n#   ndvi_annual &lt;dbl&gt;, rain_annual_mm &lt;dbl&gt;, avg_temp_annual_c &lt;dbl&gt;,\n#   hii &lt;dbl&gt;, age_yr &lt;dbl&gt;, elev_m &lt;dbl&gt;, nspp &lt;int&gt;\n\n\nThe way we get the first value of many different columns is across(Island:elev_m, first). The function across applies first to each of the columns indicated by Island:elev_m.\nThe above printed output helps us confirm that across worked, but the column nspp is not printed, and we might want to see it to make sure it looks right. How to do that? We can use select\n\n# choose a few columns in addition to `nspp`\nselect(nspp_by_plot, c(Plot_ID, Island, lon, lat, nspp))\n\n# A tibble: 530 × 5\n   Plot_ID Island                                lon   lat  nspp\n     &lt;int&gt; &lt;chr&gt;                               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1       1 O'ahu Island (incl. Mokoli'i Islet) -158.  21.4     9\n 2       2 Hawai'i Island                      -155.  19.4     9\n 3       3 Hawai'i Island                      -155.  19.4     2\n 4       4 Hawai'i Island                      -155.  19.4     9\n 5       5 Hawai'i Island                      -155.  19.4     4\n 6       6 Hawai'i Island                      -155.  19.4     9\n 7       7 Hawai'i Island                      -155.  19.4     9\n 8       8 Hawai'i Island                      -155.  19.4     9\n 9       9 Hawai'i Island                      -155.  19.4     2\n10      10 Hawai'i Island                      -155.  19.4     8\n# ℹ 520 more rows\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nUse what you’ve just learned to make a tibble called basal_by_plot that calculates the basal area (again, sum of all DBH measurements) for each plot. Let’s call the basal area column basal_area. Again keep all the plot-level data like we did in making nspp_by_plot.\n\n\nThere are a lot of other interesting things we could calculate: proportion of native species versus non-native, proportion of basal area of native species versus non-native, and many more. We need to think carefully about whether we want to make lots of separate tibbles for all these variables or not. When such a question arises, the answer is almost always do not make separate tibbles. All these tibbles would share exactly the same plot-level data. Rather than separate tibbles for all the new variables we might want to calculate, the better option is making one tibble with different columns for all those variables. That is the tidy approach. Unnecessarily separating tibbles introduces the risk of modifying one and not the other leading to inconsistent analyses of different variables that might not be easy to notice but could drastically alter your interpretation of the data.\n\n\n3.9.2 Increasingly complex calculations inside summarize\nWe have made two separate tibbles just for learning purposes. Let’s now make one big one for storing all the data we might want to summarize at the plot level. The things we’ll calculate are:\n\nnspp: number of species\nnspp_native: number of native species\nprop_spp_native: proportion of species that are native\nbasal: basal area\nbasal_native: basal area of native species\nprop_basal_native: proportion of basal area belonging to native species\ntotal_abund: total abundance (how many trees in the plot)\ntotal_abund_native: total abundance of native species\nprop_abund_native: proportion of abundance belonging to native species\n\nWe know how to count up unique species: n_distinct. We know how to sum up basal area: sum. For the other variables we need more tools. The simplest is getting total abundance: dplyr provides a function n that counts how many rows are in a split apart group created by group_by. Because each row is one tree, figuring out the number of rows will tell us the number of trees. Let’s take what we know, plus n, and get half of the way to the plot-level tibble we want.\n\n# let's name our plot-level data object `dat_by_plot`\ndat_by_plot &lt;- group_by(all_dat_tib, Plot_ID) |&gt; \n    summarize(across(Island:elev_m, first), \n              nspp = n_distinct(Scientific_name), \n              basal = sum(DBH_cm), \n              total_abund = n()) |&gt; \n    ungroup()\n\n# let's look at the new columns we made\nselect(dat_by_plot, Plot_ID:Island, nspp:total_abund)\n\n# A tibble: 530 × 5\n   Plot_ID Island                               nspp basal total_abund\n     &lt;int&gt; &lt;chr&gt;                               &lt;int&gt; &lt;dbl&gt;       &lt;int&gt;\n 1       1 O'ahu Island (incl. Mokoli'i Islet)     9 1408.          75\n 2       2 Hawai'i Island                          9 2015.          80\n 3       3 Hawai'i Island                          2  367.          37\n 4       4 Hawai'i Island                          9 1275.         100\n 5       5 Hawai'i Island                          4 2089.          99\n 6       6 Hawai'i Island                          9 1356.          64\n 7       7 Hawai'i Island                          9 1626.         114\n 8       8 Hawai'i Island                          9 1648.         113\n 9       9 Hawai'i Island                          2  172.          23\n10      10 Hawai'i Island                          8 1370.         116\n# ℹ 520 more rows\n\n\nNotice that to calculate number of rows we don’t have to pass n() any arguments. n is an unusual function in that way, but a very useful one.\nHow are we going to calculate these variables but for native species only? We are going to use booleans. Let’s go back to our clean_toy_data for a moment to understand how booleans are going to help us. To make clean_toy_data a more relevant example, let’s add a native_status column and make the whole thing a tibble called toy_tib.\n\ntoy_tib &lt;- mutate(clean_toy_data, \n                  native_status = c(\"native\", \"native\", \n                                    \"non-native\", \"non-native\")) |&gt; \n    as_tibble()\n\ntoy_tib\n\n# A tibble: 4 × 6\n  tree_ID species   sla   dbh log_dbh native_status\n    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        \n1       1 sp1       3.2   4.4    1.48 native       \n2       2 sp1       2.9   3.6    1.28 native       \n3       3 sp2       5.1   3.4    1.22 non-native   \n4       4 sp2       5.8   5.1    1.63 non-native   \n\n\nSo we made up the native_status column such that “sp1” is native and “sp2” is non-native. Inside out group_by and summarize workflow we can make a boolean by “asking” native_status == \"native\". R answers that question TRUE when a row in native_status does equal \"native\" and FALSE otherwise. We can see that same logic in action when we first looked at filtering:\n\nfilter(toy_tib, native_status == \"native\")\n\n# A tibble: 2 × 6\n  tree_ID species   sla   dbh log_dbh native_status\n    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        \n1       1 sp1       3.2   4.4    1.48 native       \n2       2 sp1       2.9   3.6    1.28 native       \n\n\nThinking back to the start of this tutorial, we also know that booleans can be used with square brackets ([) for subsetting:\n\nspp_vec &lt;- c(\"sp1\", \"sp1\", \"sp2\", \"sp2\")\nnative_vec &lt;- c(\"native\", \"native\", \"non-native\", \"non-native\")\n\nspp_vec[native_vec == \"native\"]\n\n[1] \"sp1\" \"sp1\"\n\n\nIf we have a subset of only native species names, we can calculate how many distinct names there are just like before:\n\nn_distinct(spp_vec[native_vec == \"native\"])\n\n[1] 1\n\n\nLet’s see what that would look like the in group_by and summarize workflow\n\n# not assigning this to anything, just looking\n# and not bothering with all the other site-level data\ngroup_by(all_dat_tib, Plot_ID) |&gt; \n    summarize(nspp = n_distinct(Scientific_name), \n              nspp_native = n_distinct(Scientific_name[Native_Status == \"native\"]))\n\n# A tibble: 530 × 3\n   Plot_ID  nspp nspp_native\n     &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n 1       1     9           7\n 2       2     9           7\n 3       3     2           1\n 4       4     9           6\n 5       5     4           3\n 6       6     9           4\n 7       7     9           6\n 8       8     9           6\n 9       9     2           1\n10      10     8           5\n# ℹ 520 more rows\n\n\nThat last line of code is a little long, we can easily break it across lines without causing errors\n\ngroup_by(all_dat_tib, Plot_ID) |&gt; \n    summarize(nspp = n_distinct(Scientific_name), \n              nspp_native = n_distinct(\n                  Scientific_name[Native_Status == \"native\"]\n              ))\n\nNotice sometimes we make a new like right after the opening ( in a function call, that’s totally acceptable style (some even prefer it).\n\n\n\n\n\n\nNoteYour turn\n\n\n\nWith this same [ subsetting approach we can also calculate basal_native. Try giving that a go yourself. Again, no need to assign your calculations to an object, we’re just figuring things out right now.\n\n\nWe’re going to use a convenient trick to calculate total_abund_native. Booleans can act like numbers when we want them to (specifically TRUE \\(\\Leftrightarrow\\) 1, FALSE \\(\\Leftrightarrow\\) 0). So we can sum up a bunch of TRUEs and FALSEs and its the same as counting all the instances of TRUE:\n\nbool &lt;- c(TRUE, FALSE, TRUE)\n\nsum(bool)\n\n[1] 2\n\n\nBecause each row of the data is an individual tree and each row has one value for Native_Status we just need to sum up Native_Status == \"native\" to figure out the abundance of all native trees.\n\ngroup_by(all_dat_tib, Plot_ID) |&gt; \n    summarize(total_abund = n(), \n              total_abund_native = sum(\n                  Native_Status == \"native\"\n              ))\n\n# A tibble: 530 × 3\n   Plot_ID total_abund total_abund_native\n     &lt;int&gt;       &lt;int&gt;              &lt;int&gt;\n 1       1          75                 66\n 2       2          80                 73\n 3       3          37                 36\n 4       4         100                 44\n 5       5          99                 50\n 6       6          64                 41\n 7       7         114                 63\n 8       8         113                 67\n 9       9          23                 18\n10      10         116                 62\n# ℹ 520 more rows\n\n\nThe last type of variable we need to calculate is a proportion. A proportion is just a fraction: frequency / total_observations. So calculating the proportions we need is as simple as division, for example with abundance:\n\ngroup_by(all_dat_tib, Plot_ID) |&gt; \n    summarize(total_abund = n(), \n              total_abund_native = sum(\n                  Native_Status == \"native\"\n              ), \n              prop_abund_native = total_abund_native / total_abund)\n\n# A tibble: 530 × 4\n   Plot_ID total_abund total_abund_native prop_abund_native\n     &lt;int&gt;       &lt;int&gt;              &lt;int&gt;             &lt;dbl&gt;\n 1       1          75                 66             0.88 \n 2       2          80                 73             0.912\n 3       3          37                 36             0.973\n 4       4         100                 44             0.44 \n 5       5          99                 50             0.505\n 6       6          64                 41             0.641\n 7       7         114                 63             0.553\n 8       8         113                 67             0.593\n 9       9          23                 18             0.783\n10      10         116                 62             0.534\n# ℹ 520 more rows\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nTaking everything we’ve figured out, make a final version of dat_by_plot with columns for all the site-level data (what we achieved with across(Island:elev_m, first)) and all the columns we want to calculate:\n\nnspp\nnspp_native\nprop_spp_native\nbasal\nbasal_native\nprop_basal_native\ntotal_abund\ntotal_abund_native\n\n\n\nWith our site-level data now in hand, let’s make some visualizations",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#visualizing-data-with-ggplot2",
    "href": "r-refresh.html#visualizing-data-with-ggplot2",
    "title": "3  R refresher",
    "section": "3.10 Visualizing data with ggplot2",
    "text": "3.10 Visualizing data with ggplot2\nThe ggplot2 package is the current favorite way to make graphics in R. It’s designed to build graphics by adding different components and stylistic choices until we arrive at the visualization we want. This layered approach allows us to make many different kinds of visualizations with a lot of room for customization.\nThe first layer of a graphic made with ggplot2 is a blank canvas that is aware of all the possible variables in data itself\n\n# first we need to load the package\n# remember `install.packages` if you need it\nlibrary(ggplot2) \n\n# `ggplot` is the core function for making \n# graphics, they all start with ggplot\nggplot(data = dat_by_plot)\n\n\n\n\n\n\n\n\nYes we are jumping straight into using the real plot-level data we just made! And we now have a blank canvas.\nNext we tell ggplot which variables from the data we actually want to visualize and how—which one will go on the x-axis, which on the y-axis, will others be used for color-coding different elements of the graphic, etc. Let’s start by looking at a very basic relationship: how does the size of an inventory plot (Plot_Area) affect the number of species (nspp) found in that plot? Plot_Area should go on the x-axis, nspp should go on the y-axis:\n\nggplot(data = dat_by_plot, mapping = aes(x = Plot_Area, y = nspp)) \n\n\n\n\n\n\n\n\nNow you can see we have our axes set up. The argument mapping is how we indicate which variables we actually want to work with (we “map” the variable to a part of the graph). We have to wrap those variable names in the aes function because the choice of variable is considered a visual aesthetic of the graphic. We still, however, don’t see any data other than the ranges on the axes. That’s because there are many different ways we could visualize Plot_Area and nspp. But for our question we need a scatter plot. In ggplot2 we get that by adding geom_point() to the mix\n\nggplot(data = dat_by_plot, mapping = aes(x = Plot_Area, y = nspp)) + \n    geom_point()\n\n\n\n\n\n\n\n\nWow we have a data visualization! Kind of a funny looking one, but something. All the different “kinds” of visualizations we can make are written in geom_* functions because those different kinds of visualizations are thought of as geometries by the authors of ggplot2.\nBefore we address that our visualization is kind of funny looking, let’s talk about being lazy with code. R lets us be lazy by not always forcing us to write the argument names. It matches up the inputs we give it with the arguments by order. So we can write this ggplot(dat_by_plot, aes(x = Plot_Area, y = nspp)) and ggplot knows that the first input dat_by_plot should correspond to the first argument data. And ggplot knows that the second input aes(x = Plot_Area, y = nspp) should correspond to the second argument mapping. That same approach applies to the x and y arguments in aes, we can leave them unnamed and aes will figure it out. That means we can write a lot less code—be lazy (this approach is literally called lazy evaluation)—and get the same product.\n\nggplot(dat_by_plot, aes(Plot_Area, nspp)) + \n    geom_point()\n\n\n\n\n\n\n\n\nNow what to do about the funny looking visualization. Its really those outliers with really large Plot_Area. When we have a huge range across one axis with outliers, log-transforming it can help\n\nggplot(dat_by_plot, aes(Plot_Area, nspp)) + \n    geom_point() +\n    scale_x_continuous(transform = \"log10\")\n\n\n\n\n\n\n\n\nNice, we see that species richness generally increases with plot area. That makes sense. We achieved the log-transformation with scale_x_continuous(transform = \"log10\"). Breaking that down: scale_x_continuous because we want to change the scale of the x-axis which is a continuous variable and transform = \"log10\" because we want a log-transformation. You might ask why we had to name the argument transform = \"log10\" instead of just typing scale_x_continuous(\"log10\"). The reason is that transform is not the first argument to `scale_x_continuous. Turns out transform is the \\(10^{th}\\) argument and name is the first. We can skip over arguments 1 through 10 because ggplot2 has given them meaningful default values—values it will automatically use for those arguments if we say nothing at all. That is also why calling geom_point() without any arguments works: there are lots of arguments to geom_point, but they all have default values.\nThink back to ecology classes or biogeography. If you remember anything about the “species area relationship” you’ll know that we tend to assume the affect of area \\(A\\) on species richness \\(S\\) is\n\\[\nS = cA^z\n\\]\nIf we log both sides of that equation we get a linear equation\n\\[\n\\log(S) = \\log(c) + z\\log(A)\n\\]\nThat suggests we should log-transform both the x- and y-axes.\n\n\n\n\n\n\nNoteYour turn\n\n\n\nModify the ggplot code to log both the x- and y-axes. Do you have a guess of how to do it? Once you succeed your graphic will look like this\n\n\n\n\n\n\n\n\n\nWhile you’re at it, compare the log-transformed plot and the not-log-transformed plot. What changes about the axes and why?\n\n\nWhat if other variables impact how species richness tends to increase with area? How could we add another variable to the plot without it looking like a ridiculous 3D thing? The most common approach is to color code the points (or perhaps modify another aesthetic like shape or size). Let’s color points according to which island the inventory plots are on.\n\nggplot(dat_by_plot, aes(Plot_Area, nspp, color = Island)) + \n    geom_point() +\n    scale_x_continuous(transform = \"log10\") + \n    scale_y_continuous(transform = \"log10\")\n\n\n\n\n\n\n\n\nCrazy! There seem to be some interesting patterns, but it’s kind of hard to see. We need to make some changes:\n\nshorten the island names so the legend doesn’t take up half the graphic\norder the islands in a meaningful order (how about oldest to youngest)\npick better colors\n\n\n3.10.1 Refining labels in ggplots\nOne option for refining the island names (considered a “label” in ggplot2) is to simply re-name them in the plot itself. We can use the layering function scale_color_discrete to specify what we want the labels to be:\n\nggplot(dat_by_plot, aes(Plot_Area, nspp, color = Island)) + \n    geom_point() +\n    scale_x_continuous(transform = \"log10\") + \n    scale_y_continuous(transform = \"log10\") +\n    scale_color_discrete(labels = c(\"Hawaiʻi\", \"Kauaʻi\", \"Lānaʻi\", \n                                    \"Maui\", \"Molokaʻi\", \"Oʻahu\"))\n\n\n\n\n\n\n\n\nNote that we have to be very careful to list the labels in the same order they appear in the legend. If we mess that up, our graphic will be just plain wrong. For that reason, this approach of changing the labels in the graph, is considered fragile and not best practice. Best practice is to refine the actual values of the variable (Island) in the data object itself. We can use mutate to modify the Island column and within mutate use case_match to create the more refined variable values we want:\n\ndat_by_plot &lt;- mutate(dat_by_plot, \n                      Island = case_match(\n                          Island, \n                          \"O'ahu Island (incl. Mokoli'i Islet)\" ~ \"Oʻahu\", \n                          \"Hawai'i Island\" ~ \"Hawaiʻi\", \n                          \"Maui Island\" ~ \"Maui\", \n                          \"Moloka'i Island\" ~ \"Molokaʻi\", \n                          \"Lana'i Island\" ~ \"Lānaʻi\", \n                          \"Kaua'i Island\" ~ \"Kauaʻi\"\n                      ))\n\nYou’ll notice case_match uses a kind of funny syntax: \"old label\" ~ \"new label\". This syntax indicates a mapping of the original value to a new value.\nThis might also look kind of fragile: what if we misspell the old value we’re trying to replace? Luckily, R will let you know (not the case with scale_color_discrete(labels = c(....), which makes this approach less fragile:\n\nmutate(dat_by_plot, \n       Island = case_match(\n           Island, \n           # deliberate misspelling\n           \"Oahu (incl. Mokoli'i Islet)\" ~ \"Oʻahu\", \n           \"Hawai'i Island\" ~ \"Hawaiʻi\", \n           \"Maui Island\" ~ \"Maui\", \n           \"Moloka'i Island\" ~ \"Molokaʻi\", \n           \"Lana'i Island\" ~ \"Lānaʻi\", \n           \"Kaua'i Island\" ~ \"Kauaʻi\"\n       ))\n\n# A tibble: 530 × 26\n   Plot_ID Island Study  Plot_Area  Year Census   lon   lat evapotrans_annual_mm\n     &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;                &lt;dbl&gt;\n 1       1 &lt;NA&gt;   Knigh…     1000   2012      1 -158.  21.4                 818.\n 2       2 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n 3       3 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n 4       4 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n 5       5 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n 6       6 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n 7       7 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n 8       8 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n 9       9 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n10      10 &lt;NA&gt;   Zimme…     1018.  2003      1 -155.  19.4                 934.\n# ℹ 520 more rows\n# ℹ 17 more variables: avbl_energy_annual_wm2 &lt;dbl&gt;, cloud_freq_annual &lt;dbl&gt;,\n#   ndvi_annual &lt;dbl&gt;, rain_annual_mm &lt;dbl&gt;, avg_temp_annual_c &lt;dbl&gt;,\n#   hii &lt;dbl&gt;, age_yr &lt;dbl&gt;, elev_m &lt;dbl&gt;, nspp &lt;int&gt;, nspp_native &lt;int&gt;,\n#   prop_spp_native &lt;dbl&gt;, basal &lt;dbl&gt;, basal_native &lt;dbl&gt;,\n#   prop_basal_native &lt;dbl&gt;, total_abund &lt;int&gt;, total_abund_native &lt;int&gt;,\n#   prop_abund_native &lt;dbl&gt;\n\n\nWe get an NA for what we misspelled. We would then easily see the NA in the graphic and know something went wrong so we could go back and fix it.\nWith the values themselves now aligning with what we want, we can make our graphic without need to specify labels in scale_color_discrete(labels = c(....\n\nggplot(dat_by_plot, aes(Plot_Area, nspp, color = Island)) + \n    geom_point() +\n    scale_x_continuous(transform = \"log10\") + \n    scale_y_continuous(transform = \"log10\")\n\n\n\n\n\n\n\n\nWe still have the issue of islands showing up in a nonsense order. For this we have to go back to factors, which, recall, are like character objects that have a sense of ranking or ordering. We will again use mutate to modify the Island column.\n\ndat_by_plot &lt;- mutate(dat_by_plot, \n                      Island = factor(\n                          Island, \n                          levels = c(\"Kauaʻi\", \"Oʻahu\", \"Molokaʻi\", \n                                     \"Lānaʻi\", \"Maui\", \"Hawaiʻi\")\n                      ))\n\nAgain, if we misspell any of the levels we will get NA values in the Island column and be able to realize and fix our mistake. And now the islands display in a sensible order\n\nggplot(dat_by_plot, aes(Plot_Area, nspp, color = Island)) + \n    geom_point() +\n    scale_x_continuous(transform = \"log10\") + \n    scale_y_continuous(transform = \"log10\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nThe data wrangling we just did of shortening island names and making them factors was not exactly best practices for two reasons:\n\nwe wrote-over the same object name; sometimes that’s unavoidable, but when we can avoid it we should because it introduces opportunity for us to confuse ourselves\nwe modified the data object data_by_plot without changing the objects it derives from: all_dat_tib and, in turn, all_dat. That means if we make a different data object from all_dat_tib or make other graphics/analyses directly from all_dat the variable values will be in conflict. To fix the conflict (which we might plain forget to do) we’d have to write extra redundant code.\n\nConflicting data versions and redundant code are the things of nightmares for the data analyst!\nSo your task is to go all the way back in the script to where we first made all_dat and modify the Island column so the values have short (correct) names and are factors with appropriately ordered levels. Then make sure you also\n\nre-run the code that makes all_dat_tib\ndelete the code modifying Islands in the dat_by_plot object\nand re-run the newly cleaned-up code making dat_by_plot\n\n\n\nWhile we are on the topic of labels, we might as well improve the x- and y-axis labels. The default labels, you may notice, are just the column names. We could modify the data object again to have column names equal to the axis names we want. This is actually not the conventional approach; instead we usually modify the axis labels in the ggplot itself for two reasons:\n\noften we want axis labels in normal human language with spaces and/or punctuation; spaces and some punctuation (but not all) can be allowed in column names, but it is not recommended\nthe chances for introducing data representation errors are much lower in re-labeling axes compared to re-labeling data values\n\nTo change axis labels we use xlab and ylab.\n\nggplot(dat_by_plot, aes(Plot_Area, nspp, color = Island)) + \n    geom_point() +\n    scale_x_continuous(transform = \"log10\") + \n    scale_y_continuous(transform = \"log10\") +\n    xlab(\"Plot area (ha)\") +\n    ylab(\"Number of species\")\n\n\n\n\n\n\n\n\n\n\n3.10.2 Storing a ggplot as an object\nOne pretty cool think about ggplot is that you can store a graphic as an object. Then if you want to continue adding layers to the visualization you can add them to the object like this:\n\n# store the basic graph set-up in an object \nsar &lt;- ggplot(dat_by_plot, aes(Plot_Area, nspp, color = Island)) + \n    geom_point() +\n    scale_x_continuous(transform = \"log10\") + \n    scale_y_continuous(transform = \"log10\")\n\nsar\n\n\n\n\n\n\n\n# now further refinement can be added to the *object*\nsar +\n    xlab(\"Plot area (ha)\") +\n    ylab(\"Number of species\")\n\n\n\n\n\n\n\n\n\n\n3.10.3 Customizing color\nThe last customization of the graphic we set out to make was picking better colors. ggplot2 has a lot of options to customize colors. All these options exist as functions with names starting with scale_color_*, including scale_color_manual which lets you have complete control over color coding data. Often complete control is over rated and we can let ggplot2 automate some choices for us. A crowd favorite are the viridis color palettes. We will use scale_color_viridis_d with the d indicating that the data we are coloring have discrete (i.e. not continuous number) values.\n\n# remeber, we stored the core graphic in the `sar` object\nsar +\n    xlab(\"Plot area (ha)\") +\n    ylab(\"Number of species\") +\n    scale_color_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\nThe argument option = \"magma\" chooses from several different pallets offered by viridis and seemed fitting for these data.\n\n\n\n\n\n\nNoteYour turn\n\n\n\nHave a look at other arguments in scale_color_viridis_d (use the help documentation accessed with ?scale_color_viridis_d) to see if you can further refine the color coding of islands\n\n\n\n\n3.10.4 Faceting\nThe patterns in the data are, frankly, still hard to see because multiple points fall on top of each other. There could be a few different approaches taken (some of which we’ll look at in other contexts) like making points semi-transparent (although that works best when we are not color coding points) or jittering points (although that is not ideal for a scatter plot with numeric variables for both axes). Instead, our best option is to add facets, which split the data across multiple panels so we can see one group (Island in this case) per panel. Faceting is one super power of ggplot2.\n\nsar +\n    xlab(\"Plot area (ha)\") +\n    ylab(\"Number of species\") +\n    scale_color_viridis_d(option = \"magma\") +\n    facet_wrap(vars(Island))\n\n\n\n\n\n\n\n\nThe function facet_wrap needs to be told what variable to facet by, in this case Island. For reasons obscured deep in the design choices of ggplot2 we have to put that variable inside the vars function for it to be used in faceting.\nWith a faceted graphic the colors are now not strictly necessary, but neither are they unnecessary. What is, however, unnecessary is the legend so let’s remove it\n\nsar +\n    xlab(\"Plot area (ha)\") +\n    ylab(\"Number of species\") +\n    scale_color_viridis_d(option = \"magma\") +\n    facet_wrap(vars(Island)) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe last change we might like is making the points slightly transparent. That wasn’t a great idea when all the islands had points on top of each other, but now with the facets, the transparent points can help us see if some areas of the graphic have a high density of data points compared to others. The argument alpha sets transparency and can be passed to either geom_point or scale_color_viridis_d. We’ll pass it to scale_color_viridis_d\n\nsar +\n    xlab(\"Plot area (ha)\") +\n    ylab(\"Number of species\") +\n    scale_color_viridis_d(option = \"magma\", alpha = 0.5) +\n    facet_wrap(vars(Island)) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n3.10.5 Coloring data based on a numerical variable\nUp to now we used color to represent a categorical variable—island. We can also use color to represent a numerical variable. For example, what if the effect of area on species richness is modulated by the proportion of native versus non-native\n\n# we'll make a ggplot object for ease of\n# modifying later\nsar_nat &lt;- ggplot(dat_by_plot, \n                  aes(Plot_Area, nspp, \n                      color = prop_abund_native)) + \n    geom_point() +\n    scale_x_continuous(transform = \"log10\") + \n    scale_y_continuous(transform = \"log10\") + \n    xlab(\"Plot area (ha)\") +\n    ylab(\"Number of species\") +\n    facet_wrap(vars(Island)) \n\n# view our first draft graphic\nsar_nat\n\n\n\n\n\n\n\n\nSomething interesting is going on here for sure, but the color pallet makes it a little hard to tell. Again the viridis color pallets will help us more clearly see the patterns. We now use scale_color_viridis_c, with c for continuous since the data we use for coloring are numeric.\n\nsar_nat +\n    scale_color_viridis_c()\n\n\n\n\n\n\n\n\nNow we can really see that most of the time high richness plots are dominated by native trees, low richness plots are dominated by non-native trees. But also we can see there are plenty of deviations to this pattern, especially with some very diverse plots being mixed and some mono-dominate (single species) plots on Hawaiʻi being all native.\nOne last cosmetic fix we might want to make is changing the name of the legend. We do this simply with the name argument passed to scale_color_viridis_c\n\nsar_nat +\n    scale_color_viridis_c(name = \"Proportion\\nnative trees\")\n\n\n\n\n\n\n\n\nNotice a little trick there to make the name wrap across two lines: the \\n between “Proportion” and “native” introduces a newline. It is easy to introduce a newline when directly typing the text, but when text is pulled from the data (e.g. axis labels) there are better automated approaches, for example the label_wrap function in the scales package.\n\n\n3.10.6 Graphics to show central tendency and spread across groups\nSo far we have looked extensively at scatter plots. But there are a lot of other graphics out there. One very common group of graphics are those that show how the central tendency of data (and the spread about the center) change across different groups. One intuitive grouping we have is island, so let’s look at how some variables differ across islands. It is also a good idea to get a sense of how our data look before we embark on statistical analyses: understanding how the data look could inform what considerations we want to make in our analyses.\n\n3.10.6.1 The boxplot\nThe box plot is a classic visualization of how the central tendency and spread of data change across groups. Let’s look at the density of individuals per plot across islands.\n\nggplot(dat_by_plot, aes(Island, total_abund / Plot_Area)) +\n    geom_boxplot() +\n    ylab(expression(\"Tree density (\"*ha^-1*\")\"))\n\n\n\n\n\n\n\n\nThree things to notice here: we can do math inside ggplot commands (e.g. we calculated density as total_abund / Plot_Area), the geom_* for a box plot is geom_boxplot, and we made a fancy y-axis label. In order the make “per hectare” (ha\\(^{-1}\\)) we used the expression function. This function prints out text as is (e.g. \"Tree density (\") but then formats math syntax as math text (e.g. *ha^-1* gets formatted as \\(ha^{-1}\\) and the * indicate no spaces).\n\n\n\n\n\n\nNoteYour turn\n\n\n\nMake a box plot of a climatic, geologic, or human impact variable across the islands.\n\n\n\n\n3.10.6.2 The histogram\nHistograms are a great way to get a sense of how the data are distributed, including their central tendency and spread. Because ggplot2 makes it so easy to facet graphics, we can easily view multiple histograms stack on top of each other\n\nggplot(dat_by_plot, aes(total_abund / Plot_Area)) +\n    geom_histogram() +\n    facet_grid(vars(Island)) +\n    xlab(expression(\"Tree density (\"*ha^-1*\")\")) +\n    ylab(\"Number of plots\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nBecause Hawaiʻi contains so many plots, the histogram heights are hard to read for the other plots. In such a situation we can consider if we want to over-ride the default graphic behavior and make the y-axis limits independent for each facet:\n\nggplot(dat_by_plot, aes(total_abund / Plot_Area)) +\n    geom_histogram() +\n    facet_grid(vars(Island), scale = \"free_y\") +\n    xlab(expression(\"Tree density (\"*ha^-1*\")\")) +\n    ylab(\"Number of plots\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nAnother solution is to use the increasingly popular “ridgeline plot” which shows (by default) a smoothed density representation of a histogram, and stacks the density curves in visually pleasing way. These graphics require an additional package: ggridges.\n\nlibrary(ggridges)\n\nggplot(dat_by_plot, \n       aes(total_abund / Plot_Area, y = Island, fill = (Island))) +\n    geom_density_ridges() +\n    scale_fill_viridis_d(option = \"magma\") + \n    theme(legend.position = \"none\")\n\nPicking joint bandwidth of 0.0208\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nMake a ridgeline plot of a climatic, geologic, or human impact variable across the islands.\n\n\n\n\n\n3.10.7 Graphics to show proportions and frequencies\nYou may have seen many a bar graph (hopefully with error bars) used to show mean and uncertainty, but communicating that type of information is better suited to the visualizations we just covered. The bar graph is great for proportions and frequencies.\nWhen introducing the data we were curious about the number of plots where greater than three quarters of their trees belong to native species. We found that across the entire pae ʻāina this proportion is about 70%. But what about for each island? If we want to make a visual for that question, a bar graph would be a great choice.\n\nggplot(dat_by_plot, aes(Island, fill = prop_abund_native &gt;= 0.75)) +\n    geom_bar()\n\n\n\n\n\n\n\n\nNotice we again did some math inside aes: we created a boolean of whether or not prop_abund_native is greater than or equal to 0.75. But the y-axis is showing as a count rather than a proportion. To get a proportion we use position = \"fill\"\n\nggplot(dat_by_plot, aes(Island, fill = prop_abund_native &gt;= 0.75)) +\n    geom_bar(position = \"fill\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteYour turn\n\n\n\nImprove this graphic! Make it have better labels (pay close attention to the y-axis, its a proportion, but of what? What about the legend?) and give it better colors. You might find\n\n\n\n\n3.10.8 Themes\nWhile we’re on the topic of customizing appearances, let’s talk about themes. The background grid seems very unnecessary in this last bar graph. How could we remove it? The quickest way is choosing a different theme for the visualization. There are many named themes like theme_classic that bundle multiple different cosmetic aspects of the graphic\n\nggplot(dat_by_plot, aes(Island, fill = prop_abund_native &gt;= 0.75)) +\n    geom_bar(position = \"fill\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nYou can check out ?theme_classic and it will take you to a help page with many different named themes.\nDifferent packages also offer custom themes. One popular one is theme_cowplot from the cowplot package.\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:gt':\n\n    as_gtable\n\nggplot(dat_by_plot, aes(Island, fill = prop_abund_native &gt;= 0.75)) +\n    geom_bar(position = \"fill\") +\n    theme_cowplot()",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#resources",
    "href": "r-refresh.html#resources",
    "title": "3  R refresher",
    "section": "3.11 Resources",
    "text": "3.11 Resources\nThe Carpentries has a great R tutorial for ecologists. A few differences about their approach and ours: they use the magrittr pipe and instead of loading only the packages they need (like dplyr and readr) they load a massive collection of packages: tidyverse. That might seem convenient, but my (me, Andy, here) experience is that understanding which functions come from which specific packages is useful for writing reusable code.\nAnother great resource is a course website by Jenny Bryan who is one of those pros along side Hadley Wickham. The course website also uses %&gt;% (though Jenny Bryan has more recently switched to |&gt;) and tidyverse, so pay mind to those differences.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "r-refresh.html#references",
    "href": "r-refresh.html#references",
    "title": "3  R refresher",
    "section": "3.12 References",
    "text": "3.12 References\n\n\n\n\nWickham H. 2014. Tidy data. Journal of statistical software 59:1–23.\n\n\nWickham H, François R, Henry L, Müller K, Vaughan D. 2023. Dplyr: A grammar of data manipulation. Available from https://CRAN.R-project.org/package=dplyr.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R refresher</span>"
    ]
  },
  {
    "objectID": "dynamic-doc.html",
    "href": "dynamic-doc.html",
    "title": "4  Dynamic documents with quarto",
    "section": "",
    "text": "4.1 What is a dynamic document and how does quarto make it work?\nDynamic documents are those that respond to changes in our data or code by automatically updating to reflect those changes. With dynamic documents you no longer need to have code in one file, graphics in multiple files elsewhere, and then copy and paste everything into another document when actually writing a paper, a report, or notes. Everything lives in one file and the nicely formatted finished product is generated from that one file.\nQuarto is the powerhouse that enables us to make these dynamic documents. Under the hood, quarto uses markdown, pandoc, and knitr to make all this possible. Quarto itself is not (just) an R package, so to seek help your best bet is the extensive documentation online. knitr is an R package, but the best way to learn about it is also its extensive documentation online. We will cover much of the markdown you need to know in the next section. Only if you go deep into customizing the appearance of your documents do you need to dive into pandoc. For now, rest assured that what these incredible tools allow you to do is convert your code, figures, and simply-formatted text into beautiful pdf, html, or (beautiful? maybe not) word documents.\nIt should be noted that quarto is the inheritor of Rmarkdown and pretty much everything on the web (or already in your head) about using Rmarkdown will apply to quarto as well.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic documents with *quarto*</span>"
    ]
  },
  {
    "objectID": "dynamic-doc.html#quick-start",
    "href": "dynamic-doc.html#quick-start",
    "title": "4  Dynamic documents with quarto",
    "section": "4.2 Quick-start",
    "text": "4.2 Quick-start\nHere is an example of a quarto document:\n\n\n---\ntitle: \"The title\"\nauthor: A research group\nformat: pdf\n---\n\n```{r}\n#| label: setup\n#| include: false\n\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\n```\n\n# Introduction\n\nSome background\n\n## More specific background\n\n@fig-concept explains our approach.\n\n```{r}\n#| label: fig-concept\n#| echo: false\n#| fig-width: 2\n#| fig-height: 2\n#| fig-cap: \"Our conceptual workflow, nani me he pua lā\"\n\ndat_concept &lt;- tibble(\n    rad = seq(0, 2 * pi, length.out = 200), \n    x = cos(rad) * cos(4 * rad), \n    y = sin(rad) * cos(4 * rad)\n)\n\nggplot(dat_concept, aes(x, y)) +\n    geom_polygon(fill = \"coral\")\n```\n\n# Methods\n\nOur analysis uses this kind of code\n\n```{r}\n#| label: step-01\n\nx &lt;- 1:10\nsum(x)\n```\n\n\nWe can render that document into a pdf file that looks like this:\n\n\n\n\n\n\n\n\n\nPretty cool! But how does it all work. Here we go:\n\n4.2.1 Rendering\nRendering is made simple by RStudio. When you have a quarto document open, you will see a “Render” button. Hit that button and the rendering magic happens for you! RStudio comes pre-loaded with the quarto software and its dependencies needed for rendering. Rendering will produce an actual pdf file, and a preview of that file will show up in the “Viewer” tab.\n\nIf you want more control over rendering you can also use the R function quarto_render from the package quarto. For example in the console you could run\n\nlibrary(quarto)\n\nquarto_render(\"example_quarto.qmd\")\n\nHave a look at ?quarto_render for all the arguments that let you control the specifics of how your document is rendered.\nThat’s rendering, but how does the actual quarto document work\n\n\n4.2.2 YAML header\nThe first bit of the quarto document looked like this:\n\n\n---\ntitle: \"The title\"\nauthor: A research group\nformat: pdf\n---\n\n\nIn between the --- is what’s called the YAML header. This specifies metadata about the document. For example, we want to make a pdf document with title “The title.” If we wanted to instead render an html document, all we need to do is change format: pdf to format: html.\nYAML lets you provide a lot of information that customizes or even adds content (like a bibliography) to your document. Here is a non-exhaustive accounting of some YAML options.\n\n\n4.2.3 Markdown syntax\nMarkdown is a markup language that lets you format text with simple plain text, meaning you can produce something fancy looking like different headers, lists, tables, with simple plain text characters like # and -. Alternative markup languages (like html or latex) require more laborious “markup” tags to achieve similar results to markdown, thus the attempt at a humors name for markdown. Here is an overview on markdown syntax.\n\n4.2.3.1 General formatting\nFor italics, wrap text in a single asterisks *like this*; for bold, wrap text in double asterisks **like this**; and for italic bold, wrap text in triple asterisks ***like this***.\n\n\n4.2.3.2 Headers\nMarkdown creates section headers with the hashtag: # is an H1 header, ## is an H2 header (sub-header), ### is an H3 header (sub-sub-header) and so on. You saw examples of these in the above example quarto document.\n\n\n4.2.3.3 Lists\nBullet lists are generated with hyphens - like this:\n\n- one item\n- another item\n\nWhich will be rendered as:\n\n\none item\nanother item\n\n\nNumbered lists are generated with numbers:\n\n1. first\n1. second\n\ngets rendered to\n\n\nfirst\nsecond\n\n\nNote: you don’t need to use correct numbering, markdown figures it out for you.\nYou can make hierarchical lists by tab indenting. This:\n\n- one item\n    - one smaller thought\n    - and another\n- another item\n\nproduces this:\n\n\none item\n\none smaller thought\nand another\n\nanother item\n\n\nAnd this:\n\n1. first\n    i. additionally \n    iii. and more\n2. second\n\nproduces this\n\n\nfirst\n\nadditionally\nand more\n\nsecond\n\n\n\n\n4.2.3.4 Hyperlinks and images\nThose two items seem unrelated, but their markdown syntax is similar. You can link to a URL like this: [Our website](https://uhm-biostats.github.io/stat-mod/) which will produce formatted text like this:\n\nOur website\n\nIncluding an image looks almost the same. This:\n\n![An image from Kawika et al. 2018](img/data-intro/wao.png)\n\nwill be rendered as this:\n\n\n\n\n\nAn image from Kawika et al. 2018\n\n\n\nif you were to have that image file wao.png saved at the location img/data-intro.\nYou can also include an image from a URL:\n\n![Remember this old map?](https://uhm-biostats.github.io/stat-mod/datasets-intro_files/figure-html/fig-map-1.png)\n\n\n\n\n\n\nRemember this old map?\n\n\n\n\n\n4.2.3.5 Tables\nTables can be simply created with markdown like this:\n\n|group | value|\n|:-----|-----:|\n|A     |     1|\n|A     |     2|\n|B     |     3|\n\nwhich is in turn rendered as\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.4 Code\nOne of the most awesome things about quarto and Rmarkdown is that you can write code directly into the document and run it during the rendering process. We saw that in the example quarto document.\nCode lives in “chunks” and chunks have many options that you can specify to determine how and if the code is run and presented. Those options were first defined by knitr and are not incorporated into quarto. See documentation here.\nCode chunks get indicated with triple back tick marks: ``` and the language of the code in curly brackets ({r}, we will only be using R) like this from the example document:\n\n\n```{r}\n#| label: step-01\n\nx &lt;- 1:10\nsum(x)\n```\n\n\n\neval(parse(text = q[ii[-c(1:2, length(ii))]]))\n\n[1] 55\n\n\nWe see the result of the computation gets printed out below the code. But what is this all about:\n\n\n#| label: step-01\n\n\nThe #| is called a “hash pipe” (either the people who developed quarto are huge nerds who don’t know better, or they know exactly what they’re doing). The hash pipe is how we indicate code chunk options. A good habit to be in is always providing a label: to each code chunk. Some other options we’ve seen are include: which tells quarto if we want the code itself and its output to be printed in the rendered output; The default is true so include: false means run the code, but don’t print it. We also say echo: false which means don’t print the code, but do print the output. Other options we saw like fig-width: are for specifying details about how figures generated by code are displayed.\n\n4.2.4.1 Figures and tables from code\nThat’s right! We can make figures with code and have them displayed in our rendered documents. One\nWe can also make markdown-formatted tables with help from the knitr package. Here’s an example\n\n\n\n```{r}\n#| label: table-example\n\nlibrary(knitr)\n\n# make a simple data.frame\nd &lt;- data.frame(group = c(\"A\", \"A\", \"B\"), \n                value = 1:3)\n\nkable(d)\n```\n\n\n\nwhich produces the markdown table that previously we made by hand:\n\n|group | value|\n|:-----|-----:|\n|A     |     1|\n|A     |     2|\n|B     |     3|\n\nand is in turn rendered as\n\n\n\n\n\n\n\n\n\n\n\n4.2.4.2 Cross referencing\nWe often want to refer to an figure of table like “Figure 1 shows such and such. Table 3 shows the other thing.” To make the figure and table output of our code cross-referencable we need to use special labels inside the code chunks.\nIn the example quarto file we made a figure and then referenced it. To allow that, we used this special code chunk label:\n\ncat(q[grep(\"label: fig-concept\", q)])\n\n#| label: fig-concept\n\n\nThe special part is that the label name starts with fig-. Then in our text we could cross reference the figure with the markdown code @fig-concept.\nTables are similar, but we use the special label tag tbl-. For example if we made a table in a code chunk with #| label: tbl-one_tab, we could then cross reference the table with @tbl-one_tab.\nThere is a lot more to learn about cross referencing (including in cases where we don’t make figures or tables with code) and you can read more here.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic documents with *quarto*</span>"
    ]
  },
  {
    "objectID": "dynamic-doc.html#now-its-your-turn",
    "href": "dynamic-doc.html#now-its-your-turn",
    "title": "4  Dynamic documents with quarto",
    "section": "4.3 Now it’s your turn",
    "text": "4.3 Now it’s your turn\nIn the previous chapter you produced an extensive R script with notes and code. Copy-paste and modify that big unruly R script into a nice qmd file and render it as a pdf. This pdf will be your own personal definitive R study guide and reference manual.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic documents with *quarto*</span>"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "5  Managing code with git and GitHub",
    "section": "",
    "text": "5.1 What is git and GitHub and how do they help us?\nCoding and analyzing data are complex tasks. We often need to try different approaches, step away from our code for a while, or share code with others to get their input and collaboration. We might often find ourselves in a situation like this: we make a script, say analysis.R, and then revise it based on feedback, but we don’t want to loose our old work, so we call the new version analysis_v2.R. Then we can’t figure out how to make a plot we want, so we email it to a friend, they edit it and send back a new file called analysis_v2_ajr-edits.R. We resolve those edits and add some other code and save the file with the date so we remember where we were: analysis_2025-01-28.R. But then we don’t get a chance to work on the analysis again for two months. By that time, we can’t remember if analysis_v2.R is the right version or analysis_v2_ajr-edits.R or analysis_2025-01-28.R. You can’t even remember exactly what the edits were and why you needed them.\nAvoiding this situation is the job for git and GitHub. Git is a kind of version control software that runs locally on your computer and tracks changes to files. Those files live in a project folder (usually called a “repo” short for “repository”). GitHub is a web-based platform that hosts git repositories online, allowing you to back up your work, share it with others, and contribute to others’ projects too.\nTogether, git and GitHub help us in several ways. They provide a complete history of our work. Every change we make is recorded with a timestamp and a description we write about what is important in that change. If we make a mistake or want to try something experimental, we can easily revert back to an earlier version. GitHub and git facilitate collaboration. Multiple people can work on the same project simultaneously, and git helps merge their contributions together. Given the web-based hosting and backup, this is kind of like google docs for code (but a little less automated because we manually document and describe changes). GitHub makes it easy to share code, review changes, and give and receive feedback. The feedback bit is shockingly helpful via a tool called “issues” which we will discuss in the Workflow section. Hosting code on GitHub is also a real way to work toward open and reproducible science. Anyone can see exactly how we carried out our analysis.\nAs another important motivator: we will use git and GitHub for all class assignments moving forward.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#using-git-and-github-with-help-from-rstudio",
    "href": "github.html#using-git-and-github-with-help-from-rstudio",
    "title": "5  Managing code with git and GitHub",
    "section": "5.2 Using git and GitHub with help from RStudio",
    "text": "5.2 Using git and GitHub with help from RStudio\nTo start working with git and GitHub we will need to make a free GitHub account and to install the actual git software on our computers. Here are external instructions for those tasks:\n\nmake a free GitHub account; no need to follow the “Next steps” (although eventually you should set up 2FA)\ndownloaded and installed git (click on your operating system here for instruction)\n\nInstead of step (2), if you think you have done this before, check if git is already on your machine by opening the terminal (from RStudio is fine) and run this command:\ngit --version\nYou will see a version number if git is available, you will see an error if it is not.\nNow we can integrate git, GitHub, and RStudio. This is not necessary for using git and GitHub but it makes the experience easier, especially because we are already using RStudio.\nWe will use two R packages to help us connect RStudio and GitHub: usethis and gitcreds. Let’s install those packages by typing the following in the R console\n\ninstall.packages(\"usethis\")\ninstall.packages(\"gitcreds\")\n\nA common problem has been that usethis has a dependency on the package crayon which is currently not playing nice. If you have errors involving crayon try this:\n\ninstall.packages(\"crayon\", type = \"source\")\ninstall.packages(\"usethis\")\n\nNow we can use functions from those packages to help us continue. First we use usethis to set up a personal access tokin that will serve as our login credential with GitHub. We do that by typing this into the R console:\n\nlibrary(usethis)\ncreate_github_token()\n\nThat should open up a web browser page where you might be promoted to log into your GitHub account. Do that and then on the next page scroll to the bottom and click the “Generate token” button. You will then see on a new page a string of numbers and letters—that’s the token. Copy that token and paste it somewhere temporarily, like a blank text editor document or a blank R script, where you paste it doesn’t matter—you’ll soon delete it.\nNow come back to RStudio and run the following in the R console:\n\nlibrary(gitcreds)\ngitcreds_set()\n\nThis will prompt you to enter a password or token; now you can paste the token you just generated here and hit enter. You should now be good to go!\nIt’s possible you’ve already gone through this process. If that’s the case you’ll see this:\n-&gt; Your current credentials for 'https://github.com':\n\n  protocol: https\n  host    : github.com\n  username: PersonalAccessToken\n  password: &lt;-- hidden --&gt;\n\n-&gt; What would you like to do? \n\n1: Keep these credentials\n2: Replace these credentials\n3: See the password / token\n\nSelection: \nEnter 1 for your selection. There might be be an error message but you can ignore it.\nNow we should hopefully be all connected!\n\n5.2.1 Testing our connection\nNow we’ll double check that you’ve connected RStudio with GitHub by going to our GitHub profile and making a new repository. We’ll then copy (i.e. “clone”) this repo to our computers, modify it locally, and send those changes back to GitHub via git (all this sounds overly technical right now, we will gain a deeper understanding in the next section on learning the git workflow).\nFirst let’s make a new repo. Go to your GitHub profile and click the plus sign in the top right, select “New repository”.\n\nGive the repository a name, a description, make it public and select the option to add a README. Then hit “Create repository”\n\nNext hit the green “Code” button and copy the HTTPS URL to your clipboard.\n\nNow head back to RStudio and selection File &gt; New Project. Then select “Version control” then “Git”. Finally, paste the HTTPS URL that you copied from GitHub. The “Project directory name” should auto-population. Choose a meaningful place on your computer to house this project, select the option to open in a new session and hit “Create Project”.\n\nYou should now have a new project open! Navigate to the “Git” tab in RStudio and notice that two file names are listed with yellow question marks by them.\n\nThose files were auto-generated by RStudio. The question marks indicate that they are new files which git is currently not tracking. Click the radio button next to each one under the “Staged” column. Checking those buttons stages the files. We can now hit “Commit” to add a commit message and commit the changes. If you have success it should look something like this (possibly with a warning about username or email, that’s ok!):\n\nOnce we’ve committed the changes we can send those changes to GitHub by hitting the “push” button Hopefully the result of hitting “push” looks something like this (possibly with a warning about username or email, again, that’s ok!):\n\nGo back to your web browser, refresh the GitHub repo page, and you should see the new files you just pushed!\nWhile committing and pushing you might get warning messages about your username and email. If you see these warnings you need to let git and GitHub know who you are. You can do this through the Terminal. To open a terminal tab through RStudio go to Tools &gt; Terminal &gt; New Terminal. This will open a new terminal tab next to the R Console tab. The terminal is different than the R Console. You can think of the terminal as accessing the guts of your computer. Raw R commands will not work here. However, we can interact with git here and that’s what we need to do to identify ourselves. So in the terminal type\ngit config --global user.name \"github_user_name\"\nreplace github_user_name with your actual username; hit enter. Then type\ngit config --global user.email \"my_email\"\nagain replace my_email with the actual email address you used to register your GitHub account; hit enter.\nNow your identity is known and you should be all set!",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#learning-the-git-and-github-workflow",
    "href": "github.html#learning-the-git-and-github-workflow",
    "title": "5  Managing code with git and GitHub",
    "section": "5.3 Learning the git and GitHub workflow",
    "text": "5.3 Learning the git and GitHub workflow\nWe have in fact already seen a big part of this workflow when we confirmed that RStudio is properly communicating with GitHub. Now we will work toward really understanding it.\n\n5.3.1 Clone\nCloning means copying a remote repo, like one on GitHub, to your local computer. This is how you typically first start a local git repo. Cloning is a little more complex than just downloading all the files: cloning also maintains the relationship between your local copy and the remote repo as well as all the version history.\n\n\n5.3.2 Stage and commit\nOnce you have your git repo locally, you will start changing files. “Staging” changes (hitting the radio button next to a file name in RStudio’s “Git” panel) let’s git know to pay attention to the changes you made. We can stage changes from one or multiple files at a time. Once staged, we can then write a commit message. This message briefly records what changes were made and (perhaps) why or how. These messages should be brief, think of them as notes to your future self (or collaborators). Recall from the intro to this chapter that git enables you to roll back to previous versions of your code (for example if you messed something up). The commit message is your way of knowing which version to roll back to. We will cover how to roll back or recover past versions soon.\nThese changes, and their commit messages, still only exist locally on your computer. The next steps are about interacting with the repo hosted on GitHub.\n\n\n5.3.3 Push and pull\nIf we want our local changes to be reflected in the GitHub repo, we need to “push” them. You can push from RStudio by hitting the green up arrow in the “Git” panel. Pushing does just that, it pushes all your committed changes to the GitHub repo.\nConceptually, you should think of your local version of the repo as your temporary, personal copy of the official GitHub repo. Your computer could die, you could loose it, a cat could walk over the keys and mess it all up. Your local version is not permanent, but the GitHub version is. The fact that your local copy is ephemeral also means you should commit often, push often, and pull often.\nWait, what is pull? Pull is how you keep your local copy up-to-date with the official copy on GitHub. You can pull from RStudio by hitting the turquoise down arrow in the “Git” panel. But you might ask: how could the official copy change? Primarily through the commits and pushes of your collaborators. More on that in “Working collaboratively.”\n\n\n5.3.4 Recovering past versions\nTo actually recover previous versions of your code, you have a few options. You can view the history of your repo (as it is preserved from within your local copy) right inside RStudio. On the “Git” panel, hit the watch icon.\n\nA window will pop-up showing the history of all commits along with their messages.\n\nYou can click on any of those commits to see the past versions of your work. The easiest way to recover it is to copy the code of the past version and paste it into the script you’re currently working on. This gives you maximum control to selectively recover specific parts of your past work.\nAnother option is to use code in the terminal to roll back the entire repo to a past version. When you clicked a specific commit, you saw, amoung other information, a long SHA key. You can paste that SHA key into this code to roll back to that commit:\ngit revert &lt;SHA-key&gt;\nYet another option is to use the GitHub interface to view past versions. On the page for the GitHub repo you will see another watch-looking icon.\n\nClick that and again you can see a list of past commits which you can click through to see the associated versions of the files/scripts at the times of those commits. You can again copy-paste the code you might want from these past versions.\n\n\n5.3.5 Issues: toward working collaboratively\nIn collaborative work (whether collaborating with other people or “your future self”) GitHub issues are a great tool for managing and organizing work. When you are working through a project by yourself or with others, you might find a need to jot down notes or to-do lists. Or you might find that code somebody else has written is not working the way it should. These are all great use cases for Issues. The idea of issues on GitHub originated with the need for users of open source software to report bugs—aka issues—to developers. But the utility of the issues feature goes far beyond bug reports.\nTo make a new issue select the Issues tab on any GitHub repo and then click the green “New issue” button. Many issues in the context of team coordination and task management can be expressed simply with a title, for example, “complete code for xyz task.” But you also have the ability to add detail in the “Add a description” field and, conveniently, you can format the description with markdown. So if your task “complete code for xyz” has multiple components and you want to list those out, perhaps even as a checklist, you could write in the description:\n\n- [ ] wrangle data so samples are rows\n- [ ] clean up column names\n- [ ] visualize x versus y\n\nWhich will render as\n\nwrangle data so samples are rows\nclean up column names\nvisualize x versus y\n\nYou can check your markdown by clicking the “Preview” tab of the description field. You can also use markdown (or simple plain text) to comment on issues and provide further detail, notes, or to-do’s through comments.\nYou can also assign issues to yourself or your team mates by clicking on the gear symbol next to “Assignees.” Assigning work is an excellent way to coordinate a project.\nThere are a lot of further refinements you can make with issues like labels and grouping them in projects and/or milestones. You can read more about that here.\n\n\n5.3.6 Working collaboratively: branches and pull requests\nThe most challenging part about working collaboratively—from a technical perspective—is harmonizing different versions of code and text that you and your team mates produce. This is the job of branches, forks, and pull requests. First let’s imagine a scenario:\nYou and a colleague are working on the same project which has one repo. You make some changes and to files locally, commit those changes and (try) to push them to GitHub. And your colleague (naturally) will do the same. But both of you will often end up with errors like this:\n\nThat means your work on your local computer is missing updated changes that are reflected on GitHub (your colleague must have pushed changes before you). No problem you might think! The error says to try pulling. But when you pull, you might see an even worse error like this:\n\nThat type of merge conflict results from multiple people modifying the same part of the same file. For example, perhaps you modify some plotting code in a script called data_viz.qmd to look like this:\n\n# use faceting to show different groups\nggplot(dat, aes(treatment, value)) +\n    geom_boxplot() +\n    facet_wrap(vars(group))\n\nand your colleague modifies the same code to look like this:\n\n# use color to show different groups\nggplot(dat, aes(treatment, value, fill = group)) +\n    geom_boxplot() \n\nGit cannot figure out how to merge your two conflicting versions.\nSo how do we avoid that kind of conflict?\n\n5.3.6.1 Branches\nBranches are the main approach to avoiding these kinds of conflicts. You can make a branch of a repo on GitHub, pull that new branching structure onto your local computer, switch to that new branch, and then work without conflict in your own branch. Here’s an example of all that.\nMake a branch by clicking on the drop down menu called “main” (main is the current branch), typing in a new branch name, and clicking “Create branch…”\n\nThen over in RStudio do a git pull. Once pulled, you can now switch to a different branch. Click the commit button which will open up the window where you can write commit message, but this time we aren’t committing anything, we are going to click on the “main” drop down and select the branch we want (“andy” in my case).\n\nThe files you see in your local folder may well change after you switch branches. Don’t worry–nothing is actually lost, you just have kind of switched your view of files. And in this new branch you can work without worrying about conflicting versions between you and your colleagues. There is one caveat: your colleagues will likely also be able to see your branch, so you will have to be mindful to work only in your own branch(s).\n\n\n5.3.6.2 Pull requests\nBut what if you want your work to be integrated by your colleagues or vice versa? Then you need to bring your work and theirs back into the main branch. That is the job of the pull request.\nA pull request is your way of asking your colleagues if they would like to incorporate your changes. You can do this through the GitHub web interface. In your repo you might see a notification that differences exist between branches and a prompt to start a pull request.\n\nOr you can navigate to a summary of all branches by clicking on the branches button and you will then see three dots next to each branch name allowing you to click and create a pull request\n\nOnce in the pull request interface, you will be able to title and describe your pull request. You should also assign one or more of your team members to review the pull request\n\nAt the bottom of a pull request you—and your reviewers—can see how your work differed from the work in the main branch.\n\nYou and your reviewers might decide you need to change your code before the pull request can be merged. You can return to RStudio, make those changes, commit them, and the pull request will update with those changes. No need to make a new pull request.\nBranches and pull requests are also an excellent way to collaborate with yourself. You might have an idea you want to try out, but you don’t want to break anything in your code that is currently working. Technically you can just rely on your commit history to roll back experimental changes if they don’t work. But it is a cleaner approach to use a branch and then decide if you want to pull those experimental changes in or abandon them.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#using-github-for-class-assignments",
    "href": "github.html#using-github-for-class-assignments",
    "title": "5  Managing code with git and GitHub",
    "section": "5.4 Using GitHub for class assignments",
    "text": "5.4 Using GitHub for class assignments\nStarting now, we will use GitHub for assigning and turning in class assignments. The workflow will be similar but slightly different for the two kinds of assignments we’ll have: notes and lab reports.\n\n5.4.1 Notes assignments\nYou will have your own notes assignments repo created for you in our shared GitHub organization: zool631-spr-2026. Go through the steps of integrating that repo with RStudio.\nEach notes assignment will be posted by the instructor (me Andy) as an issue to your notes repo. You will complete the assignment and mark the issue as resolved and including the commit that indicates you have completed the notes.\nFor peer review, your instructor will leave a new issue in your notes repo, assigned to you and somebody else. That somebody else will then leave you comments in the issue for you to resolve. Once you finish resolving those comments, you again mark the issue as resolved and include the commit that indicates revisions are complete.\n\n\n5.4.2 Lab reports\nFor each lab, you will have a lab report repo created for you and a group of team members in our shared GitHub organization: zool631-spr-2026.\nYou will each go through the steps of integrating that repo with RStudio and then you will each create a unique branch of the repo to hold your individual work.\nEach of you will complete all or most steps of the lab report (all or most depending on assignment details and group agreements). Once complete, you will submit a pull request and assign all your group members as reviewers. You will pick one group member’s code to serve as the reference for all other work. You will then review code and decide how to merge your different versions. That review process will potentially identify changes that need to be made to the reference work. After incorporating those changes, one final version will be merged into the main branch.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#resources",
    "href": "github.html#resources",
    "title": "5  Managing code with git and GitHub",
    "section": "5.5 Resources",
    "text": "5.5 Resources\nOne of the absolute best references on git, GitHub, and working with both through RStudio is Jenny Bryan’s online book Happy git with R.",
    "crumbs": [
      "Introductory material",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Managing code with git and GitHub</span>"
    ]
  }
]